from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_community.vectorstores.utils import filter_complex_metadata
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings
import os
import yaml
import json

def parse_chroma_metadata(doc: Document) -> dict:
    """
    Parse metadata t·ª´ ChromaDB, convert JSON strings v·ªÅ l·∫°i list/dict
    """
    metadata = doc.metadata.copy()
    json_fields = ['tags', 'references', 'detection', 'logsource', 'detection_keywords', 'falsepositives']
    
    for field in json_fields:
        if field in metadata and isinstance(metadata[field], str):
            try:
                metadata[field] = json.loads(metadata[field])
            except (json.JSONDecodeError, TypeError):
                pass  # Gi·ªØ nguy√™n n·∫øu kh√¥ng parse ƒë∆∞·ª£c
    
    return metadata

def format_doc_for_llm(doc: Document, include_full_rule: bool = False) -> str:
    """
    Format document v·ªõi metadata ƒë·ªÉ LLM d·ªÖ ƒë·ªçc
    """
    metadata = parse_chroma_metadata(doc)
    
    output = []
    output.append(f"Title: {metadata.get('title', 'N/A')}")
    output.append(f"ID: {metadata.get('id', 'N/A')}")
    output.append(f"Status: {metadata.get('status', 'N/A')}")
    output.append(f"Level: {metadata.get('level', 'N/A')}")
    output.append(f"Description: {metadata.get('description', 'N/A')}")
    
    if metadata.get('author'):
        output.append(f"Author: {metadata.get('author')}")
    
    if metadata.get('tags'):
        tags = metadata['tags'] if isinstance(metadata['tags'], list) else []
        output.append(f"Tags: {', '.join(str(t) for t in tags)}")
    
    if metadata.get('logsource'):
        logsource = metadata['logsource'] if isinstance(metadata['logsource'], dict) else {}
        output.append(f"Log Source: {json.dumps(logsource, ensure_ascii=False)}")
    
    if metadata.get('detection'):
        detection = metadata['detection'] if isinstance(metadata['detection'], dict) else {}
        if 'keywords' in detection and detection['keywords']:
            keywords = detection['keywords']
            keywords_preview = keywords[:5] if len(keywords) > 5 else keywords
            output.append(f"Detection Keywords: {', '.join(str(k) for k in keywords_preview)}")
            if len(keywords) > 5:
                output.append(f"  (+ {len(keywords)-5} more keywords)")
        if 'condition' in detection:
            output.append(f"Detection Condition: {detection['condition']}")
    
    if metadata.get('references'):
        refs = metadata['references'] if isinstance(metadata['references'], list) else []
        if refs:
            output.append(f"References: {len(refs)} reference(s)")
            for ref in refs[:3]:
                output.append(f"  - {ref}")
            if len(refs) > 3:
                output.append(f"  ... v√† {len(refs)-3} reference(s) kh√°c")
    
    output.append(f"\nContent:\n{doc.page_content}")
    
    if include_full_rule and metadata.get('full_rule'):
        output.append(f"\nFull Rule YAML:\n{metadata['full_rule']}")
    
    return "\n".join(output)

# ƒê∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi t·ª´ file test.py
sigma_path = os.path.join(os.path.dirname(__file__), "sigma", "rules", "web", "webserver_generic")

# Ki·ªÉm tra th∆∞ m·ª•c c√≥ t·ªìn t·∫°i kh√¥ng
if not os.path.exists(sigma_path):
    print(f"Error: Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {sigma_path}")
    print(f"ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi: {os.path.abspath(sigma_path)}")
    sigma_raw_docs = []
else:
    try:
        sigma_loader = DirectoryLoader(
            path=sigma_path,
            glob="**/*.yml",
            loader_cls=TextLoader,
            loader_kwargs={'encoding': 'utf-8'}
        )
        sigma_raw_docs = sigma_loader.load()
        print(f"ƒê√£ load {len(sigma_raw_docs)} file(s)")
        # In th√¥ng tin m·ªôt v√†i file ƒë·∫ßu ti√™n
        for i, doc in enumerate(sigma_raw_docs[:3]):
            print(f"\nFile {i+1}: {doc.metadata.get('source', 'N/A')}")
            print(f"ƒê·ªô d√†i n·ªôi dung: {len(doc.page_content)} k√Ω t·ª±")
    except Exception as e:
        print(f"L·ªói khi load: {e}")
        sigma_raw_docs = []

# X·ª≠ l√Ω YAML v√† t·∫°o processed docs
if sigma_raw_docs:
    print("\n" + "="*50)
    print("X·ª≠ l√Ω v√† parse YAML...")
    print("="*50)
    
    sigma_docs_processed = []
    for doc in sigma_raw_docs:
        try:
            parsed_yaml = yaml.safe_load(doc.page_content)
            
            if parsed_yaml:
                # T·∫°o summary content chi ti·∫øt h∆°n
                title = parsed_yaml.get('title', 'N/A')
                description = parsed_yaml.get('description', 'N/A')
                level = parsed_yaml.get('level', 'N/A')
                status = parsed_yaml.get('status', 'N/A')
                
                summary_content = f"Sigma Rule: {title}\nStatus: {status} | Level: {level}\nDescription: {description}"
                
                # Extract detection keywords n·∫øu c√≥
                detection = parsed_yaml.get('detection', {})
                keywords = detection.get('keywords', []) if isinstance(detection, dict) else []
                if keywords:
                    keywords_preview = keywords[:3] if len(keywords) > 3 else keywords
                    summary_content += f"\nKeywords: {', '.join(str(k) for k in keywords_preview)}"
                    if len(keywords) > 3:
                        summary_content += f" (+{len(keywords)-3} more)"
                
                new_doc = Document(
                    page_content=summary_content,
                    metadata={
                        "source": doc.metadata.get('source'),
                        "full_rule": doc.page_content,
                        # Th√¥ng tin c∆° b·∫£n
                        "title": parsed_yaml.get('title'),
                        "id": parsed_yaml.get('id'),
                        "status": parsed_yaml.get('status'),
                        "level": parsed_yaml.get('level'),
                        "description": parsed_yaml.get('description'),
                        # Th√¥ng tin t√°c gi·∫£ v√† ng√†y th√°ng
                        "author": parsed_yaml.get('author'),
                        "date": str(parsed_yaml.get('date', '')),
                        "modified": str(parsed_yaml.get('modified', '')),
                        # Tags v√† categories
                        "tags": parsed_yaml.get('tags', []),
                        # Logsource
                        "logsource": parsed_yaml.get('logsource', {}),
                        # Detection rules
                        "detection": parsed_yaml.get('detection', {}),
                        "detection_keywords": keywords,
                        "detection_keywords_count": len(keywords),
                        # References v√† false positives
                        "references": parsed_yaml.get('references', []),
                        "falsepositives": parsed_yaml.get('falsepositives', []),
                    }
                )
                sigma_docs_processed.append(new_doc)
            else:
                print(f"Warning: Kh√¥ng parse ƒë∆∞·ª£c YAML t·ª´ {doc.metadata.get('source')}")
        except Exception as e:
            print(f"L·ªói khi parse YAML t·ª´ {doc.metadata.get('source')}: {e}")
    
    print(f"\nƒê√£ x·ª≠ l√Ω {len(sigma_docs_processed)} document(s)")
    
    # In ra m·ªôt s·ªë docs ƒë√£ processed
    print("\n" + "="*50)
    print("M·ªôt s·ªë docs ƒë√£ x·ª≠ l√Ω (hi·ªÉn th·ªã 5 docs ƒë·∫ßu ti√™n):")
    print("="*50)
    for i, doc in enumerate(sigma_docs_processed[:5]):
        print(f"\n{'='*60}")
        print(f"--- Doc {i+1} ---")
        print(f"{'='*60}")
        print(f"\nüìÑ CONTENT:")
        print(f"   {doc.page_content}")
        
        print(f"\nüìã METADATA (chi ti·∫øt):")
        print(f"   {'‚îÄ'*58}")
        
        # Hi·ªÉn th·ªã c√°c tr∆∞·ªùng quan tr·ªçng tr∆∞·ªõc
        important_fields = ['title', 'id', 'status', 'level', 'description', 'author', 'date', 'modified']
        for key in important_fields:
            if key in doc.metadata and doc.metadata[key]:
                value = doc.metadata[key]
                if isinstance(value, list):
                    print(f"   {key:20s}: {', '.join(str(v) for v in value[:5])}")
                    if len(value) > 5:
                        print(f"   {'':20s}  ... v√† {len(value)-5} m·ª•c kh√°c")
                else:
                    print(f"   {key:20s}: {value}")
        
        # Tags
        if 'tags' in doc.metadata and doc.metadata['tags']:
            tags = doc.metadata['tags']
            print(f"   {'tags':20s}: {len(tags)} tag(s)")
            for tag in tags[:10]:
                print(f"   {'':20s}  - {tag}")
            if len(tags) > 10:
                print(f"   {'':20s}  ... v√† {len(tags)-10} tag(s) kh√°c")
        
        # Logsource
        if 'logsource' in doc.metadata and doc.metadata['logsource']:
            logsource = doc.metadata['logsource']
            print(f"   {'logsource':20s}:")
            print(f"   {json.dumps(logsource, indent=8, ensure_ascii=False)}")
        
        # Detection
        if 'detection' in doc.metadata and doc.metadata['detection']:
            detection = doc.metadata['detection']
            keywords_count = doc.metadata.get('detection_keywords_count', 0)
            print(f"   {'detection':20s}: {keywords_count} keyword(s)")
            if 'keywords' in detection and detection['keywords']:
                keywords = detection['keywords']
                print(f"   {'':20s}  Keywords (first 5):")
                for kw in keywords[:5]:
                    print(f"   {'':20s}    - {str(kw)[:80]}")
                if len(keywords) > 5:
                    print(f"   {'':20s}  ... v√† {len(keywords)-5} keyword(s) kh√°c")
            if 'condition' in detection:
                print(f"   {'':20s}  Condition: {detection['condition']}")
        
        # References
        if 'references' in doc.metadata and doc.metadata['references']:
            refs = doc.metadata['references']
            print(f"   {'references':20s}: {len(refs)} reference(s)")
            for ref in refs[:3]:
                print(f"   {'':20s}  - {ref}")
            if len(refs) > 3:
                print(f"   {'':20s}  ... v√† {len(refs)-3} reference(s) kh√°c")
        
        # False positives
        if 'falsepositives' in doc.metadata and doc.metadata['falsepositives']:
            fps = doc.metadata['falsepositives']
            print(f"   {'falsepositives':20s}: {', '.join(str(fp) for fp in fps)}")
        
        # Full rule (preview only)
        if 'full_rule' in doc.metadata:
            full_rule = doc.metadata['full_rule']
            print(f"   {'full_rule':20s}: [Full rule - {len(str(full_rule))} k√Ω t·ª±]")
            preview = str(full_rule)[:200] if len(str(full_rule)) > 200 else str(full_rule)
            print(f"   {'':20s}  Preview: {preview}...")
        
        # C√°c tr∆∞·ªùng kh√°c
        other_fields = {k: v for k, v in doc.metadata.items() 
                       if k not in important_fields + ['tags', 'logsource', 'detection', 'references', 'falsepositives', 'full_rule', 'detection_keywords', 'detection_keywords_count']}
        if other_fields:
            print(f"\n   {'C√°c tr∆∞·ªùng kh√°c:':20s}")
            for key, value in other_fields.items():
                if isinstance(value, (list, dict)):
                    print(f"   {key:20s}: {type(value).__name__} v·ªõi {len(value) if hasattr(value, '__len__') else 'N/A'} m·ª•c")
                else:
                    print(f"   {key:20s}: {value}")
        
        print(f"\nüìä METADATA (JSON format):")
        # T·∫°o b·∫£n copy metadata kh√¥ng c√≥ full_rule ƒë·ªÉ d·ªÖ ƒë·ªçc
        metadata_clean = {k: v for k, v in doc.metadata.items() if k != 'full_rule'}
        print(json.dumps(metadata_clean, indent=2, ensure_ascii=False))
        print()
    
    # In t·ªïng h·ª£p metadata c·ªßa t·∫•t c·∫£ docs
    print("\n" + "="*60)
    print("üìä T·ªîNG H·ª¢P METADATA T·∫§T C·∫¢ DOCS")
    print("="*60)
    print(f"\nT·ªïng s·ªë docs: {len(sigma_docs_processed)}")
    
    # Th·ªëng k√™ c√°c tr∆∞·ªùng metadata
    all_metadata_keys = set()
    for doc in sigma_docs_processed:
        all_metadata_keys.update(doc.metadata.keys())
    
    print(f"\nC√°c tr∆∞·ªùng metadata c√≥ trong docs:")
    for key in sorted(all_metadata_keys):
        count = sum(1 for doc in sigma_docs_processed if key in doc.metadata)
        print(f"  - {key}: c√≥ trong {count}/{len(sigma_docs_processed)} docs")
    
    # In metadata c·ªßa t·∫•t c·∫£ docs d·∫°ng b·∫£ng
    print(f"\n{'='*60}")
    print("DANH S√ÅCH METADATA T·∫§T C·∫¢ DOCS:")
    print(f"{'='*60}")
    for i, doc in enumerate(sigma_docs_processed):
        print(f"\n[{i+1}] {doc.metadata.get('source', 'N/A')}")
        metadata_summary = {k: v for k, v in doc.metadata.items() if k != 'full_rule'}
        print(json.dumps(metadata_summary, indent=4, ensure_ascii=False))
else:
    print("\nKh√¥ng c√≥ documents n√†o ƒë·ªÉ x·ª≠ l√Ω.")

# Embedding v√† ChromaDB
if sigma_docs_processed:
    print("\n" + "="*60)
    print("üîÆ EMBEDDING V√Ä L∆ØU V√ÄO CHROMADB")
    print("="*60)
    
    try:
        # Kh·ªüi t·∫°o embedding model (d√πng HuggingFace local)
        print("\nüì• ƒêang t·∫£i embedding model...")
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("‚úÖ Embedding model ƒë√£ s·∫µn s√†ng")
        
        # T·∫°o ƒë∆∞·ªùng d·∫´n l∆∞u ChromaDB
        persist_directory = os.path.join(os.path.dirname(__file__), "chroma_db")
        os.makedirs(persist_directory, exist_ok=True)
        
        # Chu·∫©n b·ªã documents v·ªõi metadata t∆∞∆°ng th√≠ch ChromaDB
        print(f"\nüíæ ƒêang t·∫°o/load ChromaDB t·∫°i: {persist_directory}")
        
        # Convert list/dict trong metadata th√†nh string cho ChromaDB
        docs_for_chroma = []
        for doc in sigma_docs_processed:
            new_metadata = {}
            for key, value in doc.metadata.items():
                if isinstance(value, (list, dict)):
                    # Convert list/dict th√†nh JSON string
                    new_metadata[key] = json.dumps(value, ensure_ascii=False)
                elif value is None:
                    continue  # Skip None values
                else:
                    new_metadata[key] = value
            
            # T·∫°o document m·ªõi v·ªõi metadata ƒë√£ x·ª≠ l√Ω
            new_doc = Document(
                page_content=doc.page_content,
                metadata=new_metadata
            )
            docs_for_chroma.append(new_doc)
        
        # Filter complex metadata m·ªôt l·∫ßn n·ªØa ƒë·ªÉ ch·∫Øc ch·∫Øn
        docs_for_chroma = filter_complex_metadata(docs_for_chroma)
        
        # T·∫°o ChromaDB vector store
        vectorstore = Chroma.from_documents(
            documents=docs_for_chroma,
            embedding=embeddings,
            persist_directory=persist_directory,
            collection_name="sigma_rules"
        )
        print(f"‚úÖ ƒê√£ l∆∞u {len(sigma_docs_processed)} documents v√†o ChromaDB")
        
        # V√≠ d·ª• queries
        print("\n" + "="*60)
        print("üîç V√ç D·ª§ QUERY T·ª™ CHROMADB")
        print("="*60)
        
        test_queries = [
            "Java payload attack",
            "SQL injection detection",
            "web server vulnerability",
            "remote code execution",
            "Path Travelsal",
            "Pattern SQL injection",
            "1 Or 1 = 1"
        ]
        
        for query in test_queries:
            print(f"\n{'‚îÄ'*60}")
            print(f"üîé Query: '{query}'")
            print(f"{'‚îÄ'*60}")
            
            # Search v·ªõi similarity
            results = vectorstore.similarity_search_with_score(query, k=3)
            
            for i, (doc, score) in enumerate(results, 1):
                print(f"\n  [{i}] Score: {score:.4f}")
                print(f"      Title: {doc.metadata.get('title', 'N/A')}")
                print(f"      Level: {doc.metadata.get('level', 'N/A')}")
                print(f"      Description: {doc.metadata.get('description', 'N/A')[:100]}...")
                print(f"      Source: {doc.metadata.get('source', 'N/A')}")
                
                # Parse metadata v√† hi·ªÉn th·ªã cho LLM
                parsed_meta = parse_chroma_metadata(doc)
                if parsed_meta.get('tags'):
                    tags = parsed_meta['tags'] if isinstance(parsed_meta['tags'], list) else []
                    print(f"      Tags: {', '.join(str(t) for t in tags[:5])}")
        
        # V√≠ d·ª• format document cho LLM
        print(f"\n{'='*60}")
        print("üìù V√ç D·ª§ FORMAT DOCUMENT CHO LLM")
        print("="*60)
        
        sample_query = "Java payload attack"
        sample_results = vectorstore.similarity_search(sample_query, k=2)
        
        for i, doc in enumerate(sample_results, 1):
            print(f"\n{'‚îÄ'*60}")
            print(f"Document {i} - Formatted for LLM:")
            print(f"{'‚îÄ'*60}")
            formatted = format_doc_for_llm(doc, include_full_rule=False)
            print(formatted)
        
        # Metadata filtering example
        print(f"\n{'='*60}")
        print("üîç QUERY V·ªöI METADATA FILTERING")
        print("="*60)
        
        # T√¨m c√°c rules c√≥ level HIGH
        print(f"\nüìä T√¨m rules c√≥ level='high':")
        high_level_docs = vectorstore.similarity_search(
            query="security detection",
            k=5,
            filter={"level": "high"}
        )
        print(f"  T√¨m th·∫•y {len(high_level_docs)} rules v·ªõi level=high")
        for doc in high_level_docs[:3]:
            print(f"    - {doc.metadata.get('title', 'N/A')} (Level: {doc.metadata.get('level', 'N/A')})")
        
        # T√¨m theo tags (parse JSON string tr∆∞·ªõc)
        print(f"\nüìä T√¨m rules c√≥ tag ch·ª©a 'attack':")
        attack_docs = vectorstore.similarity_search(
            query="attack detection",
            k=5
        )
        attack_docs_filtered = []
        for doc in attack_docs:
            parsed_meta = parse_chroma_metadata(doc)
            tags = parsed_meta.get('tags', [])
            if isinstance(tags, list) and any('attack' in str(tag).lower() for tag in tags):
                attack_docs_filtered.append((doc, parsed_meta))
        
        print(f"  T√¨m th·∫•y {len(attack_docs_filtered)} rules li√™n quan ƒë·∫øn attack")
        for doc, parsed_meta in attack_docs_filtered[:3]:
            tags = parsed_meta.get('tags', []) if isinstance(parsed_meta.get('tags'), list) else []
            tags_str = ', '.join([str(t) for t in tags[:3]])
            print(f"    - {parsed_meta.get('title', 'N/A')}")
            print(f"      Tags: {tags_str}")
        
        # V√≠ d·ª• s·ª≠ d·ª•ng formatted document cho LLM
        print(f"\n{'='*60}")
        print("üí° C√ÅCH S·ª¨ D·ª§NG CHO LLM")
        print("="*60)
        print("\nKhi query v√† nh·∫≠n ƒë∆∞·ª£c documents, b·∫°n c√≥ th·ªÉ:")
        print("1. Parse metadata: metadata = parse_chroma_metadata(doc)")
        print("2. Format cho LLM: formatted_text = format_doc_for_llm(doc)")
        print("3. Truy·ªÅn formatted_text v√†o LLM prompt")
        print("\nV√≠ d·ª•:")
        if attack_docs_filtered:
            example_doc, _ = attack_docs_filtered[0]
            example_formatted = format_doc_for_llm(example_doc)
            print(f"\n{example_formatted[:500]}...")
        
        print(f"\n‚úÖ ChromaDB ƒë√£ s·∫µn s√†ng ƒë·ªÉ query!")
        print(f"   ƒê∆∞·ªùng d·∫´n: {persist_directory}")
        
    except Exception as e:
        print(f"\n‚ùå L·ªói khi embedding ho·∫∑c l∆∞u v√†o ChromaDB: {e}")
        import traceback
        traceback.print_exc()