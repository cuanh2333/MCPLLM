{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Pipeline - Security Knowledge Base\n",
        "\n",
        "Notebook nÃ y thá»±c hiá»‡n RAG pipeline hoÃ n chá»‰nh vá»›i thá»© tá»± Ä‘Ãºng:\n",
        "1. **LOAD** - Load documents (Sigma, MITRE ATT&CK, OWASP)\n",
        "2. **SPLIT** - Text splitting\n",
        "3. **EMBED** - Embedding vÃ  lÆ°u vÃ o ChromaDB\n",
        "4. **RETRIEVE** - Query vÃ  Retrieve\n",
        "\n",
        "## ðŸ“‹ Thá»© tá»± cháº¡y cÃ¡c cells:\n",
        "\n",
        "### Phase 1: Setup\n",
        "- **Cell 1**: Imports\n",
        "- **Cell 2**: Helper Functions\n",
        "\n",
        "### Phase 2: LOAD Documents\n",
        "- **Cell 3**: Load Sigma Rules\n",
        "- **Cell 4**: Load MITRE ATT&CK\n",
        "- **Cell 5**: Load OWASP Cheatsheets\n",
        "\n",
        "### Phase 3: SPLIT Text\n",
        "- **Cell 6**: Text Splitting vÃ  Combine All Documents\n",
        "- **Cell 7**: Xem Chunks (Optional)\n",
        "\n",
        "### Phase 4: EMBED\n",
        "- **Cell 8**: Setup Embedding Model\n",
        "- **Cell 9**: Embed vÃ  lÆ°u vÃ o ChromaDB\n",
        "\n",
        "### Phase 5: RETRIEVE\n",
        "- **Cell 10**: Basic Retrieval (Balanced Query)\n",
        "- **Cell 11**: Hybrid Retrieval Setup (BM25 + Reranker)\n",
        "- **Cell 12**: Hybrid Retrieval + Reranking\n",
        "\n",
        "**LÆ°u Ã½:** Cáº§n cÃ i Ä‘áº·t:\n",
        "- `pip install rank-bm25 flashrank sentence-transformers` cho hybrid retrieval vÃ  reranking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Imports completed\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# Phase 1: IMPORTS\n",
        "# ===================================================================\n",
        "\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain_community.document_loaders import WebBaseLoader, AsyncChromiumLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "try:\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "except ImportError:\n",
        "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Reranker vÃ  Hybrid Retrieval\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi\n",
        "except ImportError:\n",
        "    print(\"âš ï¸ rank_bm25 chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t. Cháº¡y: pip install rank-bm25\")\n",
        "    BM25Okapi = None\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import CrossEncoder\n",
        "    CROSS_ENCODER_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CROSS_ENCODER_AVAILABLE = False\n",
        "    print(\"âš ï¸ sentence-transformers chÆ°a cÃ³ CrossEncoder. Sáº½ dÃ¹ng simple reranking.\")\n",
        "    CrossEncoder = None\n",
        "\n",
        "try:\n",
        "    from flashrank import Ranker, RerankRequest\n",
        "    FLASHRANK_AVAILABLE = True\n",
        "except ImportError:\n",
        "    FLASHRANK_AVAILABLE = False\n",
        "\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import json\n",
        "import requests\n",
        "import glob\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import time\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict\n",
        "from collections import Counter\n",
        "\n",
        "print(\"âœ… Imports completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Helper functions defined\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# Phase 1: HELPER FUNCTIONS\n",
        "# ===================================================================\n",
        "\n",
        "def parse_chroma_metadata(doc: Document) -> dict:\n",
        "    \"\"\"Parse metadata tá»« ChromaDB, convert JSON strings vá» láº¡i list/dict\"\"\"\n",
        "    metadata = doc.metadata.copy()\n",
        "    json_fields = ['tags', 'references', 'detection', 'logsource', 'detection_keywords', 'falsepositives']\n",
        "    \n",
        "    for field in json_fields:\n",
        "        if field in metadata and isinstance(metadata[field], str):\n",
        "            try:\n",
        "                metadata[field] = json.loads(metadata[field])\n",
        "            except (json.JSONDecodeError, TypeError):\n",
        "                pass\n",
        "    return metadata\n",
        "\n",
        "def format_doc_for_llm(doc: Document, include_full_rule: bool = False) -> str:\n",
        "    \"\"\"Format document vá»›i metadata Ä‘á»ƒ LLM dá»… Ä‘á»c\"\"\"\n",
        "    metadata = parse_chroma_metadata(doc)\n",
        "    output = []\n",
        "    output.append(f\"Title: {metadata.get('title', 'N/A')}\")\n",
        "    output.append(f\"ID: {metadata.get('id', 'N/A')}\")\n",
        "    output.append(f\"Status: {metadata.get('status', 'N/A')}\")\n",
        "    output.append(f\"Level: {metadata.get('level', 'N/A')}\")\n",
        "    output.append(f\"Description: {metadata.get('description', 'N/A')}\")\n",
        "    if metadata.get('author'):\n",
        "        output.append(f\"Author: {metadata.get('author')}\")\n",
        "    if metadata.get('tags'):\n",
        "        tags = metadata['tags'] if isinstance(metadata['tags'], list) else []\n",
        "        output.append(f\"Tags: {', '.join(str(t) for t in tags)}\")\n",
        "    if metadata.get('logsource'):\n",
        "        logsource = metadata['logsource'] if isinstance(metadata['logsource'], dict) else {}\n",
        "        output.append(f\"Log Source: {json.dumps(logsource, ensure_ascii=False)}\")\n",
        "    if metadata.get('detection'):\n",
        "        detection = metadata['detection'] if isinstance(metadata['detection'], dict) else {}\n",
        "        if 'keywords' in detection and detection['keywords']:\n",
        "            keywords = detection['keywords']\n",
        "            keywords_preview = keywords[:5] if len(keywords) > 5 else keywords\n",
        "            output.append(f\"Detection Keywords: {', '.join(str(k) for k in keywords_preview)}\")\n",
        "            if len(keywords) > 5:\n",
        "                output.append(f\"  (+ {len(keywords)-5} more keywords)\")\n",
        "        if 'condition' in detection:\n",
        "            output.append(f\"Detection Condition: {detection['condition']}\")\n",
        "    if metadata.get('references'):\n",
        "        refs = metadata['references'] if isinstance(metadata['references'], list) else []\n",
        "        if refs:\n",
        "            output.append(f\"References: {len(refs)} reference(s)\")\n",
        "            for ref in refs[:3]:\n",
        "                output.append(f\"  - {ref}\")\n",
        "            if len(refs) > 3:\n",
        "                output.append(f\"  ... vÃ  {len(refs)-3} reference(s) khÃ¡c\")\n",
        "    output.append(f\"\\nContent:\\n{doc.page_content}\")\n",
        "    if include_full_rule and metadata.get('full_rule'):\n",
        "        output.append(f\"\\nFull Rule YAML:\\n{metadata['full_rule']}\")\n",
        "    return \"\\n\".join(output)\n",
        "\n",
        "print(\"âœ… Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: LOAD Documents\n",
        "\n",
        "### 3.1. Load Sigma Rules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ÄÃ£ load 13 Sigma rule file(s)\n",
            "\n",
            "ðŸ“ Xá»­ lÃ½ vÃ  parse YAML...\n",
            "âœ… ÄÃ£ xá»­ lÃ½ 13 Sigma document(s)\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# Phase 2: LOAD - Sigma Rules\n",
        "# ===================================================================\n",
        "\n",
        "sigma_path = r\"D:\\MCPLLM\\test\\sigma\\rules\\web\\webserver_generic\"\n",
        "\n",
        "# Kiá»ƒm tra thÆ° má»¥c cÃ³ tá»“n táº¡i khÃ´ng\n",
        "if not os.path.exists(sigma_path):\n",
        "    print(f\"Error: ThÆ° má»¥c khÃ´ng tá»“n táº¡i: {sigma_path}\")\n",
        "    sigma_raw_docs = []\n",
        "else:\n",
        "    try:\n",
        "        sigma_loader = DirectoryLoader(\n",
        "            path=sigma_path,\n",
        "            glob=\"**/*.yml\",\n",
        "            loader_cls=TextLoader,\n",
        "            loader_kwargs={'encoding': 'utf-8'}\n",
        "        )\n",
        "        sigma_raw_docs = sigma_loader.load()\n",
        "        print(f\"âœ… ÄÃ£ load {len(sigma_raw_docs)} Sigma rule file(s)\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Lá»—i khi load: {e}\")\n",
        "        sigma_raw_docs = []\n",
        "\n",
        "# Parse YAML vÃ  táº¡o processed docs\n",
        "sigma_docs_processed = []\n",
        "\n",
        "if sigma_raw_docs:\n",
        "    print(\"\\nðŸ“ Xá»­ lÃ½ vÃ  parse YAML...\")\n",
        "    for doc in sigma_raw_docs:\n",
        "        try:\n",
        "            parsed_yaml = yaml.safe_load(doc.page_content)\n",
        "            if parsed_yaml:\n",
        "                title = parsed_yaml.get('title', 'N/A')\n",
        "                description = parsed_yaml.get('description', 'N/A')\n",
        "                level = parsed_yaml.get('level', 'N/A')\n",
        "                status = parsed_yaml.get('status', 'N/A')\n",
        "                \n",
        "                summary_content = f\"Sigma Rule: {title}\\nStatus: {status} | Level: {level}\\nDescription: {description}\"\n",
        "                \n",
        "                # Extract detection keywords\n",
        "                detection = parsed_yaml.get('detection', {})\n",
        "                keywords = detection.get('keywords', []) if isinstance(detection, dict) else []\n",
        "                if keywords:\n",
        "                    keywords_preview = keywords[:3] if len(keywords) > 3 else keywords\n",
        "                    summary_content += f\"\\nKeywords: {', '.join(str(k) for k in keywords_preview)}\"\n",
        "                    if len(keywords) > 3:\n",
        "                        summary_content += f\" (+{len(keywords)-3} more)\"\n",
        "                \n",
        "                new_doc = Document(\n",
        "                    page_content=summary_content,\n",
        "                    metadata={\n",
        "                        \"source\": doc.metadata.get('source'),\n",
        "                        \"source_type\": \"sigma_rule\",\n",
        "                        \"title\": parsed_yaml.get('title'),\n",
        "                        \"id\": parsed_yaml.get('id'),\n",
        "                        \"status\": parsed_yaml.get('status'),\n",
        "                        \"level\": parsed_yaml.get('level'),\n",
        "                        \"description\": parsed_yaml.get('description'),\n",
        "                        \"author\": parsed_yaml.get('author'),\n",
        "                        \"date\": str(parsed_yaml.get('date', '')),\n",
        "                        \"modified\": str(parsed_yaml.get('modified', '')),\n",
        "                        \"tags\": parsed_yaml.get('tags', []),\n",
        "                        \"logsource\": parsed_yaml.get('logsource', {}),\n",
        "                        \"detection\": parsed_yaml.get('detection', {}),\n",
        "                        \"detection_keywords\": keywords,\n",
        "                        \"detection_keywords_count\": len(keywords),\n",
        "                        \"references\": parsed_yaml.get('references', []),\n",
        "                        \"falsepositives\": parsed_yaml.get('falsepositives', []),\n",
        "                    }\n",
        "                )\n",
        "                sigma_docs_processed.append(new_doc)\n",
        "        except Exception as e:\n",
        "            print(f\"Lá»—i khi parse YAML: {e}\")\n",
        "    \n",
        "    print(f\"âœ… ÄÃ£ xá»­ lÃ½ {len(sigma_docs_processed)} Sigma document(s)\")\n",
        "else:\n",
        "    print(\"âš ï¸ KhÃ´ng cÃ³ Sigma documents nÃ o Ä‘á»ƒ xá»­ lÃ½.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='title: F5 BIG-IP iControl Rest API Command Execution - Webserver\n",
            "id: 85254a62-22be-4239-b79c-2ec17e566c37\n",
            "related:\n",
            "    - id: b59c98c6-95e8-4d65-93ee-f594dfb96b17\n",
            "      type: similar\n",
            "status: test\n",
            "description: Detects POST requests to the F5 BIG-IP iControl Rest API \"bash\" endpoint, which allows the execution of commands on the BIG-IP\n",
            "references:\n",
            "    - https://f5-sdk.readthedocs.io/en/latest/apidoc/f5.bigip.tm.util.html#module-f5.bigip.tm.util.bash\n",
            "    - https://community.f5.com/t5/technical-forum/icontrolrest-11-5-execute-bash-command/td-p/203029\n",
            "    - https://community.f5.com/t5/technical-forum/running-bash-commands-via-rest-api/td-p/272516\n",
            "author: Nasreddine Bencherchali (Nextron Systems), Thurein Oo\n",
            "date: 2023-11-08\n",
            "tags:\n",
            "    - attack.execution\n",
            "    - attack.t1190\n",
            "    - attack.initial-access\n",
            "logsource:\n",
            "    category: webserver\n",
            "detection:\n",
            "    selection:\n",
            "        cs-method: 'POST'\n",
            "        cs-uri-query|endswith: '/mgmt/tm/util/bash'\n",
            "    condition: selection\n",
            "falsepositives:\n",
            "    - Legitimate usage of the BIG IP REST API to execute command for administration purposes\n",
            "level: medium\n",
            "' metadata={'source': 'D:\\\\MCPLLM\\\\test\\\\sigma\\\\rules\\\\web\\\\webserver_generic\\\\web_f5_tm_utility_bash_api_request.yml'}\n"
          ]
        }
      ],
      "source": [
        "print(sigma_raw_docs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Load MITRE ATT&CK Techniques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¥ Äang load MITRE ATT&CK techniques...\n",
            "âœ… ÄÃ£ parse 216 MITRE ATT&CK techniques tá»« trang enterprise\n",
            "âœ… ÄÃ£ load 216 MITRE ATT&CK techniques\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# Phase 2: LOAD - MITRE ATT&CK Techniques\n",
        "# ===================================================================\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "\n",
        "def parse_mitre_techniques_from_enterprise_page(base_url=\"https://attack.mitre.org/techniques/enterprise/\"):\n",
        "    \"\"\"\n",
        "    Parse táº¥t cáº£ techniques tá»« trang enterprise cá»§a MITRE ATT&CK\n",
        "    Format: T-ID\\nName\\nDescription\n",
        "    \"\"\"\n",
        "    mitre_docs = []\n",
        "    \n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "        response = requests.get(base_url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        \n",
        "        # TÃ¬m táº¥t cáº£ technique links\n",
        "        technique_rows = soup.find_all('tr')\n",
        "        \n",
        "        for row in technique_rows:\n",
        "            cells = row.find_all('td')\n",
        "            if len(cells) >= 2:\n",
        "                # Cell Ä‘áº§u tiÃªn chá»©a ID vÃ  link\n",
        "                first_cell = cells[0]\n",
        "                link = first_cell.find('a', href=True)\n",
        "                if link and '/techniques/T' in link['href']:\n",
        "                    technique_id = link.get_text(strip=True)\n",
        "                    # Cell thá»© hai chá»©a tÃªn\n",
        "                    technique_name = cells[1].get_text(strip=True) if len(cells) > 1 else \"Unknown\"\n",
        "                    \n",
        "                    # Cell thá»© ba cÃ³ thá»ƒ chá»©a description hoáº·c láº¥y tá»« tooltip\n",
        "                    description = \"\"\n",
        "                    if len(cells) > 2:\n",
        "                        description = cells[2].get_text(strip=True)\n",
        "                    \n",
        "                    # Náº¿u khÃ´ng cÃ³ description tá»« cell, láº¥y tá»« tooltip\n",
        "                    if not description or len(description) < 20:\n",
        "                        tooltip = first_cell.get('title') or first_cell.get('data-bs-original-title', '')\n",
        "                        if tooltip:\n",
        "                            description = tooltip\n",
        "                    \n",
        "                    # Format: ID\\nName\\nDescription\n",
        "                    if technique_id and technique_name:\n",
        "                        content = f\"{technique_id}\\n{technique_name}\\n{description}\" if description else f\"{technique_id}\\n{technique_name}\"\n",
        "                        \n",
        "                        doc = Document(\n",
        "                            page_content=content,\n",
        "                            metadata={\n",
        "                                'source_type': 'mitre_attack',\n",
        "                                'source_url': urljoin(base_url, link['href']),\n",
        "                                'technique_id': technique_id,\n",
        "                                'technique_name': technique_name,\n",
        "                                'description_length': len(description) if description else 0\n",
        "                            }\n",
        "                        )\n",
        "                        mitre_docs.append(doc)\n",
        "        \n",
        "        print(f\"âœ… ÄÃ£ parse {len(mitre_docs)} MITRE ATT&CK techniques tá»« trang enterprise\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Lá»—i khi parse MITRE techniques: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    return mitre_docs\n",
        "\n",
        "# Load MITRE techniques\n",
        "print(\"ðŸ“¥ Äang load MITRE ATT&CK techniques...\")\n",
        "mitre_docs = parse_mitre_techniques_from_enterprise_page()\n",
        "\n",
        "if mitre_docs:\n",
        "    print(f\"âœ… ÄÃ£ load {len(mitre_docs)} MITRE ATT&CK techniques\")\n",
        "else:\n",
        "    print(\"âš ï¸ KhÃ´ng cÃ³ MITRE documents nÃ o.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='T1548\n",
            "Abuse Elevation Control Mechanism\n",
            "Adversaries may circumvent mechanisms designed to control elevate privileges to gain higher-level permissions. Most modern systems contain native elevation control mechanisms that are intended to limit privileges that a user can perform on a machine. Authorization has to be granted to specific users in order to perform tasks that can be considered of higher risk. An adversary can perform several methods to take advantage of built-in control mechanisms in order to escalate privileges on a system.' metadata={'source_type': 'mitre_attack', 'source_url': 'https://attack.mitre.org/techniques/T1548', 'technique_id': 'T1548', 'technique_name': 'Abuse Elevation Control Mechanism', 'description_length': 500}\n"
          ]
        }
      ],
      "source": [
        "print(mitre_docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 3.3. Load OWASP Cheatsheets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“‚ Äang load OWASP cheatsheets tá»«: D:\\MCPLLM\\test\\cheatsheets\n",
            "  âœ… Loaded 1 docs tá»«: Abuse_Case_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Access_Control_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: AJAX_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Attack_Surface_Analysis_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Authentication_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Authorization_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Authorization_Testing_Automation_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Automotive_Security.md\n",
            "  âœ… Loaded 1 docs tá»«: Bean_Validation_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Browser_Extension_Vulnerabilities_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: C-Based_Toolchain_Hardening_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Choosing_and_Using_Security_Questions_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: CI_CD_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Clickjacking_Defense_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Content_Security_Policy_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Cookie_Theft_Mitigation_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Credential_Stuffing_Prevention_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Cross_Site_Scripting_Prevention_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Cryptographic_Storage_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Database_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Denial_of_Service_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Dependency_Graph_SBOM_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Deserialization_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Django_REST_Framework_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Django_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Docker_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: DOM_based_XSS_Prevention_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: DOM_Clobbering_Prevention_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: DotNet_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Drone_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Error_Handling_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: File_Upload_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Forgot_Password_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: GraphQL_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: gRPC_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: HTML5_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: HTTP_Headers_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: HTTP_Strict_Transport_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Infrastructure_as_Code_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Injection_Prevention_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Injection_Prevention_in_Java_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Input_Validation_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Insecure_Direct_Object_Reference_Prevention_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: JAAS_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Java_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: JSON_Web_Token_for_Java_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Key_Management_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Kubernetes_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Laravel_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: LDAP_Injection_Prevention_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Legacy_Application_Management_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: LLM_Prompt_Injection_Prevention_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Logging_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Logging_Vocabulary_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Mass_Assignment_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Microservices_based_Security_Arch_Doc_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Microservices_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Mobile_Application_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Multifactor_Authentication_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Network_Segmentation_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: NodeJS_Docker_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Nodejs_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: NoSQL_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: NPM_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: OAuth2_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: OS_Command_Injection_Defense_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Password_Storage_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: PHP_Configuration_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Pinning_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Prototype_Pollution_Prevention_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Query_Parameterization_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: REST_Assessment_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: REST_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Ruby_on_Rails_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: SAML_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Secrets_Management_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Secure_AI_Model_Ops_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Secure_Cloud_Architecture_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Secure_Code_Review_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Secure_Product_Design_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Securing_Cascading_Style_Sheets_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Serverless_FaaS_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Server_Side_Request_Forgery_Prevention_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Session_Management_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Software_Supply_Chain_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: SQL_Injection_Prevention_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Symfony_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Third_Party_Javascript_Management_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Third_Party_Payment_Gateway_Integration.md\n",
            "  âœ… Loaded 1 docs tá»«: Threat_Modeling_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: TLS_Cipher_String_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Transaction_Authorization_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Transport_Layer_Protection_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Transport_Layer_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Unvalidated_Redirects_and_Forwards_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: User_Privacy_Protection_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Virtual_Patching_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Vulnerability_Disclosure_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Vulnerable_Dependency_Management_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: WebSocket_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Web_Service_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: XML_External_Entity_Prevention_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: XML_Security_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: XSS_Filter_Evasion_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: XS_Leaks_Cheat_Sheet.md\n",
            "  âœ… Loaded 1 docs tá»«: Zero_Trust_Architecture_Cheat_Sheet.md\n",
            "\n",
            "âœ… Total OWASP cheatsheet documents loaded: 107\n",
            "\n",
            "ðŸ“‹ Má»™t vÃ i documents máº«u:\n",
            "  [1] Abuse_Case_Cheat_Sheet.md | Abuse_Case_Cheat_Sheet.md | 28002 chars\n",
            "  [2] Access_Control_Cheat_Sheet.md | Access_Control_Cheat_Sheet.md | 176 chars\n",
            "  [3] AJAX_Security_Cheat_Sheet.md | AJAX_Security_Cheat_Sheet.md | 8339 chars\n"
          ]
        }
      ],
      "source": [
        "# Load OWASP Cheatsheets tá»« thÆ° má»¥c local\n",
        "# Read in documents using LangChain's loaders\n",
        "# Take everything in all the sub-folders of our knowledgebase\n",
        "\n",
        "import os, glob\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "\n",
        "# ÄÆ°á»ng dáº«n tá»›i thÆ° má»¥c cheatsheets\n",
        "cheatsheets_path = r\"D:\\MCPLLM\\test\\cheatsheets\"\n",
        "\n",
        "# Kiá»ƒm tra thÆ° má»¥c cÃ³ tá»“n táº¡i khÃ´ng\n",
        "if not os.path.exists(cheatsheets_path):\n",
        "    print(f\"âš ï¸  ThÆ° má»¥c khÃ´ng tá»“n táº¡i: {cheatsheets_path}\")\n",
        "    print(f\"   Táº¡o thÆ° má»¥c hoáº·c kiá»ƒm tra Ä‘Æ°á»ng dáº«n\")\n",
        "    owasp_docs = []\n",
        "else:\n",
        "    # DÃ¹ng raw string Ä‘á»ƒ trÃ¡nh lá»—i escape\n",
        "    folders = glob.glob(os.path.join(cheatsheets_path, \"*\"))\n",
        "    \n",
        "    # Thiáº¿t láº­p text loader kwargs\n",
        "    # Thá»­ dÃ¹ng autodetect náº¿u cÃ³ chardet, náº¿u khÃ´ng thÃ¬ dÃ¹ng utf-8\n",
        "    try:\n",
        "        import chardet\n",
        "        text_loader_kwargs = {'autodetect_encoding': True}\n",
        "    except ImportError:\n",
        "        print(\"  âš ï¸  Module 'chardet' chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t. DÃ¹ng encoding='utf-8' thay tháº¿.\")\n",
        "        print(\"  ðŸ’¡ Äá»ƒ cÃ i: pip install chardet\")\n",
        "        text_loader_kwargs = {'encoding': 'utf-8'}\n",
        "    \n",
        "    owasp_docs = []\n",
        "    print(f\"ðŸ“‚ Äang load OWASP cheatsheets tá»«: {cheatsheets_path}\")\n",
        "    \n",
        "    for folder in folders:\n",
        "        doc_type = os.path.basename(folder)\n",
        "        \n",
        "        try:\n",
        "            if os.path.isdir(folder):\n",
        "                loader = DirectoryLoader(\n",
        "                    folder,\n",
        "                    glob=\"**/*.md\",  # âœ… luÃ´n dÃ¹ng dáº¥u /\n",
        "                    loader_cls=TextLoader,\n",
        "                    loader_kwargs=text_loader_kwargs\n",
        "                )\n",
        "                folder_docs = loader.load()\n",
        "                \n",
        "            elif os.path.isfile(folder) and folder.endswith('.md'):\n",
        "                loader = TextLoader(folder, **text_loader_kwargs)\n",
        "                folder_docs = loader.load()\n",
        "            else:\n",
        "                continue  # Bá» qua náº¿u khÃ´ng pháº£i file .md\n",
        "            \n",
        "            for doc in folder_docs:\n",
        "                doc.metadata[\"doc_type\"] = doc_type\n",
        "                doc.metadata[\"source_type\"] = \"owasp_cheatsheet\"\n",
        "                # ThÃªm source path vÃ o metadata\n",
        "                if 'source' not in doc.metadata:\n",
        "                    doc.metadata[\"source\"] = folder\n",
        "                owasp_docs.append(doc)\n",
        "            \n",
        "            if folder_docs:\n",
        "                print(f\"  âœ… Loaded {len(folder_docs)} docs tá»«: {doc_type}\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"  âš ï¸  Lá»—i khi load {folder}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\nâœ… Total OWASP cheatsheet documents loaded: {len(owasp_docs)}\")\n",
        "    \n",
        "    # Hiá»ƒn thá»‹ má»™t vÃ i documents máº«u\n",
        "    if owasp_docs:\n",
        "        print(f\"\\nðŸ“‹ Má»™t vÃ i documents máº«u:\")\n",
        "        for i, doc in enumerate(owasp_docs[:3]):\n",
        "            doc_type = doc.metadata.get('doc_type', 'N/A')\n",
        "            source = doc.metadata.get('source', 'N/A')\n",
        "            if isinstance(source, str):\n",
        "                source_name = os.path.basename(source)\n",
        "            else:\n",
        "                source_name = str(source)\n",
        "            print(f\"  [{i+1}] {doc_type} | {source_name} | {len(doc.page_content)} chars\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='# Abuse Case Cheat Sheet (Historical)\n",
            "\n",
            "## Archive Statement\n",
            "\n",
            "Reviewers have identified that abuse cases are rarely used in practice. Additionally, the material is presented as a \"getting started tutorial\" which isn't appropriate for the cheat sheet series.\n",
            "\n",
            "## Introduction\n",
            "\n",
            "Often when the security level of an application is mentioned in requirements, the following _expressions_ are met:\n",
            "\n",
            "- _The application must be secure_.\n",
            "- _The application must defend against all attacks targeting this category of application_.\n",
            "- _The application must defend against attacks from the OWASP TOP 10_\n",
            "- ...\n",
            "\n",
            "These security requirements are too generic, and thus useless for a development team...\n",
            "\n",
            "In order to build a secure application, from a pragmatic point of view, it is important to identify the attacks which the application must defend against, according to its business and technical context. Abuse cases were a frequently recommended _threat modeling_ technique, and reviewing the [threat modeling](https://cheatsheetseries.owasp.org/cheatsheets/Threat_Modeling_Cheat_Sheet.html) cheat sheet may be helpful. In practice, the abuse case framework seems heavyweight and there are few published examples or success stories.\n",
            "\n",
            "### Objective\n",
            "\n",
            "The objective of this cheat sheet is to provide an explanation of what an **Abuse Case** is, how abuse cases can be important when considering the security of an application, and finally to provide a proposal for a pragmatic approach to building a list of abuse cases and tracking them for every feature planned for implementation as part of an application. The cheat sheet may be used for this purpose regardless of the project methodology used (waterfall or agile).\n",
            "\n",
            "**Important note about this Cheat Sheet:**\n",
            "\n",
            "```text\n",
            "The main objective is to provide a pragmatic approach in order to allow a company or a project team\n",
            "to start building and handling the list of abuse cases and then customize the elements\n",
            "proposed to its context/culture in order to, finally, build its own method.\n",
            "\n",
            "This cheat sheet can be seen as a getting-started tutorial.\n",
            "```\n",
            "\n",
            "### Context & approach\n",
            "\n",
            "#### Why clearly identify the attacks\n",
            "\n",
            "Clearly identifying the attacks against which the application must defend is essential in order to enable the following steps in a project or sprint:\n",
            "\n",
            "- Evaluate the business risk for each of the identified attacks in order to perform a selection according to the business risk and the project/sprint budget.\n",
            "- Derive security requirements and add them into the project specification or sprint's user stories and acceptance criteria.\n",
            "- Estimate the overhead of provision in the initial project/sprint charge that will be necessary to implement the countermeasures.\n",
            "- About countermeasures: Allow the project team to define them, and to determine in which location (network, infrastructure, code...) they should be located.\n",
            "\n",
            "#### Notion of Abuse Cases\n",
            "\n",
            "You can think of **Abuse cases** in two ways. The first is to discover attacks (answer the question \"what can go wrong\"), and the second is to help record those attacks (informally, this includes threats, issues, risks) in a form that may be less intimidating to developers.\n",
            "\n",
            "An **Abuse Case** can be defined as:\n",
            "\n",
            "```text\n",
            "A way to use a feature that was not expected by the implementer,\n",
            "allowing an attacker to influence the feature or outcome of use of\n",
            "the feature based on the attacker action (or input).\n",
            "```\n",
            "\n",
            "Synopsis defines an **Abuse Case** like this:\n",
            "\n",
            "```text\n",
            "Misuse and abuse cases describe how users misuse or exploit the weaknesses\n",
            "of controls in software features to attack an application.\n",
            "\n",
            "This can lead to tangible business impact when a direct attack against\n",
            "business functionalities, which may bring in revenue or provide\n",
            "positive user experience, are attacked.\n",
            "\n",
            "Abuse cases can also be an effective way to drive security requirements\n",
            "that lead to proper protection of these critical business use cases.\n",
            "```\n",
            "\n",
            "[Synopsis source](https://www.synopsys.com/blogs/software-security/abuse-cases-can-drive-security-requirements.html)\n",
            "\n",
            "#### How to define the list of Abuse Cases\n",
            "\n",
            "There are many different ways to define the list of abuse cases for a feature (that can be mapped to a user story in agile projects).\n",
            "\n",
            "[Threat Modeling](https://cheatsheetseries.owasp.org/cheatsheets/Threat_Modeling_Cheat_Sheet.html) is a set of techniques for anticipating what can go wrong, and ensuring we do something about each identified possible scenario. Taking each item on the list of \"what are we going to do about it\" and writing an abuse case may help your engineering teams process the output.\n",
            "\n",
            "The project [OWASP Open SAMM](https://owasp.org/www-project-samm/) proposes the following approach in the _Stream B_ of the Security Practice _Requirements Driven Testing_ for the Maturity level 2:\n",
            "\n",
            "```text\n",
            "Misuse and abuse cases describe unintended and malicious use scenarios of the application, describing how an attacker could do this. Create misuse and abuse cases to misuse or exploit the weaknesses of controls in software features to attack an application. Use abuse-case models for an application to serve as fuel for identification of concrete security tests that directly or indirectly exploit the abuse scenarios.\n",
            "\n",
            "Abuse of functionality, sometimes referred to as a â€œbusiness logic attackâ€, depends on the design and implementation of application functions and features. An example is using a password reset flow to enumerate accounts. As part of business logic testing, identify the business rules that are important for the application and turn them into experiments to verify whether the application properly enforces the business rule. For example, on a stock trading application, is the attacker allowed to start a trade at the beginning of the day and lock in a price, hold the transaction open until the end of the day, then complete the sale if the stock price has risen or cancel if the price dropped?\n",
            "```\n",
            "\n",
            "Open SAMM source: [Verification Requirement Driven Testing Stream B](https://owaspsamm.org/model/verification/requirements-driven-testing/stream-b/)\n",
            "\n",
            "Another way to achieve the building of the list can be the following (more bottom-up and collaboratively oriented):\n",
            "\n",
            "Make a workshop that includes people with the following profiles:\n",
            "\n",
            "- **Business analyst**: Will be the business key people that will describe each feature from a business point of view.\n",
            "- **Risk analyst**: Will be the company's risk personnel that will evaluate the business risk from a proposed attack (sometimes it is the **Business analyst** depending on the company).\n",
            "- **Penetration tester**: Will be the _attacker_ that will propose attacks that they can perform on the business feature(s) in question. If the company does not have a person with this profile then it is possible to request the service of an external specialist. If possible, include 2 penetration testers with different backgrounds in order to increase the number of possible attacks that will be identified and considered.\n",
            "- **Technical leaders of the projects**: Will be the project technical people and will allow technical exchange about attacks and countermeasures identified during the workshop.\n",
            "- **Quality assurance analyst or functional tester**: Personnel that may have a good sense of how the application/functionality is intended to work (positive testing), not work (negative testing), and what things cause it to fail (failure cases).\n",
            "\n",
            "During this workshop (duration will depend on the size of the feature list, but 4 hours is a good start) all business features that will be part of the project or the sprint will be processed. The output of the workshop will be a list of attacks (abuse cases) for all business features. All abuse cases will have a risk rating that allows for filtering and prioritization.\n",
            "\n",
            "It is important to take into account **Technical** and **Business** kind of abuse cases and mark them accordingly.\n",
            "\n",
            "_Example:_\n",
            "\n",
            "- Technical flagged abuse case: Add Cross Site Scripting injection into a comment input field.\n",
            "- Business flagged abuse case: Ability to arbitrarily modify the price of an article in an online shop prior to passing an order causing the user to pay a lower amount for the wanted article.\n",
            "\n",
            "#### When to define the list of Abuse Cases\n",
            "\n",
            "In agile projects, the definition workshop must be made after the meeting in which User Stories are included in a Sprint.\n",
            "\n",
            "In waterfall projects, the definition workshop must be made when the business features to implement are identified and known by the business.\n",
            "\n",
            "Whatever the mode of the project used (agile or waterfall), the abuse cases selected to be addressed must become security requirements in each feature specification section (waterfall) or User Story acceptance criteria (agile) in order to allow additional cost/effort evaluation, identification and implementation of the countermeasures.\n",
            "\n",
            "Each abuse case must have a unique identifier in order to allow tracking throughout the whole project/sprint (details about this point will be given in the proposal section).\n",
            "\n",
            "An example of unique ID format can be **ABUSE_CASE_001**.\n",
            "\n",
            "The following figure provides an overview of the chaining of the different steps involved (from left to right):\n",
            "\n",
            "![Overview Schema](../assets/Abuse_Case_Cheat_Sheet_Overview.png)\n",
            "\n",
            "### Proposal\n",
            "\n",
            "The proposal will focus on the output of the workshop explained in the previous section.\n",
            "\n",
            "#### Step 1: Preparation of the workshop\n",
            "\n",
            "First, even if it seems obvious, the key business people must be sure to know, understand and be able to explain the business features that will be processed during the workshop.\n",
            "\n",
            "Secondly, create a new Microsoft Excel file (you can also use Google Sheets or any other similar software) with the following sheets (or tabs):\n",
            "\n",
            "- **FEATURES**\n",
            "    - Will contain a table with the list of business features planned for the workshop.\n",
            "- **ABUSE CASES**\n",
            "    - Will contain a table with all abuse cases identified during the workshop.\n",
            "- **COUNTERMEASURES**\n",
            "    - Will contain a table with the list of possible countermeasures (light description) imagined for the abuse cases identified.\n",
            "    - This sheet is not mandatory, but it can be useful (for an abuse case to know), if a fix is easy to implement and then can impact the risk rating.\n",
            "    - Countermeasures can be identified by the AppSec profile during the workshop, because an AppSec person must be able to perform attacks but also to build or identify defenses (it is not always the case for the Pentester profile because this person's focus is generally on the attack side only, so, the combination Pentester + AppSec is very efficient to have a 360 degree view).\n",
            "\n",
            "This is the representation of each sheet along with an example of content that will be filled during the workshop:\n",
            "\n",
            "_FEATURES_ sheet:\n",
            "\n",
            "| Feature unique ID |     Feature name      |           Feature short description           |\n",
            "| :---------------: | :-------------------: | :-------------------------------------------: |\n",
            "|    FEATURE_001    | DocumentUploadFeature | Allow user to upload document along a message |\n",
            "\n",
            "_COUNTERMEASURES_ sheet:\n",
            "\n",
            "| Countermeasure unique ID | Countermeasure short description                       | Countermeasure help/hint                                |\n",
            "| ------------------------ | ------------------------------------------------------ | ------------------------------------------------------- |\n",
            "| DEFENSE_001              | Validate the uploaded file by loading it into a parser | Use advice from the OWASP Cheat Sheet about file upload |\n",
            "\n",
            "_ABUSE CASES_ sheet:\n",
            "\n",
            "| Abuse case unique ID | Feature ID impacted |                     Abuse case's attack description                     | Attack referential ID (if applicable) | CVSS V3 risk rating (score) |                CVSS V3 string                | Kind of abuse case | Countermeasure ID applicable | Handling decision (To Address or Risk Accepted) |\n",
            "| :------------------: | :-----------------: | :---------------------------------------------------------------------: | :-----------------------------------: | :-------------------------: | :------------------------------------------: | :----------------: | :--------------------------: | :---------------------------------------------: |\n",
            "|    ABUSE_CASE_001    |     FEATURE_001     | Upload Office file with malicious macro in charge of dropping a malware |               CAPEC-17                |         HIGH (7.7)          | CVSS:3.0/AV:N/AC:H/PR:L/UI:R/S:C/C:N/I:H/A:H |     Technical      |         DEFENSE_001          |                   To Address                    |\n",
            "\n",
            "#### Step 2: During the workshop\n",
            "\n",
            "Use the spreadsheet to review all the features.\n",
            "\n",
            "For each feature, follow this flow:\n",
            "\n",
            "1. Key business people explain the current feature from a business point of view.\n",
            "2. Penetration testers propose and explain a set of attacks that they can perform against the feature.\n",
            "3. For each attack proposed:\n",
            "   1. Appsec proposes a countermeasure and a preferred set up location (infrastructure, network, code, design...).\n",
            "   2. Technical people give feedback about the feasibility of the proposed countermeasure.\n",
            "   3. Penetration testers use the CVSS v3 (or other standard) calculator to determine a risk rating. (ex: [CVSS V3 calculator](https://www.first.org/cvss/calculator/3.0))\n",
            "   4. Risk leaders should accept or modify the risk rating to determine the final risk score which accurately reflects the real business impact for the company.\n",
            "\n",
            "4. Business, Risk, and Technical leaders should find a consensus and filter the list of abuses for the current feature to keep the ones that must be addressed, and then flag them accordingly in the _ABUSE CASES_ sheet (**if risk is accepted then add a comment to explain why**).\n",
            "5. Pass to next feature...\n",
            "\n",
            "If the presence of penetration testers is not possible then you can use the following references to identify the applicable attacks on your features:\n",
            "\n",
            "- [OWASP Automated Threats to Web Applications](https://owasp.org/www-project-automated-threats-to-web-applications/)\n",
            "- [OWASP Testing Guide](https://owasp.org/www-project-web-security-testing-guide/stable/)\n",
            "- [OWASP Mobile Testing Guide](https://github.com/OWASP/owasp-mstg)\n",
            "- [Common Attack Pattern Enumeration and Classification (CAPEC)](https://capec.mitre.org/)\n",
            "\n",
            "Important note on attacks and countermeasure knowledge base(s):\n",
            "\n",
            "```text\n",
            "With time and experience across projects, you will obtain your own dictionary of attacks and countermeasures\n",
            "that are applicable to the kind of application in your business domain.\n",
            "\n",
            "This dictionary will speed up the future workshops in a significant way.\n",
            "\n",
            "To promote the creation of this dictionary, you can, at the end of the project/sprint, gather the list\n",
            "of attacks and countermeasures identified in a central location (wiki, database, file...) that will be\n",
            "used during the next workshop in combination with input from penetration testers.\n",
            "```\n",
            "\n",
            "#### Step 3: After the workshop\n",
            "\n",
            "The spreadsheet contains (at this stage) the list of all abuse cases that must be handled and, potentially (depending on the capacity) corresponding countermeasures.\n",
            "\n",
            "Now, there are two remaining task:\n",
            "\n",
            "1. Key business people must update the specification of each feature (waterfall) or the User Story of each feature (agile) to include the associated abuse cases as Security Requirements (waterfall) or Acceptance Criteria (agile).\n",
            "2. Key technical people must evaluate the overhead in terms of expense/effort to take into account the countermeasure.\n",
            "\n",
            "#### Step 4: During implementation - Abuse cases handling tracking\n",
            "\n",
            "In order to track the handling of all the abuse cases, the following approach can be used:\n",
            "\n",
            "If one or several abuse cases are handled at:\n",
            "\n",
            "- **Design, Infrastructure or Network level**\n",
            "    - Make a note in the documentation or schema to indicate that _This design/network/infrastructure takes into account the abuse cases ABUSE_CASE_001, ABUSE_CASE_002, ABUSE_CASE_xxx_.\n",
            "- **Code level**\n",
            "    - Put a special comment in the classes/scripts/modules to indicate that _This class/module/script takes into account the abuse cases ABUSE_CASE_001, ABUSE_CASE_002, ABUSE_CASE_xxx_.\n",
            "    - Dedicated annotation like `@AbuseCase(ids={\"ABUSE_CASE_001\",\"ABUSE_CASE_002\"})` can be used to facilitate tracking and allow identification into integrated development environment.\n",
            "\n",
            "Using this way, it becomes possible (via some minor scripting) to identify where abuse cases are addressed.\n",
            "\n",
            "#### Step 5: During implementation - Abuse cases handling validation\n",
            "\n",
            "As abuse cases are defined, it is possible to put in place automated or manual validations to ensure that:\n",
            "\n",
            "- All the selected abuse cases are handled.\n",
            "- An abuse case is correctly/completely handled.\n",
            "\n",
            "Validations can be of the following varieties:\n",
            "\n",
            "- Automated (run regularly at commit, daily or weekly in the Continuous Integration Jobs of the project):\n",
            "    - Custom audit rules in Static Application Security Testing (SAST) or Dynamic Application Security Testing (DAST) tools.\n",
            "    - Dedicated unit, integration or functional security oriented tests.\n",
            "    - ...\n",
            "- Manual:\n",
            "    - Security code review between project's peers during the design or implementation.\n",
            "    - Provide the list of all abuse cases addressed to pentesters so that they may validate the protection efficiency for each abuse case during an intrusion test against the application (the pentester will validate that the attacks identified are no longer effective and will also try to find other possible attacks).\n",
            "    - ...\n",
            "\n",
            "Adding automated tests also allow teams to track the effectiveness of countermeasures against abuse cases and determine if the countermeasures are still in place during a maintenance or bug fixing phase of a project (to prevent accidental removal/disabling). It is also useful when a [Continuous Delivery](https://continuousdelivery.com/) approach is used, to ensure that all abuse cases protections are in place before opening access to the application.\n",
            "\n",
            "### Example of derivation of Abuse Cases as User Stories\n",
            "\n",
            "The following section shows an example of derivation of Abuse Cases as User Stories, here using the [OWASP TOP 10](https://owasp.org/www-project-top-ten/) as input source.\n",
            "\n",
            "Threat Oriented Personas:\n",
            "\n",
            "- Malicious User\n",
            "- Abusive User\n",
            "- Unknowing User\n",
            "\n",
            "#### A1:2017-Injection\n",
            "\n",
            "_Epic:_\n",
            "\n",
            "Almost any source of data can be an injection vector, environment variables, parameters, external and internal web services, and all types of users. [Injection](https://owasp.org/www-community/Injection_Flaws) flaws occur when an attacker can send hostile data to an interpreter.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I will perform an injection attack (SQL, LDAP, XPath, or NoSQL queries, OS commands, XML parsers, SMTP headers, expression languages, and ORM queries) against input fields of the User or API interfaces\n",
            "\n",
            "#### A2:2017-Broken Authentication\n",
            "\n",
            "_Epic:_\n",
            "\n",
            "Attackers have access to hundreds of millions of valid username and password combinations for credential stuffing, default administrative account lists, automated brute force, and dictionary attack tools. Session management attacks are well understood, particularly in relation to unexpired session tokens.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I have access to hundreds of millions of valid username and password combinations for credential stuffing.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I have default administrative account lists, automated brute force, and dictionary attack tools I use against login areas of the application and support systems.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I manipulate session tokens using expired and fake tokens to gain access.\n",
            "\n",
            "#### A3:2017-Sensitive Data Exposure\n",
            "\n",
            "_Epic:_\n",
            "\n",
            "Rather than directly attacking crypto, attackers steal keys, execute man-in-the-middle attacks, or steal clear text data off the server, while in transit, or from the user's client, e.g. browser. A manual attack is generally required. Previously retrieved password databases could be brute forced by Graphics Processing Units (GPUs).\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I steal keys that were exposed in the application to get unauthorized access to the application or system.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I execute man-in-the-middle attacks to get access to traffic and leverage it to obtain sensitive data and possibly get unauthorized access to the application.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I steal clear text data off the server, while in transit, or from the user's client, e.g. browser to get unauthorized access to the application or system.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I find and target old or weak cryptographic algorithms by capturing traffic and breaking the encryption.\n",
            "\n",
            "#### A4:2017-XML External Entities (XXE)\n",
            "\n",
            "_Epic:_\n",
            "\n",
            "Attackers can exploit vulnerable XML processors if they can upload XML or include hostile content in an XML document, exploiting vulnerable code, dependencies or integrations.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I exploit vulnerable areas of the application where the user or system can upload XML to extract data, execute a remote request from the server, scan internal systems, perform a denial-of-service attack, as well as execute other attacks.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I include hostile content in an XML document which is uploaded to the application or system to extract data, execute a remote request from the server, scan internal systems, perform a denial-of-service attack, as well as execute other attacks.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I include malicious XML code to exploit vulnerable code, dependencies or integrations to extract data, execute a remote request from the server, scan internal systems, perform a denial-of-service attack (e.g. Billion Laughs attack), as well as execute other attacks.\n",
            "\n",
            "#### A5:2017-Broken Access Control\n",
            "\n",
            "_Epic:_\n",
            "\n",
            "Exploitation of access control is a core skill of attackers. Access control is detectable using manual means, or possibly through automation for the absence of access controls in certain frameworks.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I bypass access control checks by modifying the URL, internal application state, or the HTML page, or simply using a custom API attack tool.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I manipulate the primary key and change it to access another's users record, allowing viewing or editing someone else's account.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I manipulate sessions, access tokens, or other access controls in the application to act as a user without being logged in, or acting as an admin/privileged user when logged in as a user.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I leverage metadata manipulation, such as replaying or tampering with a JSON Web Token (JWT) access control token or a cookie or hidden field manipulated to elevate privileges or abusing JWT invalidation.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I exploit Cross-Origin Resource Sharing CORS misconfiguration allowing unauthorized API access.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I force browsing to authenticated pages as an unauthenticated user or to privileged pages as a standard user.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I access APIs with missing access controls for POST, PUT and DELETE.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I target default crypto keys in use, weak crypto keys generated or re-used, or keys where rotation is missing.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I find areas where the user agent (e.g. app, mail client) does not verify if the received server certificate is valid and perform attacks where I get unauthorized access to data.\n",
            "\n",
            "#### A6:2017-Security Misconfiguration\n",
            "\n",
            "_Epic:_\n",
            "\n",
            "Attackers will often attempt to exploit unpatched flaws or access default accounts, unused pages, unprotected files and directories, etc to gain unauthorized access or knowledge of the system.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I find and exploit missing appropriate security hardening configurations on any part of the application stack, or improperly configured permissions on cloud services.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I find unnecessary features which are enabled or installed (e.g. unnecessary ports, services, pages, accounts, or privileges) and attack or exploit the weakness.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I use default accounts and their passwords to access systems, interfaces, or perform actions on components which I should not be able to.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I find areas of the application where error handling reveals stack traces or other overly informative error messages I can use for further exploitation.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I find areas where upgraded systems, latest security features are disabled or not configured securely.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I find security settings in the application servers, application frameworks (e.g. Struts, Spring, ASP.NET), libraries, databases, etc. not set to secure values.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I find the server does not send security headers or directives or are set to insecure values.\n",
            "\n",
            "#### A7:2017-Cross-Site Scripting (XSS)\n",
            "\n",
            "_Epic:_\n",
            "\n",
            "XSS is the second most prevalent issue in the OWASP Top 10, and is found in around two-thirds of all applications.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I perform reflected XSS where the application or API includes unvalidated and unescaped user input as part of HTML output. My successful attack can allow the attacker to execute arbitrary HTML and JavaScript in my victim's browser. Typically the victim will need to interact with some malicious link that points to an attacker-controlled page, such as malicious watering hole websites, advertisements, or similar.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I perform stored XSS where the application or API stores unsanitized user input that is viewed at a later time by another user or an administrator.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I perform DOM XSS where JavaScript frameworks, single-page applications, and APIs that dynamically include attacker-controllable data to a page is vulnerable to DOM XSS.\n",
            "\n",
            "#### A8:2017-Insecure Deserialization\n",
            "\n",
            "_Epic:_\n",
            "\n",
            "Exploitation of deserialization is somewhat difficult, as off-the-shelf exploits rarely work without changes or tweaks to the underlying exploit code.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I find areas of the application and APIs where deserialization of hostile or tampered objects can be supplied. As a result, I can focus on an object and data structure related attacks where the attacker modifies application logic or achieves arbitrary remote code execution if there are classes available to the application that can change behavior during or after deserialization. Or I focus on data tampering attacks such as access-control-related attacks where existing data structures are used but the content is changed.\n",
            "\n",
            "#### A9:2017-Using Components with Known Vulnerabilities\n",
            "\n",
            "_Epic:_\n",
            "\n",
            "While it is easy to find already-written exploits for many known vulnerabilities, other vulnerabilities require concentrated effort to develop a custom exploit.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I find common open source or closed source packages with weaknesses and perform attacks against vulnerabilities and exploits which are disclosed\n",
            "\n",
            "#### A10:2017-Insufficient Logging & Monitoring\n",
            "\n",
            "_Epic:_\n",
            "\n",
            "Exploitation of insufficient logging and monitoring is the bedrock of nearly every major incident. Attackers rely on the lack of monitoring and timely response to achieve their goals without being detected. In 2016, identifying a breach took an [average of 191 days](https://www-01.ibm.com/common/ssi/cgi-bin/ssialias?htmlfid=SEL03130WWEN) allowing substantial chance for damage to be inflicted.\n",
            "\n",
            "_Abuse Case:_\n",
            "\n",
            "As an attacker, I attack an organization and the logs, monitoring systems, and teams do not see or respond to my attacks.\n",
            "\n",
            "## Sources of the schemas\n",
            "\n",
            "All figures were created using <https://www.draw.io/> site and exported (as PNG image) for integration into this article.\n",
            "\n",
            "All XML descriptor files for each schema are available below (using XML description, modification of the schema is possible using DRAW.IO site):\n",
            "\n",
            "[Schemas descriptors archive](../assets/Abuse_Case_Cheat_Sheet_SchemaBundle.zip)\n",
            "' metadata={'source': 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Abuse_Case_Cheat_Sheet.md', 'doc_type': 'Abuse_Case_Cheat_Sheet.md', 'source_type': 'owasp_cheatsheet'}\n"
          ]
        }
      ],
      "source": [
        "print(owasp_docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Added 13 Sigma rule documents (khÃ´ng split)\n",
            "âœ… Added 216 MITRE ATT&CK documents (from 216 techniques)\n",
            "âœ… Added 3332 OWASP cheatsheet document chunks (from 107 docs)\n",
            "\n",
            "ðŸ“Š Tá»•ng cá»™ng: 3561 documents Ä‘á»ƒ embed vÃ  lÆ°u vÃ o ChromaDB\n",
            "\n",
            "ðŸ’¡ Chiáº¿n lÆ°á»£c splitting:\n",
            "   - Sigma rules: Giá»¯ nguyÃªn (khÃ´ng split)\n",
            "   - MITRE techniques: Giá»¯ nguyÃªn, chá»‰ split náº¿u >2000 chars\n",
            "   - OWASP cheatsheets: Split theo markdown headers vÃ  paragraphs\n"
          ]
        }
      ],
      "source": [
        "# Text Splitting vÃ  Combine All Documents\n",
        "# Tá»‘i Æ°u cho tá»«ng loáº¡i document: Sigma, MITRE, OWASP\n",
        "\n",
        "# 1. Text splitter cho OWASP (Markdown files - cÃ³ thá»ƒ ráº¥t dÃ i)\n",
        "# OWASP cheatsheets thÆ°á»ng lÃ  markdown, cáº§n split Ä‘á»ƒ quáº£n lÃ½\n",
        "owasp_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # PhÃ¹ há»£p cho markdown content\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n## \", \"\\n\\n### \", \"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Æ¯u tiÃªn split theo markdown headers\n",
        ")\n",
        "\n",
        "# 2. Text splitter cho MITRE (Format: T1548\\nName\\nDescription)\n",
        "# Giá»¯ nguyÃªn technique náº¿u cÃ³ thá»ƒ, chá»‰ split náº¿u quÃ¡ dÃ i\n",
        "mitre_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=800,  # Äá»§ lá»›n cho háº§u háº¿t techniques\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \"]  # Æ¯u tiÃªn giá»¯ nguyÃªn format ID\\nName\\nDescription\n",
        ")\n",
        "\n",
        "# Combine táº¥t cáº£ documents\n",
        "all_docs = []\n",
        "\n",
        "# =================================================================\n",
        "# 1. SIGMA RULES\n",
        "# =================================================================\n",
        "# Sigma rules Ä‘Ã£ Ä‘Æ°á»£c processed, format ngáº¯n gá»n, khÃ´ng cáº§n split\n",
        "if 'sigma_docs_processed' in locals() and sigma_docs_processed:\n",
        "    for doc in sigma_docs_processed:\n",
        "        # Äáº£m báº£o metadata cÃ³ source_type\n",
        "        if 'source_type' not in doc.metadata:\n",
        "            doc.metadata['source_type'] = 'sigma_rule'\n",
        "        all_docs.append(doc)\n",
        "    print(f\"âœ… Added {len(sigma_docs_processed)} Sigma rule documents (khÃ´ng split)\")\n",
        "\n",
        "# =================================================================\n",
        "# 2. MITRE ATT&CK TECHNIQUES\n",
        "# =================================================================\n",
        "# Format: T1548\\nName\\nDescription\n",
        "# Má»—i technique lÃ  má»™t document riÃªng, chá»‰ split náº¿u description quÃ¡ dÃ i\n",
        "if 'mitre_docs' in locals() and mitre_docs:\n",
        "    mitre_split = []\n",
        "    for doc in mitre_docs:\n",
        "        # Äáº£m báº£o metadata cÃ³ source_type\n",
        "        if 'source_type' not in doc.metadata:\n",
        "            doc.metadata['source_type'] = 'mitre_attack'\n",
        "        \n",
        "        # Náº¿u description quÃ¡ dÃ i (>2000 chars), split theo paragraph\n",
        "        if len(doc.page_content) > 2000:\n",
        "            chunks = mitre_splitter.split_documents([doc])\n",
        "            # Giá»¯ metadata cho má»—i chunk\n",
        "            for chunk in chunks:\n",
        "                chunk.metadata['source_type'] = 'mitre_attack'\n",
        "            mitre_split.extend(chunks)\n",
        "        else:\n",
        "            # Giá»¯ nguyÃªn document - má»—i technique lÃ  má»™t chunk\n",
        "            mitre_split.append(doc)\n",
        "    \n",
        "    all_docs.extend(mitre_split)\n",
        "    long_docs = sum(1 for doc in mitre_docs if len(doc.page_content) > 2000)\n",
        "    print(f\"âœ… Added {len(mitre_split)} MITRE ATT&CK documents (from {len(mitre_docs)} techniques)\")\n",
        "    if long_docs > 0:\n",
        "        print(f\"   - {long_docs} techniques Ä‘Ã£ Ä‘Æ°á»£c split do description quÃ¡ dÃ i\")\n",
        "\n",
        "# =================================================================\n",
        "# 3. OWASP CHEATSHEETS\n",
        "# =================================================================\n",
        "# OWASP cheatsheets lÃ  markdown files, cÃ³ thá»ƒ ráº¥t dÃ i, cáº§n split\n",
        "if 'owasp_docs' in locals() and owasp_docs:\n",
        "    owasp_split = []\n",
        "    for doc in owasp_docs:\n",
        "        # Äáº£m báº£o metadata cÃ³ source_type\n",
        "        if 'source_type' not in doc.metadata:\n",
        "            doc.metadata['source_type'] = 'owasp_cheatsheet'\n",
        "        \n",
        "        # Split OWASP documents (markdown cÃ³ thá»ƒ ráº¥t dÃ i)\n",
        "        chunks = owasp_splitter.split_documents([doc])\n",
        "        # Giá»¯ metadata cho má»—i chunk\n",
        "        for chunk in chunks:\n",
        "            chunk.metadata['source_type'] = 'owasp_cheatsheet'\n",
        "            # Giá»¯ nguyÃªn doc_type vÃ  source tá»« document gá»‘c\n",
        "            if 'doc_type' in doc.metadata:\n",
        "                chunk.metadata['doc_type'] = doc.metadata['doc_type']\n",
        "            if 'source' in doc.metadata:\n",
        "                chunk.metadata['source'] = doc.metadata['source']\n",
        "        owasp_split.extend(chunks)\n",
        "    \n",
        "    all_docs.extend(owasp_split)\n",
        "    print(f\"âœ… Added {len(owasp_split)} OWASP cheatsheet document chunks (from {len(owasp_docs)} docs)\")\n",
        "\n",
        "# =================================================================\n",
        "# Tá»”NG Káº¾T\n",
        "# =================================================================\n",
        "print(f\"\\nðŸ“Š Tá»•ng cá»™ng: {len(all_docs)} documents Ä‘á»ƒ embed vÃ  lÆ°u vÃ o ChromaDB\")\n",
        "print(f\"\\nðŸ’¡ Chiáº¿n lÆ°á»£c splitting:\")\n",
        "print(f\"   - Sigma rules: Giá»¯ nguyÃªn (khÃ´ng split)\")\n",
        "print(f\"   - MITRE techniques: Giá»¯ nguyÃªn, chá»‰ split náº¿u >2000 chars\")\n",
        "print(f\"   - OWASP cheatsheets: Split theo markdown headers vÃ  paragraphs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¥ Äang táº£i embedding model...\n",
            "âœ… Embedding model Ä‘Ã£ sáºµn sÃ ng\n"
          ]
        }
      ],
      "source": [
        "# Khá»Ÿi táº¡o embedding model (dÃ¹ng HuggingFace local)\n",
        "print(\"ðŸ“¥ Äang táº£i embedding model...\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "print(\"âœ… Embedding model Ä‘Ã£ sáºµn sÃ ng\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ Äang táº¡o/load ChromaDB táº¡i: D:\\MCPLLM\\test\\chroma_sec_db\n",
            "ðŸ“ Äang chuáº©n bá»‹ 3561 documents Ä‘á»ƒ embed...\n",
            "ðŸ“Š Thá»‘ng kÃª documents:\n",
            "   - Sigma rules: 13\n",
            "   - MITRE ATT&CK: 216\n",
            "   - OWASP cheatsheets: 3332\n",
            "\n",
            "ðŸ”„ Äang embed vÃ  lÆ°u vÃ o ChromaDB...\n",
            "âœ… ÄÃ£ lÆ°u 3561 documents vÃ o ChromaDB\n"
          ]
        }
      ],
      "source": [
        "# Táº¡o Ä‘Æ°á»ng dáº«n lÆ°u ChromaDB\n",
        "persist_directory = r\"D:\\MCPLLM\\test\\chroma_sec_db\"\n",
        "os.makedirs(persist_directory, exist_ok=True)\n",
        "\n",
        "# Chuáº©n bá»‹ documents vá»›i metadata tÆ°Æ¡ng thÃ­ch ChromaDB\n",
        "print(f\"ðŸ’¾ Äang táº¡o/load ChromaDB táº¡i: {persist_directory}\")\n",
        "\n",
        "# Kiá»ƒm tra all_docs Ä‘Ã£ Ä‘Æ°á»£c táº¡o chÆ°a\n",
        "if 'all_docs' not in locals() or not all_docs:\n",
        "    print(\"âš ï¸  ChÆ°a cÃ³ all_docs. HÃ£y cháº¡y cell 'Text Splitting vÃ  Combine All Documents' trÆ°á»›c.\")\n",
        "    print(\"    Äang dÃ¹ng sigma_docs_processed táº¡m thá»i...\")\n",
        "    docs_to_process = sigma_docs_processed if 'sigma_docs_processed' in locals() and sigma_docs_processed else []\n",
        "else:\n",
        "    docs_to_process = all_docs\n",
        "\n",
        "if not docs_to_process:\n",
        "    print(\"âŒ KhÃ´ng cÃ³ documents nÃ o Ä‘á»ƒ embed!\")\n",
        "    vectorstore = None\n",
        "else:\n",
        "    print(f\"ðŸ“ Äang chuáº©n bá»‹ {len(docs_to_process)} documents Ä‘á»ƒ embed...\")\n",
        "    \n",
        "    # Convert list/dict trong metadata thÃ nh string cho ChromaDB\n",
        "    docs_for_chroma = []\n",
        "    for doc in docs_to_process:\n",
        "        new_metadata = {}\n",
        "        for key, value in doc.metadata.items():\n",
        "            if isinstance(value, (list, dict)):\n",
        "                # Convert list/dict thÃ nh JSON string\n",
        "                new_metadata[key] = json.dumps(value, ensure_ascii=False)\n",
        "            elif value is None:\n",
        "                continue  # Skip None values\n",
        "            else:\n",
        "                new_metadata[key] = value\n",
        "        \n",
        "        # Táº¡o document má»›i vá»›i metadata Ä‘Ã£ xá»­ lÃ½\n",
        "        new_doc = Document(\n",
        "            page_content=doc.page_content,\n",
        "            metadata=new_metadata\n",
        "        )\n",
        "        docs_for_chroma.append(new_doc)\n",
        "    \n",
        "    # Filter complex metadata má»™t láº§n ná»¯a Ä‘á»ƒ cháº¯c cháº¯n\n",
        "    docs_for_chroma = filter_complex_metadata(docs_for_chroma)\n",
        "    \n",
        "    print(f\"ðŸ“Š Thá»‘ng kÃª documents:\")\n",
        "    if 'all_docs' in locals() and all_docs:\n",
        "        sigma_count = sum(1 for d in docs_for_chroma if d.metadata.get('source_type') == 'sigma_rule')\n",
        "        mitre_count = sum(1 for d in docs_for_chroma if d.metadata.get('source_type') == 'mitre_attack')\n",
        "        owasp_count = sum(1 for d in docs_for_chroma if d.metadata.get('source_type') == 'owasp_cheatsheet')\n",
        "        print(f\"   - Sigma rules: {sigma_count}\")\n",
        "        print(f\"   - MITRE ATT&CK: {mitre_count}\")\n",
        "        print(f\"   - OWASP cheatsheets: {owasp_count}\")\n",
        "    \n",
        "    # Táº¡o ChromaDB vector store\n",
        "    print(f\"\\nðŸ”„ Äang embed vÃ  lÆ°u vÃ o ChromaDB...\")\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=docs_for_chroma,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=persist_directory,\n",
        "        collection_name=\"security_knowledge_base\"  # TÃªn collection má»›i cho táº¥t cáº£\n",
        "    )\n",
        "    print(f\"âœ… ÄÃ£ lÆ°u {len(docs_for_chroma)} documents vÃ o ChromaDB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'vectorstore' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Láº¥y ra bá»™ sÆ°u táº­p vector tá»« vectorstore\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m collection = \u001b[43mvectorstore\u001b[49m._collection\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Láº¥y 1 embedding tá»« database\u001b[39;00m\n\u001b[32m      5\u001b[39m sample_embedding = collection.get(limit=\u001b[32m1\u001b[39m, include=[\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m])[\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m]\n",
            "\u001b[31mNameError\u001b[39m: name 'vectorstore' is not defined"
          ]
        }
      ],
      "source": [
        "# Láº¥y ra bá»™ sÆ°u táº­p vector tá»« vectorstore\n",
        "collection = vectorstore._collection\n",
        "\n",
        "# Láº¥y 1 embedding tá»« database\n",
        "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
        "\n",
        "# Kiá»ƒm tra sá»‘ chiá»u (sá»‘ pháº§n tá»­ trong vector)\n",
        "dimensions = len(sample_embedding)\n",
        "print(f\"The vectors have {dimensions:,} dimensions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Há»c Táº­p\\AppData\\Local\\Temp\\ipykernel_25616\\3005272459.py:12: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
            "  vs = Chroma(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Vectorstore ready.\n",
            "ðŸ“¦ Collection size (approx): 3561\n",
            "\n",
            "ðŸ”Ž Similarity search:\n",
            "[1] | Score: 0.6761 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "Source: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "SQL Injection attacks can be divided into the following three classes:  - **Inband:**Ã‚Â data is extracted using\n",
            "the same channel that is used to inject the SQL code. This is the most straightforward kind of attack, in\n",
            "which the retrieved data is presented directly in the application web page. - **Out-of-band:**Ã‚Â data is\n",
            "retrieved using a different channel (e.g., an email with the results of the que...\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "[2] | Score: 0.7689 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "Source: D:\\MCPLLM\\test\\cheatsheets\\SQL_Injection_Prevention_Cheat_Sheet.md\n",
            "## What Is a SQL Injection Attack?  Attackers can use SQL injection on an application if it has dynamic\n",
            "database queries that use string concatenation and user supplied input. To avoid SQL injection flaws,\n",
            "developers need to:  1. Stop writing dynamic queries with string concatenation or 2. Prevent malicious SQL\n",
            "input from being included in executed queries.  There are simple techniques for prevent...\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "[3] | Score: 0.7728 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "Source: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "An SQL injection attack consists of insertion or \"injection\" of either a partial or complete SQL query via the\n",
            "data input or transmitted from the client (browser) to the web application.  A successful SQL injection attack\n",
            "can read sensitive data from the database, modify database data (insert/update/delete), execute administration\n",
            "operations on the database (such as shutdown the DBMS), recover the...\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "[4] | Score: 0.7998 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "Source: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "### Query languages  The most famous form of injection is SQL Injection where an attacker can modify existing\n",
            "database queries. For more information see the [SQL Injection Prevention Cheat\n",
            "Sheet](SQL_Injection_Prevention_Cheat_Sheet.md).  But also LDAP, SOAP, XPath and REST based queries can be\n",
            "susceptible to injection attacks allowing for data retrieval or control bypass.  #### SQL Injection  An ...\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "[5] | Score: 0.8006 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "Source: D:\\MCPLLM\\test\\cheatsheets\\Virtual_Patching_Cheat_Sheet.md\n",
            "## Example Public Vulnerability  Let's take the following [SQL Injection\n",
            "vulnerability](https://packetstormsecurity.com/files/119217/WordPress-Shopping-Cart-8.1.14-Shell-Upload-SQL-\n",
            "Injection.html) as our example for the remainder of this article:  ```text WordPress Shopping Cart Plugin for\n",
            "WordPress /wp-content/plugins/levelfourstorefront/scripts/administration/exportsubscribers.php reqID\n",
            "Paramete...\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "ðŸ§­ MMR (Ä‘a dáº¡ng hoÃ¡ káº¿t quáº£):\n",
            "[1] | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "Source: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "SQL Injection attacks can be divided into the following three classes:  - **Inband:**Ã‚Â data is extracted using\n",
            "the same channel that is used to inject the SQL code. This is the most straightforward kind of attack, in\n",
            "which the retrieved data is presented directly in the application web page. - **Out-of-band:**Ã‚Â data is\n",
            "retrieved using a different channel (e.g., an email with the results of the que...\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "[2] | Title: SQL Injection Strings In URI | SourceType: sigma_rule\n",
            "Source: D:\\MCPLLM\\test\\sigma\\rules\\web\\webserver_generic\\web_sql_injection_in_access_logs.yml\n",
            "tags: [\"attack.initial-access\", \"attack.t1190\"]\n",
            "level: high\n",
            "Sigma Rule: SQL Injection Strings In URI Status: test | Level: high Description: Detects potential SQL\n",
            "injection attempts via GET requests in access logs. Keywords: @@version, %271%27%3D%271, =select  (+27 more)\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "[3] | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "Source: D:\\MCPLLM\\test\\cheatsheets\\Secure_Code_Review_Cheat_Sheet.md\n",
            "### Injection Vulnerabilities  **SQL Injection:**  Look for string concatenation in database queries and\n",
            "unsafe query construction. For more information, see [SQL Injection Prevention Cheat\n",
            "Sheet](SQL_Injection_Prevention_Cheat_Sheet.md).  **Cross-Site Scripting (XSS):**  Review output encoding, DOM\n",
            "manipulation, and user input rendering. For more information, see [Cross Site Scripting Prevention ...\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "[4] | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "Source: D:\\MCPLLM\\test\\cheatsheets\\Error_Handling_Cheat_Sheet.md\n",
            "note: The full stack trace of the root cause is available in the Apache Tomcat/7.0.56 logs. ```  Below is an\n",
            "example of disclosure of a SQL query error, along with the site installation path, that can be used to\n",
            "identify an injection point:  ```text Warning: odbc_fetch_array() expects parameter /1 to be resource, boolean\n",
            "given in D:\\app\\index_new.php on line 188 ```  The [OWASP Testing Guide](http...\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "[5] | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "Source: D:\\MCPLLM\\test\\cheatsheets\\SQL_Injection_Prevention_Cheat_Sheet.md\n",
            "### Allow-list Input Validation  In addition to being a primary defense when nothing else is possible (e.g.,\n",
            "when a bind variable isn't legal), input validation can also be a secondary defense used to detect\n",
            "unauthorized input before it is passed to the SQL query. For more information please see the [Input Validation\n",
            "Cheat Sheet](Input_Validation_Cheat_Sheet.md). Proceed with caution here. Validat...\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "[6] | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "Source: D:\\MCPLLM\\test\\cheatsheets\\SQL_Injection_Prevention_Cheat_Sheet.md\n",
            "### Least Privilege  To minimize the potential damage of a successful SQL injection attack, you should\n",
            "minimize the privileges assigned to every database account in your environment. Start from the ground up to\n",
            "determine what access rights your application accounts require, rather than trying to figure out what access\n",
            "rights you need to take away.  Make sure that accounts that only need read acces...\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "ðŸŽ¯ Filter theo metadata (vÃ­ dá»¥: chá»‰ Sigma rules):\n",
            "[1] | Score: 0.8505 | Title: SQL Injection Strings In URI | SourceType: sigma_rule\n",
            "Source: D:\\MCPLLM\\test\\sigma\\rules\\web\\webserver_generic\\web_sql_injection_in_access_logs.yml\n",
            "tags: [\"attack.initial-access\", \"attack.t1190\"]\n",
            "level: high\n",
            "Sigma Rule: SQL Injection Strings In URI Status: test | Level: high Description: Detects potential SQL\n",
            "injection attempts via GET requests in access logs. Keywords: @@version, %271%27%3D%271, =select  (+27 more)\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "[2] | Score: 1.2894 | Title: Cross Site Scripting Strings | SourceType: sigma_rule\n",
            "Source: D:\\MCPLLM\\test\\sigma\\rules\\web\\webserver_generic\\web_xss_in_access_logs.yml\n",
            "tags: [\"attack.initial-access\", \"attack.t1189\"]\n",
            "level: high\n",
            "Sigma Rule: Cross Site Scripting Strings Status: test | Level: high Description: Detects XSS attempts injected\n",
            "via GET requests in access logs Keywords: =<script>, =%3Cscript%3E, =%253Cscript%253E (+13 more)\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "[3] | Score: 1.3035 | Title: Suspicious Windows Strings In URI | SourceType: sigma_rule\n",
            "Source: D:\\MCPLLM\\test\\sigma\\rules\\web\\webserver_generic\\web_susp_windows_path_uri.yml\n",
            "tags: [\"attack.persistence\", \"attack.exfiltration\", \"attack.t1505.003\"]\n",
            "level: high\n",
            "Sigma Rule: Suspicious Windows Strings In URI Status: test | Level: high Description: Detects suspicious\n",
            "Windows strings in URI which could indicate possible exfiltration or webshell communication\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "[4] | Score: 1.3180 | Title: Server Side Template Injection Strings | SourceType: sigma_rule\n",
            "Source: D:\\MCPLLM\\test\\sigma\\rules\\web\\webserver_generic\\web_ssti_in_access_logs.yml\n",
            "tags: [\"attack.defense-evasion\", \"attack.t1221\"]\n",
            "level: high\n",
            "Sigma Rule: Server Side Template Injection Strings Status: test | Level: high Description: Detects SSTI\n",
            "attempts sent via GET requests in access logs Keywords: ={{, =%7B%7B, =${ (+7 more)\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "[5] | Score: 1.3561 | Title: Suspicious User-Agents Related To Recon Tools | SourceType: sigma_rule\n",
            "Source: D:\\MCPLLM\\test\\sigma\\rules\\web\\webserver_generic\\web_susp_useragents.yml\n",
            "tags: [\"attack.initial-access\", \"attack.t1190\"]\n",
            "level: medium\n",
            "Sigma Rule: Suspicious User-Agents Related To Recon Tools Status: test | Level: medium Description: Detects\n",
            "known suspicious (default) user-agents related to scanning/recon tools\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "âš¡ FlashRank top-5 (reranked):\n",
            "\n",
            "(â„¹ï¸ Bá» qua rerank FlashRank â€” chÆ°a cÃ i hoáº·c lá»—i nhá»: 'dict' object has no attribute 'document' )\n",
            "\n",
            "âœ… Done. Thay Ä‘á»•i biáº¿n `query` Ä‘á»ƒ thá»­ thÃªm.\n"
          ]
        }
      ],
      "source": [
        "# âœ… Test query Chroma vectorstore (similarity / MMR / filter theo metadata)\n",
        "import os, textwrap, json\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "def ensure_vectorstore(persist_dir, embeddings, collection=\"security_knowledge_base\"):\n",
        "    # Náº¿u Ä‘Ã£ cÃ³ vectorstore trong RAM thÃ¬ dÃ¹ng láº¡i\n",
        "    if 'vectorstore' in globals() and vectorstore is not None:\n",
        "        return vectorstore\n",
        "    # NgÆ°á»£c láº¡i, má»Ÿ tá»« á»• Ä‘Ä©a\n",
        "    vs = Chroma(\n",
        "        persist_directory=persist_dir,\n",
        "        collection_name=collection,\n",
        "        embedding_function=embeddings\n",
        "    )\n",
        "    return vs\n",
        "\n",
        "def pretty(doc, score=None, idx=None, width=110, preview=400):\n",
        "    h = []\n",
        "    if idx is not None:\n",
        "        h.append(f\"[{idx}]\")\n",
        "    if score is not None:\n",
        "        h.append(f\"Score: {score:.4f}\")\n",
        "    title = doc.metadata.get(\"title\") or doc.metadata.get(\"cheatsheet_name\") or doc.metadata.get(\"id\") or \"(no title)\"\n",
        "    src_type = doc.metadata.get(\"source_type\")\n",
        "    h.append(f\"Title: {title}\")\n",
        "    if src_type:\n",
        "        h.append(f\"SourceType: {src_type}\")\n",
        "    print(\" | \".join(h))\n",
        "    # nguá»“n\n",
        "    src = doc.metadata.get(\"source_url\") or doc.metadata.get(\"source\")\n",
        "    if src:\n",
        "        print(\"Source:\", src)\n",
        "    # tag/technique/tags tÃ³m táº¯t\n",
        "    for key in (\"tags\",\"technique_id\",\"technique_name\",\"level\",\"yaml_path\"):\n",
        "        if key in doc.metadata and doc.metadata[key]:\n",
        "            val = doc.metadata[key]\n",
        "            if isinstance(val, (list, dict)):\n",
        "                val = json.dumps(val, ensure_ascii=False)\n",
        "            print(f\"{key}:\", val)\n",
        "    # preview ná»™i dung\n",
        "    content = (doc.page_content or \"\").strip().replace(\"\\r\", \"\")\n",
        "    preview_text = content[:preview] + (\"...\" if len(content) > preview else \"\")\n",
        "    print(textwrap.fill(preview_text, width=width))\n",
        "    print(\"-\" * width)\n",
        "\n",
        "# ====== RUN ======\n",
        "if 'persist_directory' not in globals():\n",
        "    persist_directory = r\"D:\\MCPLLM\\test\\chroma_sec_db\"\n",
        "\n",
        "if 'embeddings' not in globals():\n",
        "    raise RuntimeError(\"ChÆ°a cÃ³ biáº¿n `embeddings`. HÃ£y khá»Ÿi táº¡o OpenAI/HF embeddings trÆ°á»›c.\")\n",
        "\n",
        "vs = ensure_vectorstore(persist_directory, embeddings, collection=\"security_knowledge_base\")\n",
        "print(\"âœ… Vectorstore ready.\")\n",
        "print(\"ðŸ“¦ Collection size (approx):\", vs._collection.count())  # sá»‘ vector (xáº¥p xá»‰)\n",
        "\n",
        "# ðŸ‘‰ Äáº·t cÃ¢u query báº¡n muá»‘n test\n",
        "query = \"Detected SQL injection\"\n",
        "k = 5\n",
        "\n",
        "print(\"\\nðŸ”Ž Similarity search:\")\n",
        "hits = vs.similarity_search_with_score(query, k=k)\n",
        "for i, (doc, score) in enumerate(hits, 1):\n",
        "    pretty(doc, score=score, idx=i)\n",
        "\n",
        "print(\"\\nðŸ§­ MMR (Ä‘a dáº¡ng hoÃ¡ káº¿t quáº£):\")\n",
        "mmr_docs = vs.max_marginal_relevance_search(query, k=min(6, k+1), fetch_k=20, lambda_mult=0.3)\n",
        "for i, doc in enumerate(mmr_docs, 1):\n",
        "    pretty(doc, score=None, idx=i)\n",
        "\n",
        "print(\"\\nðŸŽ¯ Filter theo metadata (vÃ­ dá»¥: chá»‰ Sigma rules):\")\n",
        "try:\n",
        "    # Chroma há»— trá»£ 'where' filter trÃªn metadata\n",
        "    filtered = vs.similarity_search_with_score(\n",
        "        query, k=k, filter={\"source_type\": \"sigma_rule\"}\n",
        "    )\n",
        "    if not filtered:\n",
        "        print(\"  (KhÃ´ng cÃ³ káº¿t quáº£ khá»›p filter {'source_type':'sigma_rule'})\")\n",
        "    else:\n",
        "        for i, (doc, score) in enumerate(filtered, 1):\n",
        "            pretty(doc, score=score, idx=i)\n",
        "except TypeError:\n",
        "    # Má»™t sá»‘ báº£n LangChain cÅ© dÃ¹ng 'where' thay vÃ¬ 'filter'\n",
        "    filtered = vs.similarity_search_with_score(\n",
        "        query, k=k, where={\"source_type\": \"sigma_rule\"}\n",
        "    )\n",
        "    for i, (doc, score) in enumerate(filtered, 1):\n",
        "        pretty(doc, score=score, idx=i)\n",
        "\n",
        "# (Tuá»³ chá»n) ðŸ” Re-rank báº±ng FlashRank náº¿u cÃ³\n",
        "try:\n",
        "    from flashrank import Ranker, RerankRequest\n",
        "    ranker = Ranker()  # máº·c Ä‘á»‹nh dÃ¹ng model nháº¹\n",
        "    # Láº¥y top 10 tá»« similarity Ä‘á»ƒ rerank\n",
        "    base_docs = vs.similarity_search(query, k=10)\n",
        "    passages = [d.page_content[:1000] for d in base_docs]\n",
        "    req = RerankRequest(query=query, passages=[{\"id\": str(i), \"text\": p} for i, p in enumerate(passages)])\n",
        "    rr = ranker.rerank(req)\n",
        "    print(\"\\nâš¡ FlashRank top-5 (reranked):\")\n",
        "    for r in rr[:5]:\n",
        "        d = base_docs[int(r.document[\"id\"])]\n",
        "        pretty(d, score=r.relevance_score, idx=r.index)\n",
        "except Exception as e:\n",
        "    print(\"\\n(â„¹ï¸ Bá» qua rerank FlashRank â€” chÆ°a cÃ i hoáº·c lá»—i nhá»:\", e, \")\")\n",
        "\n",
        "print(\"\\nâœ… Done. Thay Ä‘á»•i biáº¿n `query` Ä‘á»ƒ thá»­ thÃªm.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hybrid results for query: Defense XSS Attack\n",
            "Top 10:\n",
            "[1] Score: 0.3500 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "  Preview: - Cookie Attributes - These change how JavaScript and browsers can interact with cookies. Cookie attributes try to limit the impact of an XSS attack but donâ€™t prevent the execution of malicious content or address the root cause of the vulnerability. - Content Security Policy - An allowlist that prev\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[2] Score: 0.2907 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "  Preview: ### Summary  One final note: If deploying interceptors / filters as an XSS defense was a useful approach against XSS attacks, don't you think that it would be incorporated into all commercial Web Application Firewalls (WAFs) and be an approach that OWASP recommends in this cheat sheet?\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[3] Score: 0.2826 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "  Preview: ## Tests  This cheat sheet demonstrates that input filtering is an incomplete defense for XSS by supplying testers with a series of XSS attacks that can bypass certain XSS defensive filters.  ### Basic XSS Test Without Filter Evasion  This attack, which uses normal XSS JavaScript injection, serves a\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[4] Score: 0.2820 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "  Preview: Reflected and Stored XSS are server side injection issues while DOM based XSS is a client (browser) side injection issue.  All of this code originates on the server, which means it is the application owner's responsibility to make it safe from XSS, regardless of the type of XSS flaw it is. Also, XSS\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[5] Score: 0.2793 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "  Preview: ## XSS Defense Philosophy  In order for an XSS attack to be successful, an attacker must be able to insert and execute malicious content in a webpage. Thus, all variables in a web application needs to be protected. Ensuring that **all variables** go through validation and are then escaped or sanitiz\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[6] Score: 0.2704 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "  Preview: ## Attack vector  ![XS Leaks Attack Vector](../assets/XS_Attack_Vector.png)  - The entire attack takes place on the victim's browser side - just like an XSS attack - In some cases, the victim must remain on the attacker's site longer for the attack to succeed.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[7] Score: 0.2678 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "  Preview: ## Common Anti-patterns: Ineffective Approaches to Avoid  Defending against XSS is hard. For that reason, some have sought shortcuts to preventing XSS.  We're going to examine two common [anti-patterns](https://en.wikipedia.org/wiki/Anti-pattern) that frequently show up in ancient posts, but are sti\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[8] Score: 0.2654 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "  Preview: ## Methods to Bypass WAF â€“ Cross-Site Scripting  ### General issues  #### Stored XSS  If an attacker managed to push XSS through the filter, WAF wouldnâ€™t be able to prevent the attack conduction.  #### Reflected XSS in JavaScript  Example:  ```js <script> ... setTimeout(\\\\\"writetitle()\\\\\",$\\_GET\\[xs\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[9] Score: 0.2309 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "  Preview: ### XML Data Island with CDATA Obfuscation  This XSS attack works only in IE:  ```html <XMLÂ ID=\"xss\"><I><B><IMGÂ SRC=\"javas<!--Â -->cript:alert('XSS')\"></B></I></XML> <SPANÂ DATASRC=\"#xss\"Â DATAFLD=\"B\"Â DATAFORMATAS=\"HTML\"></SPAN> ```  ### Locally hosted XML with embedded JavaScript that is generated usi\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[10] Score: 0.2276 | Title: (no title) | SourceType: owasp_cheatsheet\n",
            "  Preview: ## Defense in Depth & Metrics  While not a specific technique, it is important to implement defenses that consider the impact of individual defenses being defeated or otherwise failing.  As an example, client-side defenses, such as device fingerprinting or JavaScript challenges, may be spoofed or by\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "(Äiá»u chá»‰nh alpha Ä‘á»ƒ Æ°u tiÃªn dense/sparse)\n"
          ]
        }
      ],
      "source": [
        "# Hybrid search = Dense (Chroma) + Sparse (BM25) combiner\n",
        "import sys, subprocess, math\n",
        "from typing import List, Tuple\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Náº¿u rank_bm25 chÆ°a cÃ i thÃ¬ cÃ i (cell sáº½ lá»—i náº¿u khÃ´ng cÃ³)\n",
        "try:\n",
        "    import rank_bm25  # noqa\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rank-bm25\"])\n",
        "    from rank_bm25 import BM25Okapi  # noqa\n",
        "\n",
        "# Kiá»ƒm tra tiá»n Ä‘iá»u kiá»‡n\n",
        "if 'vs' not in globals() and 'vectorstore' not in globals():\n",
        "    raise RuntimeError(\"ChÆ°a cÃ³ vectorstore (vs). HÃ£y load/khá»Ÿi táº¡o Chroma trÆ°á»›c.\")\n",
        "\n",
        "# prefer vs variable\n",
        "_vs = vs if 'vs' in globals() else vectorstore\n",
        "\n",
        "if 'docs_for_chroma' not in globals():\n",
        "    raise RuntimeError(\"Cáº§n biáº¿n `docs_for_chroma` (list of Documents) Ä‘á»ƒ xÃ¢y BM25. HÃ£y táº¡o nÃ³ khi build Chroma.\")\n",
        "\n",
        "# Chuáº©n bá»‹ corpus cho BM25 (Ä‘Æ¡n giáº£n: page_content tokenized)\n",
        "corpus_texts = [ (d.page_content or \"\").replace(\"\\n\",\" \") for d in docs_for_chroma ]\n",
        "# Tokenize: simple whitespace + lowercase; báº¡n cÃ³ thá»ƒ Ä‘Æ°a tokenizer tá»‘t hÆ¡n\n",
        "tokenized_corpus = [ txt.lower().split() for txt in corpus_texts ]\n",
        "bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "def normalize_scores(d: dict):\n",
        "    \"\"\"Min-max normalize in-place for dict key->score\"\"\"\n",
        "    if not d:\n",
        "        return {}\n",
        "    vals = list(d.values())\n",
        "    lo, hi = min(vals), max(vals)\n",
        "    if lo == hi:\n",
        "        # all equal -> give 1.0\n",
        "        return {k: 1.0 for k in d}\n",
        "    return {k: (v - lo) / (hi - lo) for k, v in d.items()}\n",
        "\n",
        "def hybrid_search(\n",
        "    query: str,\n",
        "    k_dense: int = 10,\n",
        "    k_sparse: int = 10,\n",
        "    alpha: float = 0.6\n",
        ") -> List[Tuple[object, float]]:\n",
        "    \"\"\"\n",
        "    alpha: weight for dense score (0..1). final_score = alpha * dense_norm + (1-alpha) * sparse_norm\n",
        "    returns list of (Document, combined_score) sorted desc\n",
        "    \"\"\"\n",
        "    # 1) Dense (Chroma) top-k\n",
        "    dense_hits = _vs.similarity_search_with_score(query, k=k_dense)\n",
        "    # dense_hits: list of (Document, score). Depending on implementation score may be distance or similarity.\n",
        "    # We'll assume larger score = more similar (as seen earlier). If you see scores like distances, invert them.\n",
        "    dense_scores = {}\n",
        "    for doc, s in dense_hits:\n",
        "        key = id(doc)  # unique key\n",
        "        dense_scores[key] = float(s)\n",
        "    \n",
        "    # 2) Sparse (BM25)\n",
        "    qtok = query.lower().split()\n",
        "    bm25_scores_arr = bm25.get_scores(qtok)  # length = len(corpus)\n",
        "    sparse_scores = {}\n",
        "    for idx, sc in enumerate(bm25_scores_arr):\n",
        "        if sc <= 0:\n",
        "            continue\n",
        "        # key mapping must correspond to docs_for_chroma order\n",
        "        key = id(docs_for_chroma[idx])\n",
        "        sparse_scores[key] = float(sc)\n",
        "    \n",
        "    # 3) Normalize each score map\n",
        "    dense_norm = normalize_scores(dense_scores)\n",
        "    sparse_norm = normalize_scores(sparse_scores)\n",
        "    \n",
        "    # 4) Merge keys and compute combined score\n",
        "    keys = set(dense_norm.keys()) | set(sparse_norm.keys())\n",
        "    combined = {}\n",
        "    for k in keys:\n",
        "        dv = dense_norm.get(k, 0.0)\n",
        "        sv = sparse_norm.get(k, 0.0)\n",
        "        combined[k] = alpha * dv + (1 - alpha) * sv\n",
        "    \n",
        "    # 5) create list of (Document, score) and sort\n",
        "    results = []\n",
        "    # map id->doc for lookup\n",
        "    id2doc = { id(docs_for_chroma[i]) : docs_for_chroma[i] for i in range(len(docs_for_chroma)) }\n",
        "    for k, sc in sorted(combined.items(), key=lambda x: x[1], reverse=True):\n",
        "        doc = id2doc.get(k)\n",
        "        if doc:\n",
        "            results.append((doc, sc))\n",
        "    return results\n",
        "\n",
        "# pretty printer (re-use earlier pretty if exists)\n",
        "def pretty_result(doc, score, idx=None):\n",
        "    title = doc.metadata.get(\"title\") or doc.metadata.get(\"cheatsheet_name\") or doc.metadata.get(\"id\") or \"(no title)\"\n",
        "    print(f\"[{idx}] Score: {score:.4f} | Title: {title} | SourceType: {doc.metadata.get('source_type')}\")\n",
        "    print(\"  Preview:\", (doc.page_content or \"\")[:300].replace(\"\\n\",\" \"))\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "# ===== Example usage =====\n",
        "q = \"Defense XSS Attack\"\n",
        "res = hybrid_search(q, k_dense=10, k_sparse=50, alpha=0.65)\n",
        "\n",
        "print(f\"Hybrid results for query: {q}\\nTop {min(10,len(res))}:\")\n",
        "for i, (doc, sc) in enumerate(res[:10], 1):\n",
        "    pretty_result(doc, sc, idx=i)\n",
        "\n",
        "# Tweak alpha: alpha closer to 1 => favor dense; closer to 0 => favor BM25\n",
        "print(\"\\n(Äiá»u chá»‰nh alpha Ä‘á»ƒ Æ°u tiÃªn dense/sparse)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Dense-only (Chroma) (top 10) ===\n",
            "[1] score=0.6764 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "     ### Query languages The most famous form of injection is SQL Injection where an attacker can modify [...]\n",
            "[2] score=0.6849 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "     SQL Injection attacks can be divided into the following three classes: - **Inband:**Ã‚ data is extracted [...]\n",
            "[3] score=0.7527 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "     An SQL injection attack consists of insertion or \"injection\" of either a partial or complete SQL query [...]\n",
            "[4] score=0.7770 | SQL Injection Strings In URI | sigma_rule\n",
            "    src: D:\\MCPLLM\\test\\sigma\\rules\\web\\webserver_generic\\web_sql_injection_in_access_logs.yml\n",
            "     Sigma Rule: SQL Injection Strings In URI Status: test | Level: high Description: Detects potential SQL [...]\n",
            "[5] score=0.8080 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\SQL_Injection_Prevention_Cheat_Sheet.md\n",
            "     # SQL Injection Prevention Cheat Sheet ## Introduction This cheat sheet will help you prevent SQL [...]\n",
            "[6] score=0.8137 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "     ## Forms of Injection There are several forms of injection targeting different technologies including [...]\n",
            "[7] score=0.8267 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\SQL_Injection_Prevention_Cheat_Sheet.md\n",
            "     ## What Is a SQL Injection Attack? Attackers can use SQL injection on an application if it has dynamic [...]\n",
            "[8] score=0.8279 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "     Equally Static Code Analysis Data flow rules can detect of unsanitized user controlled input can change [...]\n",
            "[9] score=0.8310 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Laravel_Cheat_Sheet.md\n",
            "     ### Validation Rule SQL Injection Certain validation rules have the option of providing database column [...]\n",
            "[10] score=0.8391 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "     #### LDAP Injection LDAP Injection is an attack used to exploit web based applications that construct [...]\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "=== Hybrid (Dense+BM25, alpha=0.60) (top 10) ===\n",
            "[1] score=0.9704 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Laravel_Cheat_Sheet.md\n",
            "     ### Validation Rule SQL Injection Certain validation rules have the option of providing database column [...]\n",
            "[2] score=0.9223 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\SQL_Injection_Prevention_Cheat_Sheet.md\n",
            "     ## What Is a SQL Injection Attack? Attackers can use SQL injection on an application if it has dynamic [...]\n",
            "[3] score=0.8614 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "     Equally Static Code Analysis Data flow rules can detect of unsanitized user controlled input can change [...]\n",
            "[4] score=0.8547 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\SQL_Injection_Prevention_Cheat_Sheet.md\n",
            "     # SQL Injection Prevention Cheat Sheet ## Introduction This cheat sheet will help you prevent SQL [...]\n",
            "[5] score=0.8442 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "     #### LDAP Injection LDAP Injection is an attack used to exploit web based applications that construct [...]\n",
            "[6] score=0.7777 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "     ## Forms of Injection There are several forms of injection targeting different technologies including [...]\n",
            "[7] score=0.7117 | SQL Injection Strings In URI | sigma_rule\n",
            "    src: D:\\MCPLLM\\test\\sigma\\rules\\web\\webserver_generic\\web_sql_injection_in_access_logs.yml\n",
            "     Sigma Rule: SQL Injection Strings In URI Status: test | Level: high Description: Detects potential SQL [...]\n",
            "[8] score=0.5782 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "     An SQL injection attack consists of insertion or \"injection\" of either a partial or complete SQL query [...]\n",
            "[9] score=0.3416 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Injection_Prevention_Cheat_Sheet.md\n",
            "     ### Query languages The most famous form of injection is SQL Injection where an attacker can modify [...]\n",
            "[10] score=0.3328 | (no title) | owasp_cheatsheet\n",
            "    src: D:\\MCPLLM\\test\\cheatsheets\\Query_Parameterization_Cheat_Sheet.md\n",
            "     # Query Parameterization Cheat Sheet ## Introduction [SQL Injection](https://owasp.org/www- [...]\n",
            "--------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# --- Stable key builders (thay vÃ¬ dÃ¹ng id(doc)) ---\n",
        "import hashlib, re\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "def doc_key(d):\n",
        "    \"\"\"KhÃ³a á»•n Ä‘á»‹nh tá»« metadata + hash 256 kÃ½ tá»± Ä‘áº§u ná»™i dung.\"\"\"\n",
        "    meta = d.metadata or {}\n",
        "    src = meta.get(\"source_url\") or meta.get(\"source\") or \"\"\n",
        "    title = meta.get(\"title\") or meta.get(\"cheatsheet_name\") or meta.get(\"id\") or \"\"\n",
        "    aux = meta.get(\"yaml_path\") or meta.get(\"technique_id\") or meta.get(\"technique_name\") or \"\"\n",
        "    head = (d.page_content or \"\")[:256]\n",
        "    h = hashlib.md5(head.encode(\"utf-8\",\"ignore\")).hexdigest()[:8]\n",
        "    return f\"{src}|{title}|{aux}|{h}\"\n",
        "\n",
        "def payload_tokenize(text: str):\n",
        "    text = (text or \"\").lower()\n",
        "    return [t for t in re.findall(r\"[a-z0-9_]+|[\\$\\{\\}\\|\\&\\;\\=\\.\\:/\\\\'\\\"][\\$\\{\\}\\|\\&\\;\\=\\.\\:/\\\\'\\\"0-9a-z_]*\", text)\n",
        "            if len(t) > 1 or t in (\"'\", '\"', \"/\", \"=\", \".\")]\n",
        "\n",
        "def bm25_build_corpus(docs):\n",
        "    corpus = []\n",
        "    keys = []\n",
        "    for d in docs:\n",
        "        meta_bits = []\n",
        "        for k in (\"title\",\"tags\",\"yaml_path\",\"technique_id\",\"technique_name\",\"cheatsheet_name\"):\n",
        "            v = d.metadata.get(k)\n",
        "            if v is None: \n",
        "                continue\n",
        "            if isinstance(v, list):\n",
        "                v = \" \".join(map(str, v))\n",
        "            elif isinstance(v, dict):\n",
        "                v = \" \".join(f\"{ik}:{iv}\" for ik, iv in v.items())\n",
        "            meta_bits.append(str(v))\n",
        "        blob = \" \\n \".join([d.page_content or \"\"] + meta_bits)\n",
        "        corpus.append(blob)\n",
        "        keys.append(doc_key(d))\n",
        "    return corpus, keys\n",
        "\n",
        "# Build BM25 + key map má»™t láº§n\n",
        "corpus_texts, bm25_keys = bm25_build_corpus(docs_for_chroma)\n",
        "bm25 = BM25Okapi([payload_tokenize(t) for t in corpus_texts])\n",
        "\n",
        "# Map stable key -> Document gá»‘c (tá»« docs_for_chroma)\n",
        "key2doc = { doc_key(d): d for d in docs_for_chroma }\n",
        "\n",
        "def _minmax(d):\n",
        "    if not d: return {}\n",
        "    vals = list(d.values()); lo, hi = min(vals), max(vals)\n",
        "    if hi == lo: return {k: 1.0 for k in d}  # táº¥t cáº£ báº±ng nhau\n",
        "    return {k: (v - lo) / (hi - lo) for k, v in d.items()}\n",
        "\n",
        "def hybrid_search(query: str, k_dense=10, k_sparse=80, alpha=0.60):\n",
        "    # 1) Dense tá»« Chroma (dÃ¹ng stable key)\n",
        "    dense_hits = vs.similarity_search_with_score(query, k=k_dense)\n",
        "    # náº¿u tháº¥y score lÃ  \"khoáº£ng cÃ¡ch\" (nhá» tá»‘t), Ä‘áº£o dáº¥u: s = -float(score)\n",
        "    dense_scores = {}\n",
        "    for d, s in dense_hits:\n",
        "        k = doc_key(d)\n",
        "        dense_scores[k] = float(s)\n",
        "\n",
        "    # 2) Sparse tá»« BM25 (map theo bm25_keys)\n",
        "    qtok = payload_tokenize(query)\n",
        "    sparse_arr = bm25.get_scores(qtok)  # Ä‘iá»ƒm cho toÃ n corpus\n",
        "    sparse_scores = { bm25_keys[i]: float(sc) for i, sc in enumerate(sparse_arr) if sc > 0 }\n",
        "\n",
        "    # 3) Chuáº©n hoÃ¡ & trá»™n\n",
        "    dn, sn = _minmax(dense_scores), _minmax(sparse_scores)\n",
        "    keys = set(dn) | set(sn)\n",
        "    combo = {k: alpha*dn.get(k,0.0) + (1-alpha)*sn.get(k,0.0) for k in keys}\n",
        "\n",
        "    # 4) Tráº£ káº¿t quáº£ (Document, score) báº±ng key2doc\n",
        "    results = []\n",
        "    for k, sc in sorted(combo.items(), key=lambda x: x[1], reverse=True):\n",
        "        d = key2doc.get(k)\n",
        "        if d:\n",
        "            results.append((d, sc))\n",
        "    return results\n",
        "\n",
        "def show_results(title, results, top=10, width=110, preview=360):\n",
        "    import textwrap\n",
        "    print(f\"\\n=== {title} (top {top}) ===\")\n",
        "    for i, (doc, sc) in enumerate(results[:top], 1):\n",
        "        met = doc.metadata\n",
        "        name = met.get(\"title\") or met.get(\"cheatsheet_name\") or met.get(\"id\") or \"(no title)\"\n",
        "        src = met.get(\"source_url\") or met.get(\"source\") or met.get(\"doc_type\")\n",
        "        print(f\"[{i}] score={sc:.4f} | {name} | {met.get('source_type')}\")\n",
        "        if src: print(\"    src:\", src)\n",
        "        snippet = (doc.page_content or \"\").replace(\"\\n\",\" \")[:preview]\n",
        "        print(\"    \", textwrap.shorten(snippet, width=width))\n",
        "    print(\"-\"*width)\n",
        "\n",
        "# Thá»­ láº¡i\n",
        "query = \"What is Rule Detection of SQL injection?\"\n",
        "dense_hits = vs.similarity_search_with_score(query, k=10)\n",
        "show_results(\"Dense-only (Chroma)\", dense_hits)\n",
        "\n",
        "hyb_hits = hybrid_search(query, k_dense=10, k_sparse=80, alpha=0.60)\n",
        "show_results(\"Hybrid (Dense+BM25, alpha=0.55)\", hyb_hits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Chroma loaded: D:\\MCPLLM\\test\\chroma_sec_db | Vectors ~ 3561\n",
            "â„¹ï¸ BM25 corpus from anchors: 766 docs\n",
            "âœ… BM25 ready on 766 docs\n",
            "\n",
            "ðŸ”Ž Demo analyze_log()...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Káº¾T LUáº¬N ===\n",
            "{\n",
            "  \"attack\": \"Unknown\",\n",
            "  \"mitre\": \"\",\n",
            "  \"confidence\": 0.0,\n",
            "  \"rationale\": \"KhÃ´ng cÃ³ báº±ng chá»©ng rÃµ rÃ ng vá» táº¥n cÃ´ng\",\n",
            "  \"sources\": [],\n",
            "  \"hypothesis\": \"SQL Injection\",\n",
            "  \"hyp_mitre\": \"T1190\",\n",
            "  \"asked\": [\n",
            "    \"CÃ³ yÃªu cáº§u HTTP nÃ o chá»©a kÃ½ tá»± Ä‘áº·c biá»‡t nhÆ° ';', '--', hoáº·c 'UNION' khÃ´ng?\",\n",
            "    \"Liá»‡u cÃ³ thá»ƒ xáº£y ra lá»—i cÆ¡ sá»Ÿ dá»¯ liá»‡u khi nháº­p dá»¯ liá»‡u khÃ´ng?\",\n",
            "    \"CÃ³ dáº¥u hiá»‡u cá»§a viá»‡c thá»±c thi lá»‡nh SQL khÃ´ng mong muá»‘n khÃ´ng?\"\n",
            "  ],\n",
            "  \"top_sources\": [\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Session_Management_Cheat_Sheet.md\",\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\REST_Assessment_Cheat_Sheet.md\",\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\OS_Command_Injection_Defense_Cheat_Sheet.md\",\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Cross_Site_Scripting_Prevention_Cheat_Sheet.md\",\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Injection_Prevention_Cheat_Sheet.md\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "=== CHI TIáº¾T (Má»–I GIáº¢ THUYáº¾T) ===\n",
            "\n",
            "[1] hyp=SQL Injection -> attack=Unknown | conf=0.0\n",
            "  asked: [\"CÃ³ yÃªu cáº§u HTTP nÃ o chá»©a kÃ½ tá»± Ä‘áº·c biá»‡t nhÆ° ';', '--', hoáº·c 'UNION' khÃ´ng?\", 'Liá»‡u cÃ³ thá»ƒ xáº£y ra lá»—i cÆ¡ sá»Ÿ dá»¯ liá»‡u khi nháº­p dá»¯ liá»‡u khÃ´ng?', 'CÃ³ dáº¥u hiá»‡u cá»§a viá»‡c thá»±c thi lá»‡nh SQL khÃ´ng mong muá»‘n khÃ´ng?']\n",
            "  sources: ['D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Session_Management_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\REST_Assessment_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\OS_Command_Injection_Defense_Cheat_Sheet.md']\n",
            "\n",
            "[2] hyp=Cross-Site Scripting (XSS) -> attack=Unknown | conf=0.0\n",
            "  asked: ['CÃ³ yÃªu cáº§u HTTP nÃ o chá»©a mÃ£ JavaScript khÃ´ng?', 'Liá»‡u cÃ³ thá»ƒ xáº£y ra viá»‡c thá»±c thi mÃ£ Ä‘á»™c trÃªn trÃ¬nh duyá»‡t ngÆ°á»i dÃ¹ng khÃ´ng?', 'CÃ³ dáº¥u hiá»‡u cá»§a viá»‡c chÃ¨n mÃ£ Ä‘á»™c vÃ o trang web khÃ´ng?']\n",
            "  sources: ['D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\XSS_Filter_Evasion_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Session_Management_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Cross_Site_Scripting_Prevention_Cheat_Sheet.md']\n",
            "\n",
            "[3] hyp=Command Injection -> attack=Unknown | conf=0.0\n",
            "  asked: ['CÃ³ yÃªu cáº§u HTTP nÃ o chá»©a lá»‡nh há»‡ thá»‘ng khÃ´ng?', 'Liá»‡u cÃ³ thá»ƒ xáº£y ra viá»‡c thá»±c thi lá»‡nh há»‡ thá»‘ng khÃ´ng mong muá»‘n khÃ´ng?', 'CÃ³ dáº¥u hiá»‡u cá»§a viá»‡c truy cáº­p trÃ¡i phÃ©p vÃ o há»‡ thá»‘ng khÃ´ng?']\n",
            "  sources: ['D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\XSS_Filter_Evasion_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Password_Storage_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Input_Validation_Cheat_Sheet.md']\n",
            "\n",
            "[4] hyp=Brute Force -> attack=Unknown | conf=0.0\n",
            "  asked: ['CÃ³ nhiá»u yÃªu cáº§u Ä‘Äƒng nháº­p tháº¥t báº¡i tá»« cÃ¹ng má»™t IP khÃ´ng?', 'Liá»‡u cÃ³ thá»ƒ xáº£y ra viá»‡c táº¥n cÃ´ng tá»« chá»‘i dá»‹ch vá»¥ khÃ´ng?', 'CÃ³ dáº¥u hiá»‡u cá»§a viá»‡c thá»­ nghiá»‡m máº­t kháº©u khÃ´ng?']\n",
            "  sources: ['D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Kubernetes_Security_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Credential_Stuffing_Prevention_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Session_Management_Cheat_Sheet.md']\n",
            "\n",
            "[5] hyp=Local File Inclusion (LFI) -> attack=Unknown | conf=0.0\n",
            "  asked: ['CÃ³ yÃªu cáº§u HTTP nÃ o chá»©a Ä‘Æ°á»ng dáº«n file cá»¥c bá»™ khÃ´ng?', 'Liá»‡u cÃ³ thá»ƒ xáº£y ra viá»‡c truy cáº­p trÃ¡i phÃ©p vÃ o file há»‡ thá»‘ng khÃ´ng?', 'CÃ³ dáº¥u hiá»‡u cá»§a viá»‡c Ä‘á»c file khÃ´ng mong muá»‘n khÃ´ng?']\n",
            "  sources: ['D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Laravel_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\XSS_Filter_Evasion_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\CI_CD_Security_Cheat_Sheet.md']\n"
          ]
        }
      ],
      "source": [
        "# =============== LLM + RAG HYBRID cho LOG (LangChain + Groq) ===============\n",
        "# - LLM: ChatGroq (GROQ_API_KEY tá»« biáº¿n mÃ´i trÆ°á»ng)\n",
        "# - VectorDB: Chroma (dense) + BM25 (sparse) = Hybrid\n",
        "# - Pipeline:\n",
        "#   (1) LLM sinh giáº£ thuyáº¿t & cÃ¢u há»i bá»• sung\n",
        "#   (2) Tá»± query retriever (hybrid) theo tá»«ng cÃ¢u há»i\n",
        "#   (3) LLM \"Judge\" káº¿t luáº­n attack + MITRE + confidence + nguá»“n\n",
        "\n",
        "import os, sys, subprocess, json, re, hashlib, textwrap\n",
        "\n",
        "# -------- 0) CÃ€I / IMPORT GÃ“I Cáº¦N THIáº¾T --------\n",
        "def _pip_install(pkgs):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ pip install error:\", e)\n",
        "\n",
        "# ThÆ° viá»‡n LangChain cáº§n\n",
        "try:\n",
        "    from langchain_groq import ChatGroq\n",
        "except Exception:\n",
        "    _pip_install([\"langchain-groq\"])\n",
        "    from langchain_groq import ChatGroq\n",
        "\n",
        "try:\n",
        "    from langchain_core.messages import SystemMessage, HumanMessage\n",
        "    from langchain_core.runnables import RunnableLambda\n",
        "except Exception:\n",
        "    _pip_install([\"langchain-core>=0.2.0\"])\n",
        "    from langchain_core.messages import SystemMessage, HumanMessage\n",
        "    from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "try:\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "except Exception:\n",
        "    _pip_install([\"langchain-community\", \"chromadb\"])\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "\n",
        "try:\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "except Exception:\n",
        "    _pip_install([\"langchain-huggingface\", \"sentence-transformers\"])\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# BM25 cho hybrid\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi\n",
        "except Exception:\n",
        "    _pip_install([\"rank-bm25\"])\n",
        "    from rank_bm25 import BM25Okapi\n",
        "\n",
        "\n",
        "# -------- 1) THIáº¾T Láº¬P LLM GROQ --------\n",
        "# Äáº·t key vÃ o biáº¿n mÃ´i trÆ°á»ng trÆ°á»›c khi cháº¡y: os.environ[\"GROQ_API_KEY\"] = \"...\"\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    print(\"âš ï¸  Thiáº¿u GROQ_API_KEY trong biáº¿n mÃ´i trÆ°á»ng. Äáº·t os.environ['GROQ_API_KEY']='<key>' trÆ°á»›c khi gá»i.\")\n",
        "# Model gá»£i Ã½: \"llama-3.3-70b-versatile\" hoáº·c \"mixtral-8x7b-32768\"\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
        "\n",
        "# -------- 2) Má»ž / KHá»žI Táº O VECTORSTORE & EMBEDDINGS --------\n",
        "# Embedding MiniLM (nhÆ° báº¡n dÃ¹ng)\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "# ThÆ° má»¥c Chroma Ä‘Ã£ persist tá»« cell trÆ°á»›c cá»§a báº¡n\n",
        "persist_directory = r\"D:\\MCPLLM\\test\\chroma_sec_db\"\n",
        "\n",
        "# Má»Ÿ láº¡i Chroma náº¿u cÃ³, ngÆ°á»£c láº¡i bÃ¡o gá»£i Ã½\n",
        "try:\n",
        "    vs = Chroma(\n",
        "        persist_directory=persist_directory,\n",
        "        collection_name=\"security_knowledge_base\",\n",
        "        embedding_function=embeddings\n",
        "    )\n",
        "    print(\"âœ… Chroma loaded:\", persist_directory, \"| Vectors ~\", vs._collection.count())\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ KhÃ´ng má»Ÿ Ä‘Æ°á»£c Chroma:\", e)\n",
        "    vs = None\n",
        "\n",
        "# -------- 3) HYBRID RETRIEVER (Dense Chroma + Sparse BM25) --------\n",
        "# Cáº§n danh sÃ¡ch Document gá»‘c Ä‘á»ƒ build BM25. Náº¿u báº¡n váº«n giá»¯ biáº¿n docs_for_chroma thÃ¬ dÃ¹ng, cÃ²n khÃ´ng sáº½ build sparse \"lazy\" tá»« DB.\n",
        "\n",
        "def payload_tokenize(text: str):\n",
        "    \"\"\"Tokenizer giá»¯ kÃ½ tá»± Ä‘áº·c biá»‡t cho payload SOC.\"\"\"\n",
        "    text = (text or \"\").lower()\n",
        "    tokens = re.findall(r\"[a-z0-9_]+|[\\$\\{\\}\\|\\&\\;\\=\\.\\:/\\\\'\\\"][\\$\\{\\}\\|\\&\\;\\=\\.\\:/\\\\'\\\"0-9a-z_]*\", text)\n",
        "    return [t for t in tokens if len(t) > 1 or t in (\"'\", '\"', \"/\", \"=\", \".\")]\n",
        "\n",
        "def doc_key(d):\n",
        "    \"\"\"Key á»•n Ä‘á»‹nh cho mapping (metadata + hash).\"\"\"\n",
        "    meta = getattr(d, \"metadata\", {}) or {}\n",
        "    src = meta.get(\"source_url\") or meta.get(\"source\") or \"\"\n",
        "    title = meta.get(\"title\") or meta.get(\"cheatsheet_name\") or meta.get(\"id\") or \"\"\n",
        "    aux = meta.get(\"yaml_path\") or meta.get(\"technique_id\") or meta.get(\"technique_name\") or \"\"\n",
        "    head = (getattr(d, \"page_content\", \"\") or \"\")[:256]\n",
        "    h = hashlib.md5(head.encode(\"utf-8\",\"ignore\")).hexdigest()[:8]\n",
        "    return f\"{src}|{title}|{aux}|{h}\"\n",
        "\n",
        "def _minmax(d):\n",
        "    if not d: return {}\n",
        "    vals = list(d.values()); lo, hi = min(vals), max(vals)\n",
        "    if hi == lo: return {k: 1.0 for k in d}\n",
        "    return {k: (v - lo) / (hi - lo) for k, v in d.items()}\n",
        "\n",
        "# Build BM25 corpus tá»« docs_for_chroma náº¿u cÃ³; náº¿u khÃ´ng, láº¥y N máº«u tá»« DB\n",
        "if 'docs_for_chroma' in globals() and docs_for_chroma:\n",
        "    # Ä‘Ã£ cÃ³ docs_for_chroma tá»« pipeline embed\n",
        "    sparse_docs = docs_for_chroma\n",
        "else:\n",
        "    # fallback: láº¥y ngáº«u nhiÃªn 2000 docs tá»« Chroma (náº¿u driver há»— trá»£). Náº¿u khÃ´ng, láº¥y báº±ng similarity vá»›i vÃ i anchor query.\n",
        "    sparse_docs = []\n",
        "    if vs is not None:\n",
        "        try:\n",
        "            # Chroma khÃ´ng cÃ³ API \"load all docs\" tiÃªu chuáº©n qua LangChain;\n",
        "            # dÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch khá»Ÿi táº¡o sparse tá»« má»™t sá»‘ truy váº¥n anchor Ä‘á»ƒ cÃ³ corpus Ä‘á»§ tá»‘t.\n",
        "            anchors = [\n",
        "                \"sql injection OR 1=1\", \"xss tag breaking\", \"command injection ; | &&\",\n",
        "                \"F5 iControl bash endpoint\", \"MITRE T1003 LSASS\", \"path traversal ../../etc/passwd\",\n",
        "                \"confluence cve-2022-26134\", \"java payload ${@java}\", \"waf rule xss\", \"authentication brute force\"\n",
        "            ]\n",
        "            captured = {}\n",
        "            for a in anchors:\n",
        "                for d in vs.similarity_search(a, k=100):\n",
        "                    k = doc_key(d)\n",
        "                    if k not in captured:\n",
        "                        captured[k] = d\n",
        "            sparse_docs = list(captured.values())\n",
        "            print(f\"â„¹ï¸ BM25 corpus from anchors: {len(sparse_docs)} docs\")\n",
        "        except Exception as e:\n",
        "            print(\"âš ï¸ KhÃ´ng thá»ƒ dá»±ng corpus BM25 tá»± Ä‘á»™ng:\", e)\n",
        "            sparse_docs = []\n",
        "\n",
        "def bm25_build_corpus(docs):\n",
        "    corpus, keys = [], []\n",
        "    for d in docs:\n",
        "        meta_bits = []\n",
        "        for k in (\"title\",\"tags\",\"yaml_path\",\"technique_id\",\"technique_name\",\"cheatsheet_name\",\"level\"):\n",
        "            v = (d.metadata or {}).get(k)\n",
        "            if v is None: \n",
        "                continue\n",
        "            if isinstance(v, list):\n",
        "                v = \" \".join(map(str, v))\n",
        "            elif isinstance(v, dict):\n",
        "                v = \" \".join(f\"{ik}:{iv}\" for ik, iv in v.items())\n",
        "            meta_bits.append(str(v))\n",
        "        blob = \" \\n \".join([getattr(d, \"page_content\", \"\") or \"\"] + meta_bits)\n",
        "        corpus.append(blob)\n",
        "        keys.append(doc_key(d))\n",
        "    return corpus, keys\n",
        "\n",
        "bm25, bm25_keys, key2doc = None, [], {}\n",
        "if sparse_docs:\n",
        "    corpus_texts, bm25_keys = bm25_build_corpus(sparse_docs)\n",
        "    bm25 = BM25Okapi([payload_tokenize(t) for t in corpus_texts])\n",
        "    key2doc = { doc_key(d): d for d in sparse_docs }\n",
        "    print(\"âœ… BM25 ready on\", len(sparse_docs), \"docs\")\n",
        "else:\n",
        "    print(\"âš ï¸ BM25 chÆ°a sáºµn sÃ ng (thiáº¿u corpus). Hybrid sáº½ rÆ¡i vá» dense-only.\")\n",
        "\n",
        "def hybrid_search(query: str, k_dense=10, k_sparse=80, alpha=0.60):\n",
        "    \"\"\"Tráº£ list[(Document, score)] theo Ä‘iá»ƒm trá»™n hybrid. Náº¿u thiáº¿u BM25, tráº£ dense-only.\"\"\"\n",
        "    # Dense tá»« Chroma\n",
        "    dense_hits = vs.similarity_search_with_score(query, k=k_dense) if vs is not None else []\n",
        "    dense_scores = {}\n",
        "    for d, s in dense_hits:\n",
        "        # náº¿u score lÃ  khoáº£ng cÃ¡ch (nhá» tá»‘t), cÃ³ thá»ƒ Ä‘áº£o dáº¥u: s = -float(s)\n",
        "        dense_scores[doc_key(d)] = float(s)\n",
        "\n",
        "    if bm25 is None:\n",
        "        # Dense-only\n",
        "        return [(d, s) for d, s in dense_hits]\n",
        "\n",
        "    # Sparse tá»« BM25\n",
        "    qtok = payload_tokenize(query)\n",
        "    sparse_arr = bm25.get_scores(qtok)\n",
        "    sparse_scores = { bm25_keys[i]: float(sc) for i, sc in enumerate(sparse_arr) if sc > 0 }\n",
        "\n",
        "    # Normalize & trá»™n\n",
        "    dn, sn = _minmax(dense_scores), _minmax(sparse_scores)\n",
        "    keys = set(dn) | set(sn)\n",
        "    combo = {k: alpha*dn.get(k,0.0) + (1-alpha)*sn.get(k,0.0) for k in keys}\n",
        "\n",
        "    # Map key -> doc\n",
        "    results = []\n",
        "    # Æ°u tiÃªn láº¥y doc tá»« vs (dense) náº¿u cÃ³, sau Ä‘Ã³ tá»« key2doc (sparse)\n",
        "    dense_map = { doc_key(d): d for d, _ in dense_hits }\n",
        "    for k, sc in sorted(combo.items(), key=lambda x: x[1], reverse=True):\n",
        "        d = dense_map.get(k) or key2doc.get(k)\n",
        "        if d:\n",
        "            results.append((d, sc))\n",
        "    return results\n",
        "\n",
        "\n",
        "# -------- 4) CHUá»–I LLM: SINH CÃ‚U Há»ŽI & JUDGE Káº¾T LUáº¬N --------\n",
        "SYSTEM_HYP = \"\"\"Báº¡n lÃ  L2 SOC analyst. Nháº­n EVENT (log thÃ´) vÃ :\n",
        "1) Sinh tá»‘i Ä‘a 5 giáº£ thuyáº¿t kiá»ƒu táº¥n cÃ´ng cÃ³ thá»ƒ (vd: SQL Injection, XSS, LFI, Command Injection, Brute Force...)\n",
        "2) Vá»›i má»—i giáº£ thuyáº¿t, sinh 2â€“4 cÃ¢u há»i ngáº¯n Ä‘á»ƒ truy KB nháº±m tÃ¬m báº±ng chá»©ng.\n",
        "3) Tráº£ JSON: {\"hypotheses\":[{\"name\":\"...\", \"mitre\":\"Txxxx\", \"questions\":[\"...\", \"...\"]}, ...]}.\n",
        "Chá»‰ tráº£ JSON há»£p lá»‡, khÃ´ng giáº£i thÃ­ch thÃªm.\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_JUDGE = \"\"\"Báº¡n lÃ  bá»™ cháº¥m Ä‘iá»ƒm báº±ng chá»©ng. Dá»±a trÃªn CONTEXT (cÃ¡c Ä‘oáº¡n KB Ä‘Ã£ truy há»“i) vÃ  EVENT (log thÃ´):\n",
        "- Káº¿t luáº­n {\"attack\":\"...\", \"mitre\":\"Txxxx\", \"confidence\":0..1, \"rationale\":\"...\", \"sources\":[...]}\n",
        "- Chá»‰ chá»n attack náº¿u cÃ³ báº±ng chá»©ng rÃµ (pattern/rule/step khá»›p). Náº¿u khÃ´ng Ä‘á»§ báº±ng chá»©ng, tráº£ {\"attack\":\"Unknown\", \"mitre\":\"\", \"confidence\":0.0, ...}\n",
        "Chá»‰ tráº£ JSON há»£p lá»‡.\n",
        "\"\"\"\n",
        "\n",
        "def llm_json(llm, system_prompt, user_content, max_retries=2):\n",
        "    \"\"\"Gá»i LLM mong muá»‘n JSON thuáº§n.\"\"\"\n",
        "    for _ in range(max_retries+1):\n",
        "        resp = llm.invoke([SystemMessage(content=system_prompt),\n",
        "                           HumanMessage(content=user_content)])\n",
        "        txt = (resp.content or \"\").strip()\n",
        "        # TÃ¬m block JSON\n",
        "        try:\n",
        "            # Náº¿u cÃ³ text rÃ¡c, báº¯t cáº·p {} Ä‘áº§u tiÃªn\n",
        "            start = txt.find(\"{\")\n",
        "            end = txt.rfind(\"}\")\n",
        "            if start >= 0 and end > start:\n",
        "                return json.loads(txt[start:end+1])\n",
        "        except Exception:\n",
        "            continue\n",
        "    raise ValueError(\"LLM khÃ´ng tráº£ JSON há»£p lá»‡:\\n\" + txt)\n",
        "\n",
        "def retrieve_evidence_for_questions(questions, topk=5, alpha=0.60):\n",
        "    \"\"\"Hybrid retrieve cho list cÃ¢u há»i, tráº£ danh sÃ¡ch Ä‘oáº¡n báº±ng chá»©ng (text + metadata).\"\"\"\n",
        "    evid = []\n",
        "    for q in questions:\n",
        "        hits = hybrid_search(q, k_dense=topk, k_sparse=80, alpha=alpha)\n",
        "        for d, sc in hits:\n",
        "            evid.append({\n",
        "                \"text\": d.page_content[:1500],\n",
        "                \"meta\": d.metadata,\n",
        "                \"score\": float(sc)\n",
        "            })\n",
        "    # lá»c trÃ¹ng theo source_url + yaml_path\n",
        "    seen = set(); uniq = []\n",
        "    for e in evid:\n",
        "        m = e[\"meta\"] or {}\n",
        "        sig = (m.get(\"source_url\") or m.get(\"source\") or \"\") + \"|\" + str(m.get(\"yaml_path\") or \"\")\n",
        "        if sig in seen: \n",
        "            continue\n",
        "        seen.add(sig)\n",
        "        uniq.append(e)\n",
        "    return uniq[:20]\n",
        "\n",
        "def analyze_log(event_text: str, alpha=0.60):\n",
        "    \"\"\"Pipeline: sinh cÃ¢u há»i -> retrieve -> judge -> tráº£ káº¿t quáº£ + evidence sá»­ dá»¥ng.\"\"\"\n",
        "    # 1) Sinh giáº£ thuyáº¿t + cÃ¢u há»i\n",
        "    hyp_json = llm_json(llm, SYSTEM_HYP, f\"EVENT:\\n{event_text}\")\n",
        "    hypotheses = hyp_json.get(\"hypotheses\", [])[:5]\n",
        "\n",
        "    # 2) Vá»›i má»—i giáº£ thuyáº¿t: retrieve báº±ng chá»©ng\n",
        "    hyp_results = []\n",
        "    for hyp in hypotheses:\n",
        "        qs = hyp.get(\"questions\", [])[:4]\n",
        "        evid = retrieve_evidence_for_questions(qs, topk=5, alpha=alpha)\n",
        "        # Soáº¡n context cho judge (rÃºt gá»n giá»¯ nguá»“n)\n",
        "        ctx_blocks = []\n",
        "        for e in evid[:10]:\n",
        "            meta = e[\"meta\"] or {}\n",
        "            src = meta.get(\"source_url\") or meta.get(\"source\") or \"\"\n",
        "            head = (e[\"text\"] or \"\")[:600]\n",
        "            ctx_blocks.append(f\"SOURCE: {src}\\n{head}\")\n",
        "        ctx = \"\\n\\n---\\n\\n\".join(ctx_blocks)\n",
        "\n",
        "        # 3) Judge káº¿t luáº­n\n",
        "        judge_in = f\"CONTEXT:\\n{ctx}\\n\\nEVENT:\\n{event_text}\\n\"\n",
        "        judge_json = llm_json(llm, SYSTEM_JUDGE, judge_in)\n",
        "        judge_json[\"hypothesis\"] = hyp.get(\"name\")\n",
        "        judge_json[\"hyp_mitre\"] = hyp.get(\"mitre\",\"\")\n",
        "        judge_json[\"asked\"] = qs\n",
        "        # lÆ°u top nguá»“n\n",
        "        judge_json[\"top_sources\"] = [ (e[\"meta\"].get(\"source_url\") or e[\"meta\"].get(\"source\") or \"\") for e in evid[:5] ]\n",
        "        hyp_results.append(judge_json)\n",
        "\n",
        "    # 4) Chá»n káº¿t quáº£ tá»‘t nháº¥t theo confidence\n",
        "    best = max(hyp_results, key=lambda x: x.get(\"confidence\", 0.0), default={\"attack\":\"Unknown\",\"confidence\":0.0})\n",
        "    return {\n",
        "        \"event\": event_text,\n",
        "        \"results\": hyp_results,\n",
        "        \"best\": best\n",
        "    }\n",
        "\n",
        "# ---------------- DEMO ----------------\n",
        "demo_log = \"\"\"What is SQL injection?\"\"\"\n",
        "\n",
        "print(\"\\nðŸ”Ž Demo analyze_log()...\")\n",
        "try:\n",
        "    out = analyze_log(demo_log, alpha=0.60)  # alpha: Æ°u tiÃªn dense ~60%, sparse ~40%\n",
        "    print(\"\\n=== Káº¾T LUáº¬N ===\")\n",
        "    print(json.dumps(out[\"best\"], ensure_ascii=False, indent=2))\n",
        "    print(\"\\n=== CHI TIáº¾T (Má»–I GIáº¢ THUYáº¾T) ===\")\n",
        "    for i, r in enumerate(out[\"results\"], 1):\n",
        "        print(f\"\\n[{i}] hyp={r.get('hypothesis')} -> attack={r.get('attack')} | conf={r.get('confidence')}\")\n",
        "        print(\"  asked:\", r.get(\"asked\"))\n",
        "        print(\"  sources:\", r.get(\"top_sources\")[:3])\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ Demo lá»—i nhá»:\", e)\n",
        "    print(\"Kiá»ƒm tra: GROQ_API_KEY, Chroma persist_directory, vÃ  dá»¯ liá»‡u Ä‘Ã£ embed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ChatGroq client initialized.\n",
            "âœ… Chroma loaded: D:\\MCPLLM\\test\\chroma_sec_db | vectors: 3561\n",
            "â„¹ï¸ BM25 corpus from anchors: 923 docs\n",
            "âœ… BM25 ready on 923 docs\n",
            "\n",
            "ðŸ”Ž RAG-QA DEMO\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- QUESTION ---\n",
            " So sÃ¡nh XSS Reflected vá»›i Stored, cÃ¡ch phÃ¡t hiá»‡n vÃ  rule Sigma máº«u?\n",
            "\n",
            "--- ANSWER (markdown) ---\n",
            " XSS Reflected vÃ  Stored lÃ  hai loáº¡i táº¥n cÃ´ng Cross-Site Scripting (XSS) khÃ¡c nhau:\n",
            "\n",
            "- **XSS Reflected**: LÃ  loáº¡i táº¥n cÃ´ng xáº£y ra khi á»©ng dá»¥ng web nháº­n dá»¯ liá»‡u tá»« ngÆ°á»i dÃ¹ng vÃ  pháº£n Ã¡nh láº¡i trÃªn trang web mÃ  khÃ´ng kiá»ƒm tra hoáº·c lá»c dá»¯ liá»‡u. Äiá»u nÃ y cho phÃ©p káº» táº¥n cÃ´ng tiÃªm mÃ£ Ä‘á»™c vÃ o trang web thÃ´ng qua cÃ¡c yÃªu cáº§u HTTP. VÃ­ dá»¥, náº¿u má»™t trang web cÃ³ má»™t biá»ƒu máº«u tÃ¬m kiáº¿m vÃ  khÃ´ng kiá»ƒm tra dá»¯ liá»‡u nháº­p vÃ o, káº» táº¥n cÃ´ng cÃ³ thá»ƒ tiÃªm mÃ£ Ä‘á»™c vÃ o trÆ°á»ng tÃ¬m kiáº¿m vÃ  khi ngÆ°á»i dÃ¹ng nháº¥p vÃ o liÃªn káº¿t, mÃ£ Ä‘á»™c sáº½ Ä‘Æ°á»£c thá»±c thi.\n",
            "\n",
            "- **XSS Stored**: LÃ  loáº¡i táº¥n cÃ´ng xáº£y ra khi dá»¯ liá»‡u tá»« ngÆ°á»i dÃ¹ng Ä‘Æ°á»£c lÆ°u trá»¯ trÃªn mÃ¡y chá»§ vÃ  sau Ä‘Ã³ Ä‘Æ°á»£c hiá»ƒn thá»‹ trÃªn trang web mÃ  khÃ´ng Ä‘Æ°á»£c kiá»ƒm tra hoáº·c lá»c. Äiá»u nÃ y cho phÃ©p káº» táº¥n cÃ´ng lÆ°u trá»¯ mÃ£ Ä‘á»™c trÃªn mÃ¡y chá»§ vÃ  khi ngÆ°á»i dÃ¹ng truy cáº­p trang web, mÃ£ Ä‘á»™c sáº½ Ä‘Æ°á»£c thá»±c thi.\n",
            "\n",
            "Äá»ƒ phÃ¡t hiá»‡n cÃ¡c táº¥n cÃ´ng XSS, cÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p sau:\n",
            "\n",
            "- **Kiá»ƒm tra mÃ£ nguá»“n**: Kiá»ƒm tra mÃ£ nguá»“n cá»§a trang web Ä‘á»ƒ tÃ¬m cÃ¡c lá»— há»•ng báº£o máº­t.\n",
            "- **Sá»­ dá»¥ng cÃ´ng cá»¥ quÃ©t lá»— há»•ng**: Sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ quÃ©t lá»— há»•ng nhÆ° OWASP ZAP, Burp Suite Ä‘á»ƒ tÃ¬m cÃ¡c lá»— há»•ng báº£o máº­t.\n",
            "- **Kiá»ƒm tra nháº­t kÃ½ truy cáº­p**: Kiá»ƒm tra nháº­t kÃ½ truy cáº­p cá»§a mÃ¡y chá»§ Ä‘á»ƒ tÃ¬m cÃ¡c yÃªu cáº§u HTTP báº¥t thÆ°á»ng.\n",
            "\n",
            "Rule Sigma máº«u cho phÃ¡t hiá»‡n XSS Reflected:\n",
            "```yml\n",
            "title: Cross Site Scripting Strings\n",
            "status: test\n",
            "level: high\n",
            "description: Detects XSS attempts injected via GET requests in access logs\n",
            "keywords:\n",
            "  - =<script>\n",
            "  - =%3Cscript%3E\n",
            "  - =%253Cscript%253E\n",
            "...\n",
            "```\n",
            "Rule nÃ y sáº½ phÃ¡t hiá»‡n cÃ¡c yÃªu cáº§u HTTP cÃ³ chá»©a chuá»—i `<script>` hoáº·c cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³, cÃ³ thá»ƒ chá»‰ ra má»™t táº¥n cÃ´ng XSS Reflected.\n",
            "\n",
            "MITRE ID liÃªn quan: T1056 (Phishing), T1064 (Scripting).\n",
            "\n",
            "Nguá»“n:\n",
            "- OWASP: [XSS Filter Evasion Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/XSS_Filter_Evasion_Cheat_Sheet.html)\n",
            "- OWASP: [Cross-Site Scripting (XSS)](https://owasp.org/www-community/attacks/xss/)\n",
            "- Sigma Rule: [Cross Site Scripting Strings](https://github.com/SigmaHQ/sigma/blob/master/rules/web/webserver_generic/web_xss_in_access_logs.yml)\n",
            "\n",
            "--- SUBQUESTIONS ---\n",
            " ['XSS Reflected lÃ  gÃ¬?', 'XSS Stored lÃ  gÃ¬?', 'Sá»± khÃ¡c biá»‡t giá»¯a XSS Reflected vÃ  XSS Stored', 'CÃ¡ch phÃ¡t hiá»‡n XSS Reflected vÃ  Stored', 'Rule Sigma máº«u cho viá»‡c phÃ¡t hiá»‡n XSS Reflected vÃ  Stored']\n",
            "\n",
            "--- SOURCES ---\n",
            " ['D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\XSS_Filter_Evasion_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\DOM_based_XSS_Prevention_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\HTTP_Headers_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Abuse_Case_Cheat_Sheet.md', 'D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Content_Security_Policy_Cheat_Sheet.md']\n",
            "\n",
            "\n",
            "ðŸ”Ž LOG-ANALYZE DEMO\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== BEST ===\n",
            " {\n",
            "  \"attack\": \"Path Traversal\",\n",
            "  \"mitre\": \"T1083\",\n",
            "  \"confidence\": 0.8,\n",
            "  \"rationale\": \"YÃªu cáº§u GET Ä‘áº¿n Ä‘Æ°á»ng dáº«n /../../etc.conf cho tháº¥y má»™t ná»— lá»±c Ä‘á»ƒ truy cáº­p file há»‡ thá»‘ng báº±ng cÃ¡ch sá»­ dá»¥ng traversal directory, liÃªn quan Ä‘áº¿n khÃ¡m phÃ¡ vÃ  truy cáº­p file trÃªn há»‡ thá»‘ng má»¥c tiÃªu.\",\n",
            "  \"sources\": [\n",
            "    \"https://attack.mitre.org/techniques/T1083\"\n",
            "  ],\n",
            "  \"hypothesis\": \"Path Traversal\",\n",
            "  \"hyp_mitre\": \"T1005\",\n",
            "  \"asked\": [\n",
            "    \"CÃ³ dáº¥u hiá»‡u truy cáº­p file há»‡ thá»‘ng khÃ´ng?\",\n",
            "    \"Liá»‡u yÃªu cáº§u GET cÃ³ thá»ƒ truy cáº­p Ä‘Æ°á»£c file ngoÃ i thÆ° má»¥c gá»‘c khÃ´ng?\",\n",
            "    \"CÃ³ lá»—i nÃ o liÃªn quan Ä‘áº¿n viá»‡c kiá»ƒm soÃ¡t Ä‘Æ°á»ng dáº«n khÃ´ng?\"\n",
            "  ],\n",
            "  \"top_sources\": [\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\DotNet_Security_Cheat_Sheet.md\",\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Denial_of_Service_Cheat_Sheet.md\",\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Input_Validation_Cheat_Sheet.md\",\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Password_Storage_Cheat_Sheet.md\",\n",
            "    \"https://attack.mitre.org/techniques/T1222\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "=== ALL HYPOTHESES ===\n",
            "- Path Traversal -> Path Traversal | conf= 0.8\n",
            "- Directory Traversal -> Path Traversal | conf= 0.8\n",
            "- LFI (Local File Inclusion) -> Directory Traversal | conf= 0.8\n",
            "- Information Disclosure -> Path Traversal | conf= 0.8\n",
            "- Unauthorized Access -> Directory Traversal | conf= 0.7\n"
          ]
        }
      ],
      "source": [
        "# ================= FULL CELL: RAG-HYBRID QA + Log-Analysis (LangChain + Groq) =================\n",
        "# Copy/paste toÃ n bá»™ cell nÃ y vÃ o Jupyter. Chá»‰nh GROQ_API_KEY vÃ  persist_directory trÆ°á»›c khi cháº¡y.\n",
        "import os, sys, subprocess, json, re, hashlib, textwrap, random, time\n",
        "\n",
        "# --------------------- helper: pip install khi cáº§n ---------------------\n",
        "def _pip_install(pkgs):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ pip install error:\", e)\n",
        "\n",
        "# CÃ¡c import chÃ­nh (vá»›i fallback cÃ i Ä‘áº·t)\n",
        "try:\n",
        "    from langchain_groq import ChatGroq\n",
        "except Exception:\n",
        "    _pip_install([\"langchain-groq\"])\n",
        "    from langchain_groq import ChatGroq\n",
        "\n",
        "try:\n",
        "    from langchain_core.messages import SystemMessage, HumanMessage\n",
        "    from langchain_core.runnables import RunnableLambda\n",
        "except Exception:\n",
        "    _pip_install([\"langchain-core>=0.2.0\"])\n",
        "    from langchain_core.messages import SystemMessage, HumanMessage\n",
        "    from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "try:\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "except Exception:\n",
        "    _pip_install([\"langchain-community\", \"chromadb\"])\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "\n",
        "try:\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "except Exception:\n",
        "    _pip_install([\"langchain-huggingface\", \"sentence-transformers\"])\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "try:\n",
        "    from rank_bm25 import BM25Okapi\n",
        "except Exception:\n",
        "    _pip_install([\"rank-bm25\"])\n",
        "    from rank_bm25 import BM25Okapi\n",
        "\n",
        "# --------------------- Cáº¥u hÃ¬nh: GROQ key & Chroma path ---------------------\n",
        "# Äáº·t biáº¿n mÃ´i trÆ°á»ng trÆ°á»›c khi cháº¡y (hoáº·c gÃ¡n trá»±c tiáº¿p á»Ÿ Ä‘Ã¢y)\n",
        "# VÃ­ dá»¥: os.environ[\"GROQ_API_KEY\"] = \"sk-....\"\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    print(\"âš ï¸ Thiáº¿u GROQ_API_KEY trong biáº¿n mÃ´i trÆ°á»ng. HÃ£y Ä‘áº·t: os.environ['GROQ_API_KEY']='<key>' trÆ°á»›c khi cháº¡y.\")\n",
        "# Chroma persist directory (chá»‰nh theo mÃ´i trÆ°á»ng cá»§a báº¡n)\n",
        "persist_directory = r\"D:\\MCPLLM\\test\\chroma_sec_db\"  # <-- chá»‰nh Ä‘Æ°á»ng dáº«n nÃ y\n",
        "\n",
        "# --------------------- Khá»Ÿi táº¡o LLM ---------------------\n",
        "# Model gá»£i Ã½: \"llama-3.3-70b-versatile\" hoáº·c adapt theo key/model báº¡n cÃ³\n",
        "try:\n",
        "    llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
        "    print(\"âœ… ChatGroq client initialized.\")\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ KhÃ´ng khá»Ÿi táº¡o Ä‘Æ°á»£c ChatGroq:\", e)\n",
        "    llm = None\n",
        "\n",
        "# --------------------- Embeddings + Chroma ---------------------\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "vs = None\n",
        "try:\n",
        "    vs = Chroma(\n",
        "        persist_directory=persist_directory,\n",
        "        collection_name=\"security_knowledge_base\",\n",
        "        embedding_function=embeddings\n",
        "    )\n",
        "    # Tries to show approximate count if API exposes it\n",
        "    try:\n",
        "        count = vs._collection.count()\n",
        "    except Exception:\n",
        "        count = \"unknown\"\n",
        "    print(\"âœ… Chroma loaded:\", persist_directory, \"| vectors:\", count)\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ KhÃ´ng má»Ÿ Ä‘Æ°á»£c Chroma:\", e)\n",
        "    vs = None\n",
        "\n",
        "# --------------------- BM25 corpus builder & tokenizer ---------------------\n",
        "def payload_tokenize(text: str):\n",
        "    text = (text or \"\").lower()\n",
        "    tokens = re.findall(r\"[a-z0-9_]+|[\\$\\{\\}\\|\\&\\;\\=\\.\\:/\\\\'\\\"][\\$\\{\\}\\|\\&\\;\\=\\.\\:/\\\\'\\\"0-9a-z_]*\", text)\n",
        "    return [t for t in tokens if len(t) > 1 or t in (\"'\", '\"', \"/\", \"=\", \".\")]\n",
        "\n",
        "def doc_key(d):\n",
        "    meta = getattr(d, \"metadata\", {}) or {}\n",
        "    src = meta.get(\"source_url\") or meta.get(\"source\") or \"\"\n",
        "    title = meta.get(\"title\") or meta.get(\"cheatsheet_name\") or meta.get(\"id\") or \"\"\n",
        "    aux = meta.get(\"yaml_path\") or meta.get(\"technique_id\") or meta.get(\"technique_name\") or \"\"\n",
        "    head = (getattr(d, \"page_content\", \"\") or \"\")[:256]\n",
        "    h = hashlib.md5(head.encode(\"utf-8\",\"ignore\")).hexdigest()[:8]\n",
        "    return f\"{src}|{title}|{aux}|{h}\"\n",
        "\n",
        "def _minmax(d):\n",
        "    if not d: return {}\n",
        "    vals = list(d.values()); lo, hi = min(vals), max(vals)\n",
        "    if hi == lo: return {k: 1.0 for k in d}\n",
        "    return {k: (v - lo) / (hi - lo) for k, v in d.items()}\n",
        "\n",
        "# Build sparse_docs from Chroma anchors if docs_for_chroma missing\n",
        "sparse_docs = []\n",
        "if 'docs_for_chroma' in globals() and docs_for_chroma:\n",
        "    sparse_docs = docs_for_chroma\n",
        "else:\n",
        "    if vs is not None:\n",
        "        try:\n",
        "            anchors = [\n",
        "                \"sql injection OR 1=1\", \"xss tag breaking\", \"command injection ; | &&\",\n",
        "                \"F5 iControl bash endpoint\", \"MITRE T1003 LSASS\", \"path traversal ../../etc/passwd\",\n",
        "                \"confluence cve-2022-26134\", \"java payload ${@java}\", \"waf rule xss\", \"authentication brute force\",\n",
        "                \"sigma rule example\", \"mitre t1059\", \"cve exploit example\"\n",
        "            ]\n",
        "            captured = {}\n",
        "            for a in anchors:\n",
        "                try:\n",
        "                    hits = vs.similarity_search(a, k=100)\n",
        "                except Exception:\n",
        "                    hits = []\n",
        "                for d in hits:\n",
        "                    k = doc_key(d)\n",
        "                    if k not in captured:\n",
        "                        captured[k] = d\n",
        "            sparse_docs = list(captured.values())\n",
        "            print(f\"â„¹ï¸ BM25 corpus from anchors: {len(sparse_docs)} docs\")\n",
        "        except Exception as e:\n",
        "            print(\"âš ï¸ KhÃ´ng thá»ƒ dá»±ng corpus BM25 tá»± Ä‘á»™ng:\", e)\n",
        "            sparse_docs = []\n",
        "    else:\n",
        "        print(\"âš ï¸ Chroma khÃ´ng sáºµn sÃ ng, sáº½ cháº¡y dense-only.\")\n",
        "\n",
        "def bm25_build_corpus(docs):\n",
        "    corpus, keys = [], []\n",
        "    for d in docs:\n",
        "        meta_bits = []\n",
        "        for k in (\"title\",\"tags\",\"yaml_path\",\"technique_id\",\"technique_name\",\"cheatsheet_name\",\"level\"):\n",
        "            v = (d.metadata or {}).get(k)\n",
        "            if v is None:\n",
        "                continue\n",
        "            if isinstance(v, list):\n",
        "                v = \" \".join(map(str, v))\n",
        "            elif isinstance(v, dict):\n",
        "                v = \" \".join(f\"{ik}:{iv}\" for ik, iv in v.items())\n",
        "            meta_bits.append(str(v))\n",
        "        blob = \" \\n \".join([getattr(d, \"page_content\", \"\") or \"\"] + meta_bits)\n",
        "        corpus.append(blob)\n",
        "        keys.append(doc_key(d))\n",
        "    return corpus, keys\n",
        "\n",
        "bm25, bm25_keys, key2doc = None, [], {}\n",
        "if sparse_docs:\n",
        "    corpus_texts, bm25_keys = bm25_build_corpus(sparse_docs)\n",
        "    bm25 = BM25Okapi([payload_tokenize(t) for t in corpus_texts])\n",
        "    key2doc = { doc_key(d): d for d in sparse_docs }\n",
        "    print(\"âœ… BM25 ready on\", len(sparse_docs), \"docs\")\n",
        "else:\n",
        "    print(\"âš ï¸ BM25 chÆ°a sáºµn sÃ ng (thiáº¿u corpus). Hybrid sáº½ rÆ¡i vá» dense-only.\")\n",
        "\n",
        "# --------------------- Hybrid search ---------------------\n",
        "def hybrid_search(query: str, k_dense=10, k_sparse=80, alpha=0.60):\n",
        "    dense_hits = []\n",
        "    if vs is not None:\n",
        "        try:\n",
        "            dense_hits = vs.similarity_search_with_score(query, k=k_dense)\n",
        "        except Exception as e:\n",
        "            # fallback: empty\n",
        "            dense_hits = []\n",
        "    dense_scores = {}\n",
        "    for d, s in dense_hits:\n",
        "        # assume higher score = more relevant; adapt if driver gives distance\n",
        "        dense_scores[doc_key(d)] = float(s)\n",
        "\n",
        "    if bm25 is None:\n",
        "        return [(d, s) for d, s in dense_hits]\n",
        "\n",
        "    qtok = payload_tokenize(query)\n",
        "    sparse_arr = bm25.get_scores(qtok)\n",
        "    sparse_scores = { bm25_keys[i]: float(sc) for i, sc in enumerate(sparse_arr) if sc > 0 }\n",
        "\n",
        "    dn, sn = _minmax(dense_scores), _minmax(sparse_scores)\n",
        "    keys = set(dn) | set(sn)\n",
        "    combo = {k: alpha*dn.get(k,0.0) + (1-alpha)*sn.get(k,0.0) for k in keys}\n",
        "\n",
        "    results = []\n",
        "    dense_map = { doc_key(d): d for d, _ in dense_hits }\n",
        "    for k, sc in sorted(combo.items(), key=lambda x: x[1], reverse=True):\n",
        "        d = dense_map.get(k) or key2doc.get(k)\n",
        "        if d:\n",
        "            results.append((d, sc))\n",
        "    return results\n",
        "\n",
        "# --------------------- LLM JSON helpers (giá»¯ nguyÃªn) ---------------------\n",
        "SYSTEM_HYP = \"\"\"Báº¡n lÃ  L2 SOC analyst. Nháº­n EVENT (log thÃ´) vÃ :\n",
        "1) Sinh tá»‘i Ä‘a 5 giáº£ thuyáº¿t kiá»ƒu táº¥n cÃ´ng cÃ³ thá»ƒ (vd: SQL Injection, XSS, LFI, Command Injection, Brute Force...)\n",
        "2) Vá»›i má»—i giáº£ thuyáº¿t, sinh 2â€“4 cÃ¢u há»i ngáº¯n Ä‘á»ƒ truy KB nháº±m tÃ¬m báº±ng chá»©ng.\n",
        "3) Tráº£ JSON: {\"hypotheses\":[{\"name\":\"...\", \"mitre\":\"Txxxx\", \"questions\":[\"...\", \"...\"]}, ...]}.\n",
        "Chá»‰ tráº£ JSON há»£p lá»‡, khÃ´ng giáº£i thÃ­ch thÃªm.\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_JUDGE = \"\"\"Báº¡n lÃ  bá»™ cháº¥m Ä‘iá»ƒm báº±ng chá»©ng. Dá»±a trÃªn CONTEXT (cÃ¡c Ä‘oáº¡n KB Ä‘Ã£ truy há»“i) vÃ  EVENT (log thÃ´):\n",
        "- Káº¿t luáº­n {\"attack\":\"...\", \"mitre\":\"Txxxx\", \"confidence\":0..1, \"rationale\":\"...\", \"sources\":[...]}.\n",
        "- Chá»‰ chá»n attack náº¿u cÃ³ báº±ng chá»©ng rÃµ (pattern/rule/step khá»›p). Náº¿u khÃ´ng Ä‘á»§ báº±ng chá»©ng, tráº£ {\"attack\":\"Unknown\", \"mitre\":\"\", \"confidence\":0.0, ...}\n",
        "Chá»‰ tráº£ JSON há»£p lá»‡.\n",
        "\"\"\"\n",
        "\n",
        "def llm_json(llm, system_prompt, user_content, max_retries=2):\n",
        "    if llm is None:\n",
        "        raise ValueError(\"LLM chÆ°a khá»Ÿi táº¡o.\")\n",
        "    for _ in range(max_retries+1):\n",
        "        resp = llm.invoke([SystemMessage(content=system_prompt),\n",
        "                           HumanMessage(content=user_content)])\n",
        "        txt = (resp.content or \"\").strip()\n",
        "        try:\n",
        "            start = txt.find(\"{\")\n",
        "            end = txt.rfind(\"}\")\n",
        "            if start >= 0 and end > start:\n",
        "                return json.loads(txt[start:end+1])\n",
        "        except Exception:\n",
        "            continue\n",
        "    raise ValueError(\"LLM khÃ´ng tráº£ JSON há»£p lá»‡:\\n\" + txt)\n",
        "\n",
        "def retrieve_evidence_for_questions(questions, topk=5, alpha=0.60):\n",
        "    evid = []\n",
        "    for q in questions:\n",
        "        hits = hybrid_search(q, k_dense=topk, k_sparse=80, alpha=alpha)\n",
        "        for d, sc in hits:\n",
        "            evid.append({\n",
        "                \"text\": d.page_content[:1500],\n",
        "                \"meta\": d.metadata,\n",
        "                \"score\": float(sc)\n",
        "            })\n",
        "    seen = set(); uniq = []\n",
        "    for e in evid:\n",
        "        m = e[\"meta\"] or {}\n",
        "        sig = (m.get(\"source_url\") or m.get(\"source\") or \"\") + \"|\" + str(m.get(\"yaml_path\") or \"\")\n",
        "        if sig in seen:\n",
        "            continue\n",
        "        seen.add(sig)\n",
        "        uniq.append(e)\n",
        "    return uniq[:20]\n",
        "\n",
        "def analyze_log(event_text: str, alpha=0.60):\n",
        "    hyp_json = llm_json(llm, SYSTEM_HYP, f\"EVENT:\\n{event_text}\")\n",
        "    hypotheses = hyp_json.get(\"hypotheses\", [])[:5]\n",
        "    hyp_results = []\n",
        "    for hyp in hypotheses:\n",
        "        qs = hyp.get(\"questions\", [])[:4]\n",
        "        evid = retrieve_evidence_for_questions(qs, topk=5, alpha=alpha)\n",
        "        ctx_blocks = []\n",
        "        for e in evid[:10]:\n",
        "            meta = e[\"meta\"] or {}\n",
        "            src = meta.get(\"source_url\") or meta.get(\"source\") or \"\"\n",
        "            head = (e[\"text\"] or \"\")[:600]\n",
        "            ctx_blocks.append(f\"SOURCE: {src}\\n{head}\")\n",
        "        ctx = \"\\n\\n---\\n\\n\".join(ctx_blocks)\n",
        "        judge_in = f\"CONTEXT:\\n{ctx}\\n\\nEVENT:\\n{event_text}\\n\"\n",
        "        judge_json = llm_json(llm, SYSTEM_JUDGE, judge_in)\n",
        "        judge_json[\"hypothesis\"] = hyp.get(\"name\")\n",
        "        judge_json[\"hyp_mitre\"] = hyp.get(\"mitre\",\"\")\n",
        "        judge_json[\"asked\"] = qs\n",
        "        judge_json[\"top_sources\"] = [ (e[\"meta\"].get(\"source_url\") or e[\"meta\"].get(\"source\") or \"\") for e in evid[:5] ]\n",
        "        hyp_results.append(judge_json)\n",
        "    best = max(hyp_results, key=lambda x: x.get(\"confidence\", 0.0), default={\"attack\":\"Unknown\",\"confidence\":0.0})\n",
        "    return {\n",
        "        \"event\": event_text,\n",
        "        \"results\": hyp_results,\n",
        "        \"best\": best\n",
        "    }\n",
        "\n",
        "# --------------------- RAG-QA prompts & function ---------------------\n",
        "SYSTEM_PLAN = \"\"\"Báº¡n lÃ  trá»£ lÃ½ an toÃ n thÃ´ng tin. Nháº­n CÃ‚U Há»ŽI cá»§a ngÆ°á»i dÃ¹ng.\n",
        "HÃ£y:\n",
        "1) PhÃ¢n rÃ£ tá»‘i Ä‘a 5 cÃ¢u há»i con cáº§n tra cá»©u KB Ä‘á»ƒ tráº£ lá»i.\n",
        "2) Tráº£ JSON há»£p lá»‡:\n",
        "{\"subquestions\": [\"...\", \"...\", \"...\"]}\n",
        "\n",
        "Chá»‰ tráº£ JSON, khÃ´ng giáº£i thÃ­ch thÃªm.\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_WRITE = \"\"\"Báº¡n lÃ  trá»£ lÃ½ an toÃ n thÃ´ng tin tráº£ lá»i NGáº®N Gá»ŒN, CHÃNH XÃC vÃ  cÃ³ dáº«n nguá»“n.\n",
        "Dá»±a trÃªn CONTEXT (cÃ¡c Ä‘oáº¡n KB Ä‘Ã£ truy há»“i) vÃ  QUESTION (cÃ¢u há»i ngÆ°á»i dÃ¹ng), hÃ£y:\n",
        "- Tráº£ lá»i trá»±c tiáº¿p, sÃºc tÃ­ch. Náº¿u cÃ³ cáº£nh bÃ¡o/ngoáº¡i lá»‡, nÃªu rÃµ.\n",
        "- Khi cÃ³ thá»ƒ, nÃªu thÃªm MITRE ID liÃªn quan hoáº·c CVE náº¿u cÃ¢u há»i dÃ­nh tá»›i lá»— há»•ng/chiáº¿n thuáº­t.\n",
        "- Cuá»‘i cÃ¢u tráº£ lá»i, thÃªm má»¥c 'Nguá»“n:' dáº¡ng danh sÃ¡ch ngáº¯n gá»n (tá»‘i Ä‘a 5 dÃ²ng) gá»“m tiÃªu Ä‘á»/ngáº¯n gá»n + URL.\n",
        "\n",
        "Chá»‰ tráº£ ná»™i dung Markdown (Ä‘á»«ng tráº£ JSON).\n",
        "\"\"\"\n",
        "\n",
        "def qa_answer(user_question: str, alpha=0.60, topk=5):\n",
        "    # 1) Láº­p káº¿ hoáº¡ch (subquestions)\n",
        "    try:\n",
        "        plan = llm_json(llm, SYSTEM_PLAN, f\"QUESTION:\\n{user_question}\")\n",
        "        subqs = plan.get(\"subquestions\", [])[:5]\n",
        "    except Exception:\n",
        "        subqs = [user_question]\n",
        "    if not subqs:\n",
        "        subqs = [user_question]\n",
        "\n",
        "    # 2) Retrieve\n",
        "    evid = retrieve_evidence_for_questions(subqs, topk=topk, alpha=alpha)\n",
        "\n",
        "    # 3) Build context\n",
        "    ctx_blocks, source_urls = [], []\n",
        "    for e in evid[:12]:\n",
        "        meta = e.get(\"meta\") or {}\n",
        "        url = meta.get(\"source_url\") or meta.get(\"source\") or \"\"\n",
        "        title = meta.get(\"title\") or meta.get(\"technique_id\") or meta.get(\"cheatsheet_name\") or \"\"\n",
        "        head = (e.get(\"text\") or \"\")[:800]\n",
        "        ctx_blocks.append(f\"SOURCE: {title or url}\\nURL: {url}\\n{head}\")\n",
        "        if url:\n",
        "            source_urls.append(url)\n",
        "    context = \"\\n\\n---\\n\\n\".join(ctx_blocks)\n",
        "\n",
        "    # 4) Write final answer\n",
        "    writer_in = f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{user_question}\\n\"\n",
        "    if llm is None:\n",
        "        raise ValueError(\"LLM chÆ°a khá»Ÿi táº¡o.\")\n",
        "    resp = llm.invoke([SystemMessage(content=SYSTEM_WRITE),\n",
        "                       HumanMessage(content=writer_in)])\n",
        "    answer_md = (resp.content or \"\").strip()\n",
        "\n",
        "    # Compact sources\n",
        "    uniq = []\n",
        "    seen = set()\n",
        "    for u in source_urls:\n",
        "        if u and u not in seen:\n",
        "            uniq.append(u); seen.add(u)\n",
        "        if len(uniq) >= 5:\n",
        "            break\n",
        "\n",
        "    return {\n",
        "        \"question\": user_question,\n",
        "        \"answer_markdown\": answer_md,\n",
        "        \"sources\": uniq,\n",
        "        \"subquestions\": subqs\n",
        "    }\n",
        "\n",
        "# --------------------- Simple router (QA vs log analysis) ---------------------\n",
        "def smart_router(user_input: str) -> str:\n",
        "    s = (user_input or \"\").lower()\n",
        "    payloadish = bool(re.search(r\"(\\.\\./|%[0-9a-f]{2}|;|\\||&&|\\$\\{|\\bselect\\b|\\binsert\\b|\\bunion\\b)\", s))\n",
        "    longish = len(s) > 300\n",
        "    if payloadish or longish:\n",
        "        return \"log\"\n",
        "    return \"qa\"\n",
        "\n",
        "# --------------------- Demo: 2 vÃ­ dá»¥ (QA + Log) ---------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Demo QA\n",
        "    try:\n",
        "        q = \"So sÃ¡nh XSS Reflected vá»›i Stored, cÃ¡ch phÃ¡t hiá»‡n vÃ  rule Sigma máº«u?\"\n",
        "        print(\"\\nðŸ”Ž RAG-QA DEMO\")\n",
        "        qa = qa_answer(q, alpha=0.55, topk=6)\n",
        "        print(\"\\n--- QUESTION ---\\n\", q)\n",
        "        print(\"\\n--- ANSWER (markdown) ---\\n\", qa[\"answer_markdown\"])\n",
        "        print(\"\\n--- SUBQUESTIONS ---\\n\", qa[\"subquestions\"])\n",
        "        print(\"\\n--- SOURCES ---\\n\", qa[\"sources\"])\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ QA demo lá»—i:\", e)\n",
        "\n",
        "    # Demo Log analysis (sá»­ dá»¥ng pipeline cÅ©)\n",
        "    try:\n",
        "        demo_log = \"GET /../../etc.conf HTTP/1.1\\nHost: victim\\nUser-Agent: test\\n\"\n",
        "        print(\"\\n\\nðŸ”Ž LOG-ANALYZE DEMO\")\n",
        "        out = analyze_log(demo_log, alpha=0.60)\n",
        "        print(\"\\n=== BEST ===\\n\", json.dumps(out[\"best\"], ensure_ascii=False, indent=2))\n",
        "        print(\"\\n=== ALL HYPOTHESES ===\")\n",
        "        for r in out[\"results\"]:\n",
        "            print(\"-\", r.get(\"hypothesis\"), \"->\", r.get(\"attack\"), \"| conf=\", r.get(\"confidence\"))\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ Log analyze demo lá»—i:\", e)\n",
        "\n",
        "# ================================================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ”§ Generating Splunk savedsearch for SQLi...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- RULE JSON ---\n",
            "{\n",
            "  \"title\": \"Simple SQL Injection Detection\",\n",
            "  \"description\": \"Detects simple SQL injection attempts in web URI and POST params. This rule looks for common SQL injection payloads like ' OR '1'='1, union select, sleep(5), and benchmark.\",\n",
            "  \"spl\": \"search (index=web_logs \\\"' OR '1'='1\\\" OR \\\"union select\\\" OR \\\"sleep(5)\\\" OR \\\"benchmark\\\") | stats count as num_events by src_ip, uri_path, user_agent\",\n",
            "  \"condition_examples\": [\n",
            "    \"http://example.com/user?id=' OR '1'='1\",\n",
            "    \"http://example.com/user?id=1 union select * from users\"\n",
            "  ],\n",
            "  \"severity\": \"high\",\n",
            "  \"mitre\": [\n",
            "    \"T1190\"\n",
            "  ],\n",
            "  \"schedule\": \"*/5 * * * *\",\n",
            "  \"alert_actions\": [\n",
            "    \"email\",\n",
            "    \"notable\",\n",
            "    \"webhook\"\n",
            "  ],\n",
            "  \"tags\": [\n",
            "    \"web\",\n",
            "    \"sqli\",\n",
            "    \"input-validation\"\n",
            "  ],\n",
            "  \"detection_notes\": \"This rule may generate false positives if the application uses SQL-like syntax in its URI or POST params. Tuning suggestions: adjust the search query to exclude known false positives, and consider adding additional conditions to filter out legitimate traffic.\",\n",
            "  \"evidence_urls\": [\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\sigma\\\\rules\\\\web\\\\webserver_generic\\\\web_sql_injection_in_access_logs.yml\",\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Query_Parameterization_Cheat_Sheet.md\",\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Injection_Prevention_Cheat_Sheet.md\",\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\SQL_Injection_Prevention_Cheat_Sheet.md\",\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\GraphQL_Cheat_Sheet.md\",\n",
            "    \"D:\\\\MCPLLM\\\\test\\\\cheatsheets\\\\Laravel_Cheat_Sheet.md\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# --------------------- Generate Splunk Rule (Saved Search) from question/log ---------------------\n",
        "SYSTEM_SPLUNK_RULE = \"\"\"Báº¡n lÃ  SOC engineer + Splunk expert.\n",
        "Nhiá»‡m vá»¥: dá»±a trÃªn CONTEXT (Ä‘oáº¡n KB Ä‘Ã£ truy há»“i) vÃ  INPUT (cÃ³ thá»ƒ lÃ  má»™t EVENT log hoáº·c cÃ¢u há»i ká»¹ thuáº­t),\n",
        "hÃ£y sinh má»™t Splunk savedsearch / detection rule gá»“m:\n",
        "- title: ngáº¯n gá»n\n",
        "- description: 1-2 cÃ¢u (gá»“m rationale & Ä‘iá»u kiá»‡n phÃ¡t hiá»‡n)\n",
        "- spl: cÃ¢u lá»‡nh SPL (Ä‘Ã£ test logic, khÃ´ng dÃ i quÃ¡ 5 dÃ²ng; dÃ¹ng rex, search, stats, transaction, where, eval, mvexpand khi cáº§n)\n",
        "- condition_examples: 2 vÃ­ dá»¥ payload/log máº«u khá»›p rule\n",
        "- severity: one of [\"low\",\"medium\",\"high\",\"critical\"]\n",
        "- mitre: list of MITRE ATT&CK ids (náº¿u cÃ³)\n",
        "- schedule: cron-like hoáº·c interval (\"*/5 * * * *\" or \"every 5m\")\n",
        "- alert_actions: list (vÃ­ dá»¥: [\"email\",\"notable\",\"webhook\",\"runbook\"])\n",
        "- tags: short list of tags (e.g., [\"web\",\"sqli\",\"input-validation\"])\n",
        "- detection_notes: ngáº¯n gá»n cÃ¡c false-positive possible & tuning suggestions\n",
        "\n",
        "Tráº£ vá» **má»™t JSON** há»£p lá»‡ vá»›i nhá»¯ng field trÃªn. Chá»‰ tráº£ JSON, KHÃ”NG giáº£i thÃ­ch thÃªm.\n",
        "\"\"\"\n",
        "\n",
        "def generate_splunk_rule(input_text: str, alpha=0.60, topk=6):\n",
        "    \"\"\"\n",
        "    Pipeline:\n",
        "      1) táº¡o sub-queries (dÃ¹ng llm_json nhÆ° PLAN) -> retrieve evidence\n",
        "      2) build context -> gá»i llm_json vá»›i SYSTEM_SPLUNK_RULE -> parse JSON -> tráº£ vá» dict\n",
        "    \"\"\"\n",
        "    # 1) Sá»­ dá»¥ng plan Ä‘á»ƒ láº¥y subquestions (tÃ¡i dÃ¹ng SYSTEM_PLAN náº¿u muá»‘n)\n",
        "    try:\n",
        "        plan = llm_json(llm, SYSTEM_PLAN, f\"QUESTION (for rule generation):\\n{input_text}\")\n",
        "        subqs = plan.get(\"subquestions\", [])[:6]\n",
        "    except Exception:\n",
        "        subqs = [input_text]\n",
        "\n",
        "    # 2) Retrieve evidence\n",
        "    evid = retrieve_evidence_for_questions(subqs, topk=topk, alpha=alpha)\n",
        "    ctx_blocks = []\n",
        "    for e in evid[:12]:\n",
        "        meta = e.get(\"meta\") or {}\n",
        "        url = meta.get(\"source_url\") or meta.get(\"source\") or \"\"\n",
        "        title = meta.get(\"title\") or meta.get(\"cheatsheet_name\") or \"\"\n",
        "        head = (e.get(\"text\") or \"\")[:800]\n",
        "        ctx_blocks.append(f\"SOURCE: {title or url}\\nURL: {url}\\n{head}\")\n",
        "    context = \"\\n\\n---\\n\\n\".join(ctx_blocks)\n",
        "\n",
        "    # 3) Ask LLM to generate Splunk rule JSON\n",
        "    llm_input = f\"CONTEXT:\\n{context}\\n\\nINPUT:\\n{input_text}\\n\\nPlease produce the JSON as requested.\"\n",
        "    try:\n",
        "        rule_json = llm_json(llm, SYSTEM_SPLUNK_RULE, llm_input, max_retries=2)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"LLM khÃ´ng tráº£ JSON rule há»£p lá»‡: \" + str(e))\n",
        "\n",
        "    # 4) Basic validation / normalize fields\n",
        "    # Ensure required keys exist and have defaults\n",
        "    keys_defaults = {\n",
        "        \"title\": \"Generated Splunk Rule\",\n",
        "        \"description\": \"\",\n",
        "        \"spl\": \"\",\n",
        "        \"condition_examples\": [],\n",
        "        \"severity\": \"medium\",\n",
        "        \"mitre\": [],\n",
        "        \"schedule\": \"every 5m\",\n",
        "        \"alert_actions\": [],\n",
        "        \"tags\": [],\n",
        "        \"detection_notes\": \"\"\n",
        "    }\n",
        "    for k, dv in keys_defaults.items():\n",
        "        if k not in rule_json:\n",
        "            rule_json[k] = dv\n",
        "\n",
        "    # optional: attach top evidence urls\n",
        "    top_urls = []\n",
        "    for e in evid[:6]:\n",
        "        m = e.get(\"meta\") or {}\n",
        "        u = m.get(\"source_url\") or m.get(\"source\") or \"\"\n",
        "        if u and u not in top_urls:\n",
        "            top_urls.append(u)\n",
        "    rule_json.setdefault(\"evidence_urls\", top_urls)\n",
        "\n",
        "    return rule_json\n",
        "\n",
        "# --------------------- Example usage ---------------------\n",
        "if __name__ == \"__main__\" and llm is not None:\n",
        "    # Example: muá»‘n rule phÃ¡t hiá»‡n SQLi basic trÃªn webapp\n",
        "    try:\n",
        "        inp = \"Detect simple SQL injection attempts in web URI and POST params (payloads like \\\"' OR '1'='1\\\" , union select, sleep(5), benchmark).\"\n",
        "        print(\"\\nðŸ”§ Generating Splunk savedsearch for SQLi...\")\n",
        "        spl_rule = generate_splunk_rule(inp, alpha=0.55, topk=8)\n",
        "        print(\"\\n--- RULE JSON ---\")\n",
        "        print(json.dumps(spl_rule, ensure_ascii=False, indent=2))\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ Lá»—i khi sinh Splunk rule:\", e)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
