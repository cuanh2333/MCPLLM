# ===================================================================
# üìö UNIVERSAL DOCUMENT LOADER + SPLITTER + CHROMA BUILDER
# Cho KB c·ªßa SOC (MITRE, Sigma, Playbook, CVE, CSV event, v.v.)
# ===================================================================
import os
import shutil
from pathlib import Path

# -----------------------------
# 1Ô∏è‚É£ LOADERS (Document Loaders)
# -----------------------------
try:
    from langchain_community.document_loaders import (
        PyMuPDFLoader,                # PDF
        TextLoader,                   # Markdown (kh√¥ng c·∫ßn 'unstructured')
        CSVLoader                     # CSV
    )
except ImportError:
    os.system("pip install -q langchain-community pymupdf")
    from langchain_community.document_loaders import (
        PyMuPDFLoader,
        TextLoader,
        CSVLoader
    )

# -----------------------------
# 2Ô∏è‚É£ SPLITTERS (Text Splitters)
# -----------------------------
try:
    from langchain_text_splitters import (
        RecursiveCharacterTextSplitter,  # Cho PDF, text
        MarkdownHeaderTextSplitter       # Cho playbook .md
    )
except ImportError:
    os.system("pip install -q langchain-text-splitters")
    from langchain_text_splitters import (
        RecursiveCharacterTextSplitter,
        MarkdownHeaderTextSplitter
    )

# -----------------------------
# 3Ô∏è‚É£ VECTOR STORE + EMBEDDING
# -----------------------------
try:
    from langchain_chroma import Chroma
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    os.system("pip install -q langchain-chroma chromadb langchain-huggingface sentence-transformers")
    from langchain_chroma import Chroma
    from langchain_huggingface import HuggingFaceEmbeddings

from langchain_core.documents import Document
# -----------------------------
# 4Ô∏è‚É£ EMBEDDING MODEL
# -----------------------------
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    encode_kwargs={"normalize_embeddings": True}
)

# ===================================================================
# ‚öôÔ∏è FUNCTION: LOAD + SPLIT + STORE
# ===================================================================
# ===================================================================
# üìö (C√°c import c·ªßa b·∫°n gi·ªØ nguy√™n ·ªü ƒë√¢y)
# ...
# from langchain_core.documents import Document
# embeddings = HuggingFaceEmbeddings(...)
# ===================================================================

# ===================================================================
# üìö UNIVERSAL DOCUMENT LOADER + SPLITTER + CHROMA BUILDER
# ===================================================================

import os
import shutil
from pathlib import Path

# -----------------------------
# 1Ô∏è‚É£ (Import Loaders)
# -----------------------------
try:
    from langchain_community.document_loaders import (
        PyMuPDFLoader, TextLoader, CSVLoader
    )
except ImportError:
    # (Code c√†i ƒë·∫∑t c·ªßa b·∫°n)
    pass 

# -----------------------------
# 2Ô∏è‚É£ (Import Splitters)
# -----------------------------
try:
    from langchain_text_splitters import (
        RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
    )
except ImportError:
    # (Code c√†i ƒë·∫∑t c·ªßa b·∫°n)
    pass

# -----------------------------
# 3Ô∏è‚É£ (Import Vector Store)
# -----------------------------
try:
    from langchain_chroma import Chroma
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    # (Code c√†i ƒë·∫∑t c·ªßa b·∫°n)
    pass
    
from langchain_core.documents import Document

# -----------------------------
# 4Ô∏è‚É£ EMBEDDING MODEL (ƒê·ªãnh nghƒ©a 1 l·∫ßn)
# -----------------------------
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    encode_kwargs={"normalize_embeddings": True}
)

# ===================================================================
# ‚öôÔ∏è H√ÄM PH·ª§ 1: X·ª¨ L√ù FILES (T√°ch ra t·ª´ h√†m build)
# (H√†m n√†y gi·ªØ nguy√™n c·∫•u tr√∫c, nh∆∞ng thay Markdown loader sang TextLoader)
# ===================================================================
def process_files_into_chunks(
    file_paths: list[Path], 
    chunk_size: int = 1000, 
    chunk_overlap: int = 150
) -> list[Document]:
    """
    Nh·∫≠n m·ªôt DANH S√ÅCH c√°c ƒë∆∞·ªùng d·∫´n file v√† ch·∫°y Load + Split
    tr√™n ch√∫ng, tr·∫£ v·ªÅ m·ªôt danh s√°ch c√°c chunks.
    """
    docs_all = []
    
    pdf_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[
        ("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3")
    ])

    for file in file_paths:
        ext = file.suffix.lower()
        split_docs = []
        try:
            if ext == ".pdf":
                loader = PyMuPDFLoader(str(file))
                docs = loader.load()
                split_docs = pdf_splitter.split_documents(docs)
                
            elif ext in (".md", ".markdown"):
                # ‚úÖ D√πng TextLoader thay cho UnstructuredMarkdownLoader
                loader = TextLoader(str(file), encoding="utf-8")
                docs = loader.load()
                # MarkdownHeaderTextSplitter c·∫ßn CHU·ªñI markdown ƒë·ªÉ t√°ch theo header
                merged_markdown = "\n".join([d.page_content for d in docs])
                split_docs = md_splitter.split_text(merged_markdown)  # tr·∫£ v·ªÅ List[Document]

                # ƒê·∫£m b·∫£o metadata c√≥ 'source'
                for d in split_docs:
                    d.metadata = {**(d.metadata or {}), "source": file.name, "ext": ext}

            elif ext == ".csv":
                loader = CSVLoader(str(file))
                docs = loader.load()
                split_docs = docs  # CSV th∆∞·ªùng coi m·ªói h√†ng l√† 1 Document
            
            else:
                print(f"‚ö†Ô∏è B·ªè qua file kh√¥ng h·ªó tr·ª£: {file.name}")
                continue

            # G·∫Øn metadata 'source' n·∫øu thi·∫øu
            for doc in split_docs:
                doc.metadata = {**(doc.metadata or {}), "source": doc.metadata.get("source", file.name)}
                
            docs_all.extend(split_docs)
            print(f"‚úÖ ƒê√£ x·ª≠ l√Ω {file.name} th√†nh {len(split_docs)} chunks.")

        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω {file.name}: {e}")
            
    return docs_all

# ===================================================================
# ‚öôÔ∏è H√ÄM PH·ª§ 2: BUILD DB (T·∫°o m·ªõi)
# (H√†m n√†y gi·ªØ nguy√™n, ƒë√£ ƒë√∫ng ‚Äì ch·ªâ ƒëang d√πng Chroma t·ª´ langchain_chroma)
# ===================================================================
def build_chroma_vectorstore(
    source_dir: str,
    persist_dir: str,
    collection_name: str, # <-- ƒê·∫£m b·∫£o d√πng t√™n n√†y
    chunk_size: int = 1000,
    chunk_overlap: int = 150
):
    """
    X√ìA DB C≈® (n·∫øu c√≥) v√† BUILD M·ªöI HO√ÄN TO√ÄN.
    """
    source_path = Path(source_dir)
    if not source_path.exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c ngu·ªìn: {source_dir}")
        return None
        
    if Path(persist_dir).exists():
        print(f"üßπ D·ªçn th∆∞ m·ª•c c≈©: {persist_dir}")
        shutil.rmtree(persist_dir)
        
    all_files = [file for file in source_path.glob("*") if file.suffix.lower() in [".pdf", ".md", ".csv"]]
    all_chunks = process_files_into_chunks(all_files, chunk_size, chunk_overlap)
    
    if not all_chunks:
        print("‚ö†Ô∏è Kh√¥ng c√≥ t√†i li·ªáu h·ª£p l·ªá ƒë·ªÉ embed.")
        return None

    print(f"\nüíæ ƒêang embed {len(all_chunks)} chunks v√†o Chroma (T·∫°o m·ªõi)...")
    vectorstore = Chroma.from_documents(
        documents=all_chunks,
        embedding=embeddings,
        persist_directory=persist_dir,
        collection_name=collection_name # <-- D√πng t√™n collection
    )
    
    count = vectorstore._collection.count()
    print(f"‚úÖ Ho√†n t·∫•t! ƒê√£ l∆∞u v√†o: {persist_dir}")
    print(f"üìä T·ªïng s·ªë t√†i li·ªáu (chunks) trong collection '{collection_name}': {count}")
    
    return vectorstore

# ===================================================================
# ‚öôÔ∏è H√ÄM PH·ª§ 3: TH√äM V√ÄO DB (*** gi·ªØ c·∫•u tr√∫c, ch·ªâ s·ª≠a import/collection ***)
# ===================================================================
def add_to_existing_db(
    new_chunks: list[Document], 
    db_directory: str, 
    embedding_model,
    collection_name: str  # <-- *** ƒê·∫£m b·∫£o truy·ªÅn tham s·ªë n√†y ***
):
    """ T·∫£i DB hi·ªán c√≥ v√† th√™m chunks m·ªõi v√†o ƒê√öNG collection. """
    if not new_chunks:
        print("Kh√¥ng c√≥ t√†i li·ªáu m·ªõi n√†o ƒë·ªÉ th√™m. B·ªè qua.")
        return
    
    print(f"\n--- üîÑ ƒêang t·∫£i DB hi·ªán c√≥ t·ª´: {db_directory} (Collection: {collection_name}) ---")
    vectorstore = Chroma(
        persist_directory=db_directory,
        embedding_function=embedding_model,
        collection_name=collection_name # <-- *** Ch·ªâ ƒë·ªãnh collection ***
    )
    
    count_before = vectorstore._collection.count()
    print(f"S·ªë l∆∞·ª£ng t√†i li·ªáu tr∆∞·ªõc khi th√™m: {count_before}")
    
    print(f"--- ‚ûï ƒêang th√™m {len(new_chunks)} t√†i li·ªáu m·ªõi v√†o DB... ---")
    vectorstore.add_documents(new_chunks)
    # Note: persist() kh√¥ng c√≤n c·∫ßn thi·∫øt trong version m·ªõi c·ªßa Chroma
    # Data ƒë∆∞·ª£c t·ª± ƒë·ªông persist khi add_documents()
    
    count_after = vectorstore._collection.count()
    print(f"S·ªë l∆∞·ª£ng t√†i li·ªáu sau khi th√™m: {count_after}")
    print("--- Th√™m t√†i li·ªáu th√†nh c√¥ng! ---")
    return vectorstore

# ===================================================================
# üöÄ H√ÄM ƒê·ªíNG B·ªò (SYNC) CH√çNH (*** gi·ªØ c·∫•u tr√∫c, ch·ªânh loader/collection ***)
# ===================================================================
def sync_kb_directory(
    source_dir: str, 
    persist_dir: str, 
    embedding_model,
    collection_name: str # <-- *** Truy·ªÅn t√™n n√†y ***
):
    """
    Ki·ªÉm tra file n√†o trong source_dir ƒë√£ c√≥ trong DB,
    v√† ch·ªâ n·∫°p (ingest) nh·ªØng file M·ªöI v√†o ƒê√öNG collection.
    """
    db_path = Path(persist_dir)
    source_path = Path(source_dir)
    
    # 1. KI·ªÇM TRA BUILD L·∫¶N ƒê·∫¶U
    if not db_path.exists():
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y DB t·∫°i {persist_dir}.")
        print("B·∫Øt ƒë·∫ßu BUILD DB M·ªöI...")
        build_chroma_vectorstore(
            source_dir=source_dir, 
            persist_dir=persist_dir,
            collection_name=collection_name # <-- *** Truy·ªÅn t√™n ***
        )
        return

    # 2. DB ƒê√É T·ªíN T·∫†I -> T·∫¢I V√Ä KI·ªÇM TRA
    print(f"‚úÖ ƒê√£ t√¨m th·∫•y DB. ƒêang t·∫£i (Collection: {collection_name})...")
    vectorstore = Chroma(
        persist_directory=persist_dir,
        embedding_function=embedding_model,
        collection_name=collection_name # <-- *** Ch·ªâ ƒë·ªãnh collection ***
    )
    
    current_doc_count = vectorstore._collection.count()
    print(f"üìä S·ªë t√†i li·ªáu (chunks) hi·ªán c√≥ trong collection '{collection_name}': {current_doc_count}")
    
    all_docs_in_db = vectorstore.get(include=["metadatas"])
    processed_files = set()
    if all_docs_in_db and all_docs_in_db['metadatas']:
        for meta in all_docs_in_db['metadatas']:
            if 'source' in meta:
                processed_files.add(meta['source'])
                
    print(f"üîç ƒê√£ t√¨m th·∫•y {len(processed_files)} file ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω trong DB.")

    # 3. QU√âT TH∆Ø M·ª§C NGU·ªíN
    print(f"üìÇ ƒêang qu√©t th∆∞ m·ª•c ngu·ªìn: {source_dir}")
    files_on_disk = [
        file for file in source_path.glob("*") 
        if file.suffix.lower() in [".pdf", ".md", ".csv"]
    ]
    
    # 4. T√åM FILE M·ªöI
    new_files_to_process = []
    for file in files_on_disk:
        if file.name not in processed_files:
            new_files_to_process.append(file)
            
    if not new_files_to_process:
        print("\n--- ‚úÖ KB ƒê√É ƒê∆Ø·ª¢C ƒê·ªíNG B·ªò. Kh√¥ng c√≥ file m·ªõi. ---")
        return

    print(f"\n--- üì£ Ph√°t hi·ªán {len(new_files_to_process)} file M·ªöI c·∫ßn n·∫°p ---")
    for f in new_files_to_process:
        print(f"  -> {f.name}")

    # 5. X·ª¨ L√ù V√Ä TH√äM FILE M·ªöI
    new_chunks = process_files_into_chunks(new_files_to_process)
    
    add_to_existing_db(
        new_chunks=new_chunks,
        db_directory=persist_dir,
        embedding_model=embedding_model,
        collection_name=collection_name # <-- *** Truy·ªÅn t√™n ***
    )
    
    print("\n--- ‚úÖ ƒê·ªìng b·ªô KB ho√†n t·∫•t! ---")

# ===================================================================
# üì¶ H√ÄM MAIN (ƒê·ªÇ CH·∫†Y) (*** gi·ªØ c·∫•u tr√∫c, ƒë·ªìng b·ªô import s·ª≠a ***)
# ===================================================================
if __name__ == "__main__":
    
    # --- 1. ƒê·ªãnh nghƒ©a c√°c ƒë∆∞·ªùng d·∫´n V√Ä T√äN ---
    SRC_DIR = r"D:\MCPLLM\KB\Security" 
    CHROMA_DIR = r"D:\MCPLLM\KB\chroma_db"
    # *** T√™n collection 1 l·∫ßn duy nh·∫•t ***
    MY_COLLECTION_NAME = "security_knowledge_base"
    
    # --- 2. Ch·∫°y h√†m ƒë·ªìng b·ªô (SYNC) ---
    sync_kb_directory(
        source_dir=SRC_DIR,
        persist_dir=CHROMA_DIR,
        embedding_model=embeddings,
        collection_name=MY_COLLECTION_NAME
    )

    # --- 3. Query th·ª≠ (T√πy ch·ªçn) ---
    print("\n--- KI·ªÇM TRA: T·∫£i DB v√† query th·ª≠ ---")
    try:
        final_db = Chroma(
            persist_directory=CHROMA_DIR,
            embedding_function=embeddings,
            collection_name=MY_COLLECTION_NAME
        )
        total = final_db._collection.count()
        print(f"üì¶ DB hi·ªán c√≥ {total} chunks.")
        final_count = final_db._collection.count()
        print(f"üìä T·ªïng s·ªë t√†i li·ªáu cu·ªëi c√πng trong collection '{MY_COLLECTION_NAME}': {final_count}")
        try:
            all_docs = final_db.get(include=["metadatas"])
        except Exception:
            all_docs = final_db._collection.get(include=["metadatas"])  # fallback

        # In danh s√°ch c√°c file ngu·ªìn
        sources = set()
        if all_docs and all_docs.get("metadatas"):
            for meta in all_docs["metadatas"]:
                if isinstance(meta, dict) and "source" in meta:
                    sources.add(meta["source"])

        print(f"üì¶ T·ªïng s·ªë source kh√°c nhau: {len(sources)}")
        for s in sorted(sources):
            print(f" - {s}")
        print()
    #     query = "IP: 192.168.1.100"
    #     print(f"\nƒêang t√¨m ki·∫øm: '{query}'")
    #     results = final_db.similarity_search(query, k=5)
        
    #     if results:
    #         print(f"T√¨m th·∫•y {len(results)} k·∫øt qu·∫£:")
    #         for doc in results:
    #              print(f"-> {doc.page_content[:300]}... (Ngu·ªìn: {doc.metadata.get('source','?')})")
    #     else:
    #         print("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£.")
    except Exception as e:
        print(f"‚ùå L·ªói khi query: {e}")
